{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "master_normal seq2seq",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1xk_TGJm51bIgyGwIqmEiR6YI6fDSN6W3",
          "timestamp": 1520410824800
        },
        {
          "file_id": "1Os9oWOWM-thM7tlXMNYfNlAC_L-l7arY",
          "timestamp": 1515554387158
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "FnS-GXJOJOY2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Tensorflow 1.4.0 is required.\n",
        "This is based on [NMT Tutorial](https://github.com/tensorflow/nmt).\n",
        "\n",
        "## notes\n",
        "* Training w/o USERNAME token\n",
        "  * replacing @username with USERNAME token caused high frequency of appearence of USERNAME. The result was poor.\n",
        "  * So simply replacing it with \"\".\n",
        "  * [commit](https://github.com/higepon/tensorflow_seq2seq_chatbot/commit/a2d53ee1b3196bffc97c226230e41e25a1b46904)\n",
        "* Make Attention work\n",
        "  * Now beam/greedy can't work together.\n",
        "* Testing Attention if it coverges with learning rate\n",
        "  * Adam 0.05 and age decay 0.99 didn't work, perplexity stays around 50000-100000\n",
        "  * Adam 0.5, didn't work\n",
        "  * SGD with learning rate = 0.5 paramter no decay, worked very well.\n",
        "    * perplexity goes down to around 80\n",
        "    * おはよう -> おはようございます\n",
        "    * [commit](https://github.com/higepon/tensorflow_seq2seq_chatbot/commit/c9e230a4fbd485e89f838af9e33b1d9c9f7bc0d0)\n",
        "    * 'num_layers': 3, 'vocab_size': 50000, 'embedding_size': 1024\n",
        "  * SGD 0.1 with conversation based data\n",
        "* Future plan\n",
        "  * Observation: See many dull responses, in normal seq2seq. So we try RL method.\n",
        "  * Steps\n",
        "    * Train normal seq2seq\n",
        "    * Record how many dull responses does it have\n",
        "    * Define dull responses list.\n",
        "    * Check code is working\n",
        "      * RL destination\n",
        "      * learning rate etc.\n",
        "  \n",
        "  \n",
        "  \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "bPLkjCHPSyGx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "\n",
        "# @formatter:off\n",
        "import copy as copy\n",
        "import datetime\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from enum import Enum\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.layers import core as layers_core\n",
        "from tensorflow.python.platform import gfile\n",
        "\n",
        "#@formatter:on\n",
        "\n",
        "\n",
        "def colab():\n",
        "    return '/tools/node/bin/forever' == os.environ['_']\n",
        "\n",
        "\n",
        "if colab():\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "# Note for myself.\n",
        "# You've summarized Seq2Seq\n",
        "# at http://d.hatena.ne.jp/higepon/20171210/1512887715.\n",
        "\n",
        "# If you see following error, it means your max(len(tweets of training set)) <  decoder_length.\n",
        "# This should be a bug somewhere in build_decoder, but couldn't find one yet.\n",
        "# You can workaround by setting hparams.decoder_length=max len of tweet in training set.\n",
        "# InvalidArgumentError: logits and labels must have the same first dimension, got logits shape [48,50] and labels shape [54]\n",
        "#\t [[Node: root/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, \n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "def data_dir():\n",
        "    target_dir = \"{}/chatbot_data\".format(str(Path.home()))\n",
        "    if not colab() and not os.path.exists(target_dir):\n",
        "        raise Exception(\"{} not found, you may create\".format(target_dir))\n",
        "    return target_dir\n",
        "\n",
        "\n",
        "def info(message, hparams):\n",
        "    if hparams.debug_verbose:\n",
        "        print(message)\n",
        "\n",
        "\n",
        "def p(path):\n",
        "    if colab():\n",
        "        return path\n",
        "    else:\n",
        "        return \"{}/{}\".format(data_dir(), path)\n",
        "\n",
        "\n",
        "def has_gpu0():\n",
        "    return tf.test.gpu_device_name() == \"/device:GPU:0\"\n",
        "\n",
        "\n",
        "class ModelDirectory(Enum):\n",
        "    tweet_large = 'model/tweet_large'\n",
        "    tweet_large_rl = 'model/tweet_large_rl'\n",
        "    tweet_large_swapped = 'model/tweet_large_swapped'\n",
        "    tweet_small = 'model/tweet_small'\n",
        "    tweet_small_swapped = 'model/tweet_small_swapped'\n",
        "    tweet_small_rl = 'model/tweet_small_rl'\n",
        "    test_multiple1 = 'model/test_multiple1'\n",
        "    test_multiple2 = 'model/test_multiple2'\n",
        "    test_multiple3 = 'model/test_multiple3'\n",
        "    test_distributed = 'model/test_distributed'\n",
        "\n",
        "    @staticmethod\n",
        "    def create_all_directories():\n",
        "        for d in ModelDirectory:\n",
        "            os.makedirs(p(d.value), exist_ok=True)\n",
        "\n",
        "\n",
        "# todo\n",
        "# collect all initializer\n",
        "ModelDirectory.create_all_directories()\n",
        "\n",
        "if False:\n",
        "    class GoogleDriveFolder(Enum):\n",
        "        root = '146ZLldWXLDH0l9WbSUNFKi3nVK_HV0Sz'\n",
        "        seq2seq = '18lYBgKvX3AG1zhwJqP1tRYJU688U1N95'\n",
        "        seq2seq_swapped = '1w56FFoKStEfZNThA2Y_jx3NDTELcau52'\n",
        "        seq2seq_rl = '1pHnOuT_7JjD1TS8VQ4KN9oUiblBIABXJ'\n",
        "else:\n",
        "    class GoogleDriveFolder(Enum):\n",
        "        root = '15Z3wbaSjR34ziPgAVEFiXgM67ln1Z9Xt'\n",
        "        seq2seq = '1KdMwLNbUfI_PZ5QTyi2zvcS___ct399p'\n",
        "        seq2seq_swapped = '1OjvR4TXAVudSiI-A7EBjw3gLoY4mrO7i'\n",
        "        seq2seq_rl = '161ler0gTpsvFUPAvc4x1__jyClJ_8IyB'\n",
        "\n",
        "base_hparams = tf.contrib.training.HParams(\n",
        "    batch_size=3,\n",
        "    encoder_length=5,\n",
        "    decoder_length=5,\n",
        "    num_units=6,\n",
        "    num_layers=2,\n",
        "    vocab_size=9,\n",
        "    embedding_size=8,\n",
        "    learning_rate=0.01,\n",
        "    learning_rate_decay=0.99,\n",
        "    max_gradient_norm=5.0,\n",
        "    beam_width=2,\n",
        "    use_attention=False,\n",
        "    num_train_steps=100,\n",
        "    debug_verbose=False,\n",
        "    model_folder_in_drive=GoogleDriveFolder.seq2seq.value,\n",
        "    model_path='Please override model_directory',\n",
        "    sos_id=0,\n",
        "    eos_id=1,\n",
        "    pad_id=2,\n",
        "    unk_id=3,\n",
        "    sos_token=\"[SOS]\",\n",
        "    eos_token=\"[EOS]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    unk_token=\"[UNK]\",\n",
        ")\n",
        "\n",
        "test_hparams = copy.deepcopy(base_hparams).override_from_dict(\n",
        "    {'beam_width': 0, 'num_train_steps': 100, 'learning_rate': 0.5})\n",
        "\n",
        "\n",
        "\n",
        "test_attention_hparams = copy.deepcopy(test_hparams).override_from_dict(\n",
        "    {'use_attention': True})\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DFEYKvBwL3Nm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# For debug purpose.\n",
        "tf.reset_default_graph()\n",
        "\n",
        "\n",
        "class ChatbotModel:\n",
        "    def __init__(self, sess, hparams, model_path, scope='ChatbotModel'):\n",
        "        self.sess = sess\n",
        "        # todo remove\n",
        "        self.hparams = hparams\n",
        "\n",
        "        # todo\n",
        "        self.model_path = model_path\n",
        "        self.name = scope\n",
        "\n",
        "        self.encoder_inputs, self.encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder = self._build_encoder(\n",
        "            hparams, scope)\n",
        "        self.decoder_inputs, self.decoder_target_lengths, self.logits = self._build_decoder(\n",
        "            hparams, self.encoder_inputs_lengths, embedding_encoder,\n",
        "            encoder_state, encoder_outputs)\n",
        "\n",
        "        self.reward = tf.placeholder(tf.float32, name=\"reward\")\n",
        "        self.target_labels, self.loss, self.global_step, self.learning_rate, self.train_op = self._build_optimizer(\n",
        "            hparams, self.logits)\n",
        "        self.train_loss_summary = tf.summary.scalar(\"loss\", self.loss)\n",
        "        self.valiation_loss_summary = tf.summary.scalar(\"validation_loss\", self.loss)\n",
        "        self.merged_summary = tf.summary.merge_all()\n",
        "\n",
        "        # Initialize saver after model created\n",
        "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
        "\n",
        "    def restore(self):\n",
        "        ckpt = tf.train.get_checkpoint_state(self.model_path)\n",
        "        if ckpt:\n",
        "            last_model = ckpt.model_checkpoint_path\n",
        "            self.saver.restore(self.sess, last_model)\n",
        "            return True\n",
        "        else:\n",
        "            print(\"Created fresh model.\")\n",
        "            return False\n",
        "\n",
        "    def train(self, encoder_inputs, encoder_inputs_lengths, target_labels,\n",
        "              decoder_inputs, decoder_target_lengths, reward=1.0):\n",
        "        feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "            self.target_labels: target_labels,\n",
        "            self.decoder_inputs: decoder_inputs,\n",
        "            self.decoder_target_lengths: decoder_target_lengths,\n",
        "            # For normal Seq2Seq reward is always 1.\n",
        "            self.reward: reward\n",
        "        }\n",
        "        _, global_step, summary = self.sess.run(\n",
        "            [self.train_op, self.global_step, self.train_loss_summary], feed_dict=feed_dict)\n",
        "        return global_step, self.learning_rate, summary\n",
        "\n",
        "    def batch_loss(self, encoder_inputs, encoder_inputs_lengths, target_labels,\n",
        "                   decoder_inputs, decoder_target_lengths):\n",
        "        feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "            self.target_labels: target_labels,\n",
        "            self.decoder_inputs: decoder_inputs,\n",
        "            self.decoder_target_lengths: decoder_target_lengths,\n",
        "            # For normal Seq2Seq reward is always 1.\n",
        "            self.reward: 1\n",
        "        }\n",
        "        return self.sess.run([self.loss, self.valiation_loss_summary],\n",
        "                             feed_dict=feed_dict)\n",
        "\n",
        "    def train_with_reward(self, infer_model, standard_seq2seq_model,\n",
        "                          encoder_inputs, encoder_inputs_lengths, target_labels,\n",
        "                          decoder_inputs, decoder_target_lengths,\n",
        "                          dull_responses):\n",
        "        infered_replies = infer_model.infer(encoder_inputs,\n",
        "                                            encoder_inputs_lengths)\n",
        "        standard_seq2seq_encoder_inputs = []\n",
        "        standard_seq2seq_encoder_inputs_lengths = []\n",
        "        for reply in infered_replies:\n",
        "            standard_seq2seq_encoder_inputs_lengths.append(len(reply))\n",
        "            if len(reply) <= self.hparams.encoder_length:\n",
        "                standard_seq2seq_encoder_inputs.append(np.append(reply, (\n",
        "                        [self.hparams.pad_id] * (self.hparams.encoder_length - len(reply)))))\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    \"Inferred\"\n",
        "                    \" reply shouldn't be longer than encoder_input\")\n",
        "        standard_seq2seq_encoder_inputs = np.transpose(\n",
        "            np.array(standard_seq2seq_encoder_inputs))\n",
        "        reward1 = standard_seq2seq_model.reward_ease_of_answering(\n",
        "            standard_seq2seq_encoder_inputs,\n",
        "            standard_seq2seq_encoder_inputs_lengths, dull_responses)\n",
        "        reward2 = 0  # todo\n",
        "        reward3 = 0  # todo\n",
        "        reward = 0.25 * reward1 + 0.25 * reward2 + 0.5 * reward3\n",
        "        return self.train(encoder_inputs, encoder_inputs_lengths, target_labels,\n",
        "                          decoder_inputs, decoder_target_lengths, reward)\n",
        "\n",
        "    def save(self, model_path=None):\n",
        "        if model_path is None:\n",
        "            model_path = self.model_path\n",
        "        model_dir = \"{}/{}\".format(model_path, self.name)\n",
        "        self.saver.save(self.sess, model_dir, global_step=self.global_step)\n",
        "\n",
        "    @staticmethod\n",
        "    def _softmax(x):\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    def _build_optimizer(self, hparams, logits):\n",
        "        # Target labels\n",
        "        #   As described in doc for sparse_softmax_cross_entropy_with_logits,\n",
        "        #   labels should be [batch_size, decoder_target_lengths]\n",
        "        #   instead of [batch_size, decoder_target_lengths, vocab_size].\n",
        "        #   So labels should have indices instead of vocab_size classes.\n",
        "        target_labels = tf.placeholder(tf.int32, shape=(\n",
        "            hparams.batch_size, hparams.decoder_length), name=\"target_labels\")\n",
        "        \n",
        "        # Loss\n",
        "        #   target_labels: [batch_size, decoder_length]\n",
        "        #   logits: [batch_size, decoder_length, vocab_size]\n",
        "        #   crossent: [batch_size, decoder_length]\n",
        "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            labels=target_labels, logits=logits)\n",
        "        \n",
        "        target_weights = tf.sequence_mask(self.decoder_target_lengths, hparams.decoder_length, dtype=logits.dtype)\n",
        "\n",
        "        loss = tf.reduce_sum(\n",
        "          crossent * target_weights) / tf.to_float(hparams.batch_size)\n",
        "        \n",
        "        # Adjust loss with reward.\n",
        "        loss = tf.multiply(loss, self.reward)\n",
        "\n",
        "        # Train\n",
        "        global_step = tf.get_variable(name=\"global_step\", shape=[],\n",
        "                                      dtype=tf.int32,\n",
        "                                      initializer=tf.constant_initializer(0),\n",
        "                                      trainable=False)\n",
        "        \n",
        "        learning_rate = hparams.learning_rate # tf.train.exponential_decay(hparams.learning_rate, global_step, 100, hparams.learning_rate_decay) \n",
        "\n",
        "        # Calculate and clip gradients\n",
        "        params = tf.trainable_variables()\n",
        "        for param in params:\n",
        "          info(\"  {}, {}, {}\".format(param.name, str(param.get_shape()),\n",
        "                                        param.op.device), hparams)\n",
        "        \n",
        "        gradients = tf.gradients(loss, params)\n",
        "        clipped_gradients, _ = tf.clip_by_global_norm(\n",
        "            gradients, hparams.max_gradient_norm)\n",
        "\n",
        "        # Optimization\n",
        "#        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "        device = '/cpu:0'\n",
        "        if has_gpu0():\n",
        "            device = '/gpu:0'\n",
        "            print(\"!!!GPU ENABLED !!!\")\n",
        "        with tf.device(device):\n",
        "            train_op = optimizer.apply_gradients(\n",
        "                zip(clipped_gradients, params), global_step=global_step)\n",
        "        return target_labels, loss, global_step, learning_rate, train_op\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_encoder(hparams, scope):\n",
        "        # Encoder\n",
        "        #   encoder_inputs: [encoder_length, batch_size]\n",
        "        #   This is time major where encoder_length comes\n",
        "        #   first instead of batch_size.\n",
        "        #   encoder_inputs_lengths: [batch_size]\n",
        "        encoder_inputs = tf.placeholder(tf.int32, shape=(\n",
        "            hparams.encoder_length, hparams.batch_size), name=\"encoder_inputs\")\n",
        "        encoder_inputs_lengths = tf.placeholder(tf.int32,\n",
        "                                                shape=hparams.batch_size,\n",
        "                                                name=\"encoder_inputs_lengtsh\")\n",
        "\n",
        "        # Embedding\n",
        "        #   We originally didn't share embedding between encoder and decoder.\n",
        "        #   But now we share it. It makes much easier to calculate rewards.\n",
        "        #   Matrix for embedding: [vocab_size, embedding_size]\n",
        "        #   Should be shared between training and inference.\n",
        "        with tf.variable_scope(scope):\n",
        "            embedding_encoder = tf.get_variable(\"embedding_encoder\",\n",
        "                                                [hparams.vocab_size,\n",
        "                                                 hparams.embedding_size])\n",
        "\n",
        "        # Look up embedding:\n",
        "        #   encoder_inputs: [encoder_length, batch_size]\n",
        "        #   encoder_emb_inputs: [encoder_length, batch_size, embedding_size]\n",
        "        encoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder,\n",
        "                                                    encoder_inputs)\n",
        "\n",
        "        # LSTM cell.\n",
        "        with tf.variable_scope(scope):\n",
        "            # Should be shared between training and inference.\n",
        "            cell_list = []\n",
        "            for _ in range(hparams.num_layers):\n",
        "               cell_list.append(tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
        "            encoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
        "\n",
        "        # Run Dynamic RNN\n",
        "        #   encoder_outputs: [encoder_length, batch_size, num_units]\n",
        "        #   encoder_state: [batch_size, num_units],\n",
        "        #   this is final state of the cell for each batch.\n",
        "        with tf.variable_scope(scope):\n",
        "            encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell,\n",
        "                                                               encoder_emb_inputs,\n",
        "                                                               time_major=True,\n",
        "                                                               dtype=tf.float32,\n",
        "                                                               sequence_length=encoder_inputs_lengths)\n",
        "\n",
        "        return encoder_inputs, encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_training_decoder(hparams, encoder_inputs_lengths,\n",
        "                                encoder_state, encoder_outputs, decoder_cell,\n",
        "                                decoder_emb_inputs, decoder_target_lengths,\n",
        "                                projection_layer):\n",
        "        # Decoder with helper:\n",
        "        #   decoder_emb_inputs: [decoder_length, batch_size, embedding_size]\n",
        "        #   decoder_target_lengths: [batch_size] vector,\n",
        "        #   which represents each target sequence length.\n",
        "        training_helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inputs,\n",
        "                                                            decoder_target_lengths,\n",
        "                                                            time_major=True)\n",
        "\n",
        "        # See https://github.com/tensorflow/tensorflow/issues/11904\n",
        "        if hparams.use_attention:\n",
        "            # Attention\n",
        "            # encoder_outputs is time major, so transopse it to batch major.\n",
        "            # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
        "            attention_encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2])\n",
        "\n",
        "            # Create an attention mechanism\n",
        "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "                hparams.num_units,\n",
        "                attention_encoder_outputs,\n",
        "                memory_sequence_length=encoder_inputs_lengths)\n",
        "\n",
        "            wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "                decoder_cell, attention_mechanism,\n",
        "                attention_layer_size=hparams.num_units)\n",
        "\n",
        "            initial_state = wrapped_decoder_cell.zero_state(hparams.batch_size,\n",
        "                                                            tf.float32).clone(\n",
        "                cell_state=encoder_state)\n",
        "        else:\n",
        "            wrapped_decoder_cell = decoder_cell\n",
        "            initial_state = encoder_state\n",
        "\n",
        "            # Decoder and decode\n",
        "        training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "            wrapped_decoder_cell, training_helper, initial_state,\n",
        "            output_layer=projection_layer)\n",
        "\n",
        "        # Dynamic decoding\n",
        "        #   final_outputs.rnn_output: [batch_size, decoder_length,\n",
        "        #                             vocab_size], list of RNN state.\n",
        "        #   final_outputs.sample_id: [batch_size, decoder_length],\n",
        "        #                            list of argmax of rnn_output.\n",
        "        #   final_state: [batch_size, num_units],\n",
        "        #                list of final state of RNN on decode process.\n",
        "        #   final_sequence_lengths: [batch_size], list of each decoded sequence. \n",
        "        final_outputs, _final_state, _final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n",
        "            training_decoder)\n",
        "\n",
        "        if hparams.debug_verbose:\n",
        "            print(\"rnn_output.shape=\", final_outputs.rnn_output.shape)\n",
        "            print(\"sample_id.shape=\", final_outputs.sample_id.shape)\n",
        "            print(\"final_state=\", _final_state)\n",
        "            print(\"final_sequence_lengths.shape=\",\n",
        "                  _final_sequence_lengths.shape)\n",
        "\n",
        "        logits = final_outputs.rnn_output\n",
        "        return logits, wrapped_decoder_cell, initial_state\n",
        "\n",
        "    def _build_decoder(self, hparams, encoder_inputs_lengths, embedding_encoder,\n",
        "                       encoder_state, encoder_outputs):\n",
        "        # Decoder input\n",
        "        #   decoder_inputs: [decoder_length, batch_size]\n",
        "        #   decoder_target_lengths: [batch_size]\n",
        "        #   This is grand truth target inputs for training.\n",
        "        decoder_inputs = tf.placeholder(tf.int32, shape=(\n",
        "            hparams.decoder_length, hparams.batch_size), name=\"decoder_inputs\")\n",
        "        decoder_target_lengths = tf.placeholder(tf.int32,\n",
        "                                                shape=hparams.batch_size,\n",
        "                                                name=\"decoder_target_lengths\")\n",
        "\n",
        "        # Look up embedding:\n",
        "        #   decoder_inputs: [decoder_length, batch_size]\n",
        "        #   decoder_emb_inp: [decoder_length, batch_size, embedding_size]\n",
        "        decoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder,\n",
        "                                                    decoder_inputs)\n",
        "\n",
        "        # https://stackoverflow.com/questions/39573188/output-projection-in-seq2seq-model-tensorflow\n",
        "        # Internally, a neural network operates on dense vectors of some size,\n",
        "        # often 256, 512 or 1024 floats (let's say 512 for here). \n",
        "        # But at the end it needs to predict a word\n",
        "        # from the vocabulary which is often much larger,\n",
        "        # e.g., 40000 words. Output projection is the final linear layer\n",
        "        # that converts (projects) from the internal representation\n",
        "        #  to the larger one.\n",
        "        # So, for example, it can consist of a 512 x 40000 parameter matrix\n",
        "        # and a 40000 parameter for the bias vector.\n",
        "        projection_layer = layers_core.Dense(hparams.vocab_size, use_bias=False)\n",
        "\n",
        "        # We share this between training and inference.\n",
        "        cell_list = []\n",
        "        for _ in range(hparams.num_layers):\n",
        "           cell_list.append(tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
        "        decoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
        "        \n",
        "#        decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
        "\n",
        "        # Training graph\n",
        "        logits, wrapped_decoder_cell, initial_state = self._build_training_decoder(\n",
        "            hparams, encoder_inputs_lengths, encoder_state, encoder_outputs,\n",
        "            decoder_cell, decoder_emb_inputs, decoder_target_lengths,\n",
        "            projection_layer)\n",
        "\n",
        "        return decoder_inputs, decoder_target_lengths, logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JzDknaQZV-iU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class ChatbotInferenceModel:\n",
        "    def __init__(self, sess, hparams, model_path, scope='ChatbotModel'):\n",
        "        self.sess = sess\n",
        "        # todo remove\n",
        "        self.hparams = hparams\n",
        "\n",
        "        # todo\n",
        "        self.model_path = model_path\n",
        "        self.name = scope\n",
        "\n",
        "        self.encoder_inputs, self.encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder = self._build_encoder(\n",
        "            hparams, scope)\n",
        "        self.decoder_inputs, self.decoder_target_lengths, self.replies, self.beam_replies, self.infer_logits = self._build_decoder(\n",
        "            hparams, self.encoder_inputs_lengths, embedding_encoder,\n",
        "            encoder_state, encoder_outputs)\n",
        "\n",
        "        self.reward = tf.placeholder(tf.float32, name=\"reward\")\n",
        "\n",
        "        # we can't use variable length here, \n",
        "        # because tiled_batch requires constant length.\n",
        "        self.batch_size = 1\n",
        "\n",
        "        # Initialize saver after model created\n",
        "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
        "\n",
        "    def restore(self):\n",
        "        ckpt = tf.train.get_checkpoint_state(self.model_path)\n",
        "        if ckpt:\n",
        "            last_model = ckpt.model_checkpoint_path\n",
        "            self.saver.restore(self.sess, last_model)\n",
        "            return True\n",
        "        else:\n",
        "            print(\"Created fresh model.\")\n",
        "            return False\n",
        "\n",
        "    def infer(self, encoder_inputs, encoder_inputs_lengths):\n",
        "        # Should not call this when beam search enabled.\n",
        "        assert(self.hparams.beam_width == 0)\n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "        }\n",
        "        replies = self.sess.run(self.replies, feed_dict=inference_feed_dict)\n",
        "        return replies\n",
        "\n",
        "    def infer_beam_search(self, encoder_inputs, encoder_inputs_lengths):\n",
        "        # Should not call this when beam search disabled.\n",
        "        assert(self.hparams.beam_width > 0)      \n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "        }\n",
        "        replies = self.sess.run(self.beam_replies,\n",
        "                                feed_dict=inference_feed_dict)\n",
        "        return replies\n",
        "     \n",
        "    # imakoko\n",
        "    def infer_mi(self, swapped_model, encoder_inputs, encoder_inputs_lengths):\n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "        }\n",
        "        beam_replies = self.sess.run(self.beam_replies, feed_dict=inference_feed_dict)\n",
        "        # beam_replis [batch_size, length, , batch_width]\n",
        "        # for now we assume encoder_inputs is batch_size = 1\n",
        "        \n",
        "        swapped_encoder_inputs = beam_replies[0]\n",
        "        # beam_width = batch_size\n",
        "        swapped_batch_size = swapped_encoder_inputs.shape[1]\n",
        "        \n",
        "        # beam_replies can be shorten less than decoder_output_legth, so we pad them.\n",
        "        paddings = tf.constant([[0, self.hparams.encoder_length - swapped_encoder_inputs.shape[0],], [0, 0]])\n",
        "        swapped_encoder_inputs = swapped_model.sess.run(tf.pad(swapped_encoder_inputs, paddings, \"CONSTANT\", constant_values=self.hparams.pad_id))\n",
        "        swapped_encoder_inputs_lengths = np.empty(swapped_batch_size, dtype=np.int)\n",
        "        for i in range(swapped_batch_size):\n",
        "          swapped_encoder_inputs_lengths[i] = swapped_encoder_inputs.shape[0]\n",
        "        \n",
        "        return swapped_model.infer_beam_search(swapped_encoder_inputs, swapped_encoder_inputs_lengths)\n",
        "        # todo make correct length\n",
        "#        for repy in beam_replies:\n",
        "          # logits from swapped_model for this reply\n",
        "          # cals prob for in original encoder_input\n",
        "      \n",
        "\n",
        "    def log_prob(self, encoder_inputs, encoder_inputs_lengths, expected_output):\n",
        "        \"\"\"Return sum of log probability of given\n",
        "           one specific expected_output for encoder_inputs.\n",
        "    \n",
        "        Args:\n",
        "            encoder_inputs: [encoder_length, batch_size], eg) tweets\n",
        "            expected_output: [1, decoder_length or less than decoder_length],\n",
        "            eg) One reply.\n",
        "    \n",
        "        Returns:\n",
        "            Return log probablity of expected output for given encoder inputs.\n",
        "            eg) sum of log probability of reply \"Good\" when given [\"How are you?\",\n",
        "             \"What's up?\"]\n",
        "        \"\"\"\n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths\n",
        "        }\n",
        "\n",
        "        # Logits\n",
        "        #   logits_value: [batch_size, actual_decoder_length, vocab_size]\n",
        "        logits_batch_value = self.sess.run(self.infer_logits,\n",
        "                                           feed_dict=inference_feed_dict)\n",
        "\n",
        "        sum_p = []\n",
        "        # For each batch: [actual_decoder_length, vocab_size]\n",
        "        for logits in logits_batch_value:\n",
        "            p = 1\n",
        "            # Note that expected_output and logits don't always have\n",
        "            # same length, but zip takes care of the case.\n",
        "            for word_id, logit in zip(expected_output, logits):\n",
        "                # Apply softmax first, see definition of softmax.\n",
        "                norm = (self._softmax(logit))[word_id]\n",
        "                p *= norm\n",
        "            p = np.log(p)\n",
        "            sum_p.append(p)\n",
        "        ret = np.sum(sum_p) / len(sum_p)\n",
        "        return ret\n",
        "\n",
        "    def reward_ease_of_answering(self, encoder_inputs, encoder_inputs_lengths,\n",
        "                                 expected_outputs):\n",
        "        \"\"\" Return reward for ease of answering. \n",
        "            See Deep Reinforcement Learning for Dialogue Generation\n",
        "            for more details.\n",
        "    \n",
        "        Args:\n",
        "            encoder_inputs: [encoder_length, batch_size], eg) tweets\n",
        "            expected_outputs: [number of pre-defined dull responses,\n",
        "            decoder_length or less than decoder_length].\n",
        "            eg) [[\"I'm\", \"Good\"], [\"fine\"]]\n",
        "    \n",
        "        Returns:\n",
        "            Return reward for ease of answering.\n",
        "            Note that this can be calculated\n",
        "            by calling log_prob function for each dull response,\n",
        "            but this function is more efficient\n",
        "            because this calculated the reward at once.\n",
        "        \"\"\"\n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths\n",
        "        }\n",
        "\n",
        "        # Logits\n",
        "        #   logits_value: [batch_size, actual_decoder_length, vocab_size]\n",
        "        logits_batch_value = self.sess.run(self.infer_logits,\n",
        "                                           feed_dict=inference_feed_dict)\n",
        "\n",
        "        batch_sum_p = []\n",
        "        # For each batch: [actual_decoder_length, vocab_size]\n",
        "        for logits in logits_batch_value:\n",
        "            sum_p = []\n",
        "            for expected_output in expected_outputs:\n",
        "                p = 1\n",
        "                # Note that expected_output and logits don't\n",
        "                # always have same length, but zip takes care of the case.\n",
        "                for word_id, logit in zip(expected_output, logits):\n",
        "                    # Apply softmax first, see definition of softmax.\n",
        "                    norm = (self._softmax(logit))[word_id]\n",
        "                    p *= norm\n",
        "                p = np.log(p) / len(expected_output)\n",
        "                sum_p.append(p)\n",
        "            one_batch_p = np.sum(sum_p)\n",
        "            batch_sum_p.append(one_batch_p)\n",
        "        ret = np.sum(batch_sum_p) / len(batch_sum_p)\n",
        "        return -ret\n",
        "\n",
        "    @staticmethod\n",
        "    def _softmax(x):\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_encoder(hparams, scope):\n",
        "        # Encoder\n",
        "        #   encoder_inputs: [encoder_length, batch_size]\n",
        "        #   This is time major where encoder_length\n",
        "        #   comes first instead of batch_size.\n",
        "        #   encoder_inputs_lengths: [batch_size]\n",
        "        encoder_inputs = tf.placeholder(tf.int32,\n",
        "                                        shape=[hparams.encoder_length, None],\n",
        "                                        name=\"encoder_inputs\")\n",
        "        encoder_inputs_lengths = tf.placeholder(tf.int32, shape=[None],\n",
        "                                                name=\"encoder_inputs_lengths\")\n",
        "\n",
        "        # Embedding\n",
        "        #   We originally didn't share embedding between encoder and decoder.\n",
        "        #   But now we share it. It makes much easier to calculate rewards.\n",
        "        #   Matrix for embedding: [vocab_size, embedding_size]\n",
        "        #   Should be shared between training and inference.\n",
        "        with tf.variable_scope(scope):\n",
        "            embedding_encoder = tf.get_variable(\"embedding_encoder\",\n",
        "                                                [hparams.vocab_size,\n",
        "                                                 hparams.embedding_size])\n",
        "\n",
        "        # Look up embedding:\n",
        "        #   encoder_inputs: [encoder_length, batch_size]\n",
        "        #   encoder_emb_inputs: [encoder_length, batch_size, embedding_size]\n",
        "        encoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder,\n",
        "                                                    encoder_inputs)\n",
        "\n",
        "        # LSTM cell.\n",
        "        with tf.variable_scope(scope):\n",
        "            # Should be shared between training and inference.\n",
        "#            encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
        "            cell_list = []\n",
        "            for _ in range(hparams.num_layers):\n",
        "               cell_list.append(tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
        "            encoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
        "\n",
        "        # Run Dynamic RNN\n",
        "        #   encoder_outputs: [encoder_length, batch_size, num_units]\n",
        "        #   encoder_state: [batch_size, num_units],\n",
        "        #   this is final state of the cell for each batch.\n",
        "        with tf.variable_scope(scope):\n",
        "            encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell,\n",
        "                                                               encoder_emb_inputs,\n",
        "                                                               time_major=True,\n",
        "                                                               dtype=tf.float32,\n",
        "                                                               sequence_length=encoder_inputs_lengths)\n",
        "\n",
        "        return encoder_inputs, encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_greedy_inference(hparams, embedding_encoder, encoder_state,\n",
        "                                encoder_inputs_lengths, encoder_outputs,\n",
        "                                decoder_cell, projection_layer):\n",
        "        if hparams.beam_width > 0:\n",
        "          return None, None\n",
        "        \n",
        "        # Greedy decoder\n",
        "        dynamic_batch_size = tf.shape(encoder_inputs_lengths)[0]\n",
        "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
        "            embedding_encoder,\n",
        "            tf.fill([dynamic_batch_size], hparams.sos_id), hparams.eos_id)\n",
        "\n",
        "        # See https://github.com/tensorflow/tensorflow/issues/11904\n",
        "        if hparams.use_attention:\n",
        "            # Attention\n",
        "            # encoder_outputs is time major, so transopse it to batch major.\n",
        "            # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
        "            attention_encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2])\n",
        "\n",
        "            # Create an attention mechanism\n",
        "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "                hparams.num_units,\n",
        "                attention_encoder_outputs,\n",
        "                memory_sequence_length=encoder_inputs_lengths)\n",
        "\n",
        "            wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "                decoder_cell, attention_mechanism,\n",
        "                attention_layer_size=hparams.num_units)\n",
        "\n",
        "            initial_state = wrapped_decoder_cell.zero_state(dynamic_batch_size,\n",
        "                                                            tf.float32).clone(\n",
        "                cell_state=encoder_state)\n",
        "        else:\n",
        "            wrapped_decoder_cell = decoder_cell\n",
        "            initial_state = encoder_state\n",
        "\n",
        "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "            wrapped_decoder_cell, inference_helper, initial_state,\n",
        "            output_layer=projection_layer)\n",
        "\n",
        "        # len(inferred_reply) is lte encoder_length,\n",
        "        # because we are targeting tweet (140 for each tweet)\n",
        "        # Also by doing this,\n",
        "        # we can pass the reply to other seq2seq w/o shorten it.\n",
        "        maximum_iterations = hparams.encoder_length\n",
        "\n",
        "        # Dynamic decoding\n",
        "        outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "            inference_decoder, maximum_iterations=maximum_iterations)\n",
        "        replies = outputs.sample_id\n",
        "\n",
        "        # We use infer_logits instead of logits when calculating log_prob,\n",
        "        # because infer_logits doesn't require decoder_target_lengths input.\n",
        "        infer_logits = outputs.rnn_output\n",
        "        return infer_logits, replies\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_beam_search_inference(hparams, encoder_inputs_lengths,\n",
        "                                     embedding_encoder, encoder_state,\n",
        "                                     encoder_outputs, decoder_cell,\n",
        "                                     projection_layer):\n",
        "      \n",
        "        if hparams.beam_width == 0:\n",
        "          return None\n",
        "      \n",
        "        dynamic_batch_size = tf.shape(encoder_inputs_lengths)[0]\n",
        "        # https://github.com/tensorflow/tensorflow/issues/11904\n",
        "        if hparams.use_attention:\n",
        "            # Attention\n",
        "            # encoder_outputs is time major, so transopse it to batch major.\n",
        "            # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
        "            attention_encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2])\n",
        "\n",
        "            tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(\n",
        "                attention_encoder_outputs, multiplier=hparams.beam_width)\n",
        "            tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(\n",
        "                encoder_state, multiplier=hparams.beam_width)\n",
        "            tiled_encoder_inputs_lengths = tf.contrib.seq2seq.tile_batch(\n",
        "                encoder_inputs_lengths, multiplier=hparams.beam_width)\n",
        "\n",
        "            # Create an attention mechanism\n",
        "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "                hparams.num_units, tiled_encoder_outputs,\n",
        "                memory_sequence_length=tiled_encoder_inputs_lengths)\n",
        "\n",
        "            wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "                decoder_cell, attention_mechanism,\n",
        "                attention_layer_size=hparams.num_units)\n",
        "\n",
        "            decoder_initial_state = wrapped_decoder_cell.zero_state(\n",
        "                dtype=tf.float32,\n",
        "                batch_size=dynamic_batch_size * hparams.beam_width)\n",
        "            decoder_initial_state = decoder_initial_state.clone(\n",
        "                cell_state=tiled_encoder_final_state)\n",
        "        else:\n",
        "            wrapped_decoder_cell = decoder_cell\n",
        "            decoder_initial_state = tf.contrib.seq2seq.tile_batch(encoder_state,\n",
        "                                                                  multiplier=hparams.beam_width)\n",
        "\n",
        "        # len(inferred_reply) is lte encoder_length,\n",
        "        # because we are targeting tweet (140 for each tweet)\n",
        "        # Also by doing this,\n",
        "        # we can pass the reply to other seq2seq w/o shorten it.\n",
        "        maximum_iterations = hparams.encoder_length\n",
        "\n",
        "        inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
        "            cell=wrapped_decoder_cell,\n",
        "            embedding=embedding_encoder,\n",
        "            start_tokens=tf.fill([dynamic_batch_size], hparams.sos_id),\n",
        "            end_token=hparams.eos_id,\n",
        "            initial_state=decoder_initial_state,\n",
        "            beam_width=hparams.beam_width,\n",
        "            output_layer=projection_layer,\n",
        "            length_penalty_weight=0.0)\n",
        "\n",
        "        # Dynamic decoding\n",
        "        beam_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "            inference_decoder, maximum_iterations=maximum_iterations)\n",
        "        beam_replies = beam_outputs.predicted_ids\n",
        "        return beam_replies\n",
        "\n",
        "    def _build_decoder(self, hparams, encoder_inputs_lengths, embedding_encoder,\n",
        "                       encoder_state, encoder_outputs):\n",
        "        # Decoder input\n",
        "        #   decoder_inputs: [decoder_length, batch_size]\n",
        "        #   decoder_target_lengths: [batch_size]\n",
        "        #   This is grand truth target inputs for training.\n",
        "        decoder_inputs = tf.placeholder(tf.int32,\n",
        "                                        shape=[hparams.decoder_length, None],\n",
        "                                        name=\"decoder_inputs\")\n",
        "        decoder_target_lengths = tf.placeholder(tf.int32, shape=[None],\n",
        "                                                name=\"decoder_target_lengths\")\n",
        "\n",
        "        # https://stackoverflow.com/questions/39573188/output-projection-in-seq2seq-model-tensorflow\n",
        "        # Internally, a neural network operates on dense vectors of some size,\n",
        "        # often 256, 512 or 1024 floats (let's say 512 for here). \n",
        "        # But at the end it needs to predict a word\n",
        "        # from the vocabulary which is often much larger,\n",
        "        # e.g., 40000 words. Output projection is the final linear\n",
        "        # layer that converts (projects) from the internal\n",
        "        # representation to the larger one.\n",
        "        # So, for example, it can consist of a 512 x 40000 parameter\n",
        "        # matrix and a 40000 parameter for the bias vector.\n",
        "        projection_layer = layers_core.Dense(hparams.vocab_size, use_bias=False)\n",
        "\n",
        "        # We share this between training and inference.\n",
        "#        decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
        "        cell_list = []\n",
        "        for _ in range(hparams.num_layers):\n",
        "           cell_list.append(tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
        "        decoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
        "        \n",
        "\n",
        "        # Greedy Inference graph\n",
        "        infer_logits, replies = self._build_greedy_inference(hparams,\n",
        "                                                             embedding_encoder,\n",
        "                                                             encoder_state,\n",
        "                                                             encoder_inputs_lengths,\n",
        "                                                             encoder_outputs,\n",
        "                                                             decoder_cell,\n",
        "                                                             projection_layer)\n",
        "\n",
        "        # Beam Search Inference graph\n",
        "        beam_replies = self._build_beam_search_inference(hparams,\n",
        "                                                         encoder_inputs_lengths,\n",
        "                                                         embedding_encoder,\n",
        "                                                         encoder_state,\n",
        "                                                         encoder_outputs,\n",
        "                                                         decoder_cell,\n",
        "                                                         projection_layer)\n",
        "\n",
        "        return decoder_inputs, decoder_target_lengths, replies, beam_replies, infer_logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ul5WBjSF3vy9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class InferenceHelper:\n",
        "    def __init__(self, model, vocab, rev_vocab):\n",
        "        self.model = model\n",
        "        self.vocab = vocab\n",
        "        self.rev_vocab = rev_vocab\n",
        "\n",
        "    def inferences(self, tweet):\n",
        "        encoder_inputs, encoder_inputs_lengths = self.create_inference_input(\n",
        "            tweet)\n",
        "        if self.model.hparams.beam_width == 0:\n",
        "          replies = self.model.infer(encoder_inputs, encoder_inputs_lengths)\n",
        "          ids = replies[0].tolist()\n",
        "          return [self.ids_to_words(ids)]\n",
        "        else:\n",
        "          beam_replies = self.model.infer_beam_search(encoder_inputs,\n",
        "                                                      encoder_inputs_lengths)\n",
        "\n",
        "          return [self.ids_to_words(beam_replies[0][:, i]) for i in range(self.model.hparams.beam_width)]\n",
        "\n",
        "    def print_inferences(self, tweet):\n",
        "        print(tweet)\n",
        "        for i, reply in enumerate(self.inferences(tweet)):\n",
        "            print(\"    [{}]{}\".format(i, reply))\n",
        "\n",
        "    def words_to_ids(self, words):\n",
        "        ids = []\n",
        "        for word in words:\n",
        "            if word in self.vocab:\n",
        "                ids.append(self.vocab[word])\n",
        "            else:\n",
        "                ids.append(self.model.hparams.unk_id)\n",
        "        return ids\n",
        "\n",
        "    def ids_to_words(self, ids):\n",
        "        words = \"\"\n",
        "        for id in ids:\n",
        "            words += self.rev_vocab[id]\n",
        "        return words\n",
        "\n",
        "    def create_inference_input(self, text):\n",
        "        inference_encoder_inputs = np.empty((self.model.hparams.encoder_length, 1),\n",
        "                                            dtype=np.int)\n",
        "        inference_encoder_inputs_lengths = np.empty(1, dtype=np.int)\n",
        "        text = TrainDataGenerator.sanitize_line(text)\n",
        "        tagger = MeCab.Tagger(\"-Owakati\")\n",
        "        words = tagger.parse(text).split()\n",
        "        ids = self.words_to_ids(words)\n",
        "        ids = ids[:self.model.hparams.encoder_length]\n",
        "        len_ids = len(ids)\n",
        "        ids.extend([self.model.hparams.pad_id] * (self.model.hparams.encoder_length - len(ids)))\n",
        "        for i in range(1):\n",
        "            inference_encoder_inputs[:, i] = np.array(ids, dtype=np.int)\n",
        "            inference_encoder_inputs_lengths[i] = len_ids\n",
        "        return inference_encoder_inputs, inference_encoder_inputs_lengths\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DQg8kU-2Dr-q",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Helper functions to test\n",
        "def make_test_training_data(hparams):\n",
        "    train_encoder_inputs = np.empty(\n",
        "        (hparams.encoder_length, hparams.batch_size), dtype=np.int)\n",
        "    train_encoder_inputs_lengths = np.empty(hparams.batch_size, dtype=np.int)\n",
        "    training_target_labels = np.empty(\n",
        "        (hparams.batch_size, hparams.decoder_length), dtype=np.int)\n",
        "    training_decoder_inputs = np.empty(\n",
        "        (hparams.decoder_length, hparams.batch_size), dtype=np.int)\n",
        "\n",
        "    # We keep first tweet to validate inference.\n",
        "    first_tweet = None\n",
        "\n",
        "    for i in range(hparams.batch_size):\n",
        "        # Tweet\n",
        "        tweet = np.random.randint(low=0, high=hparams.vocab_size,\n",
        "                                  size=hparams.encoder_length)\n",
        "        train_encoder_inputs[:, i] = tweet\n",
        "        train_encoder_inputs_lengths[i] = len(tweet)\n",
        "        # Reply\n",
        "        #   Note that low = 2, as 0 and 1 are reserved.\n",
        "        reply = np.random.randint(low=2, high=hparams.vocab_size,\n",
        "                                  size=hparams.decoder_length - 1)\n",
        "\n",
        "        training_target_label = np.concatenate((reply, np.array([hparams.eos_id])))\n",
        "        training_target_labels[i] = training_target_label\n",
        "\n",
        "        training_decoder_input = np.concatenate(([hparams.sos_id], reply))\n",
        "        training_decoder_inputs[:, i] = training_decoder_input\n",
        "\n",
        "        if i == 0:\n",
        "            first_tweet = tweet\n",
        "            info(\"0th tweet={}\".format(tweet), hparams)\n",
        "            info(\"0th reply_with_eos_suffix={}\".format(training_target_label),\n",
        "                 hparams)\n",
        "            info(\"0th reply_with_sos_prefix={}\".format(training_decoder_input),\n",
        "                 hparams)\n",
        "\n",
        "        info(\"Tweets\", hparams)\n",
        "        info(train_encoder_inputs, hparams)\n",
        "        info(\"Replies\", hparams)\n",
        "        info(training_target_labels, hparams)\n",
        "        info(training_decoder_inputs, hparams)\n",
        "    return first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs\n",
        "\n",
        "\n",
        "def test_training(test_hparams, model, infer_model):\n",
        "    if test_hparams.use_attention:\n",
        "        print(\"==== training model[attention] ====\")\n",
        "    else:\n",
        "        print(\"==== training model ====\")\n",
        "    first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs = make_test_training_data(\n",
        "        test_hparams)\n",
        "    for i in range(test_hparams.num_train_steps):\n",
        "        _ = model.train(train_encoder_inputs,\n",
        "                        train_encoder_inputs_lengths,\n",
        "                        training_target_labels,\n",
        "                        training_decoder_inputs,\n",
        "                        np.ones(test_hparams.batch_size,\n",
        "                                dtype=int) * test_hparams.decoder_length)\n",
        "        if i % 5 == 0 and test_hparams.debug_verbose:\n",
        "            print('.', end='')\n",
        "\n",
        "        if i % 15 == 0:\n",
        "            model.save()\n",
        "\n",
        "    inference_encoder_inputs = np.empty((test_hparams.encoder_length, 1),\n",
        "                                        dtype=np.int)\n",
        "    inference_encoder_inputs_lengths = np.empty(1, dtype=np.int)\n",
        "    for i in range(1):\n",
        "        inference_encoder_inputs[:, i] = first_tweet\n",
        "        inference_encoder_inputs_lengths[i] = len(first_tweet)\n",
        "\n",
        "    # testing \n",
        "    log_prob54 = infer_model.log_prob(inference_encoder_inputs,\n",
        "                                      inference_encoder_inputs_lengths,\n",
        "                                      np.array([5, 4]))\n",
        "    log_prob65 = infer_model.log_prob(inference_encoder_inputs,\n",
        "                                      inference_encoder_inputs_lengths,\n",
        "                                      np.array([6, 5]))\n",
        "    print(\"log_prob for 54\", log_prob54)\n",
        "    print(\"log_prob for 65\", log_prob65)\n",
        "\n",
        "    reward = infer_model.reward_ease_of_answering(inference_encoder_inputs,\n",
        "                                                  inference_encoder_inputs_lengths,\n",
        "                                                  np.array([[5], [6]]))\n",
        "    print(\"reward=\", reward)\n",
        "\n",
        "    if test_hparams.debug_verbose:\n",
        "        print(inference_encoder_inputs)\n",
        "    replies = infer_model.infer(inference_encoder_inputs,\n",
        "                                inference_encoder_inputs_lengths)\n",
        "    print(\"Infered replies\", replies[0])\n",
        "    print(\"Expected replies\", training_target_labels[0])\n",
        "\n",
        "def create_train_infer_models(graph, sess, hparams, force_restore=False):\n",
        "    with graph.as_default():\n",
        "        with tf.variable_scope('root'):\n",
        "            model = ChatbotModel(sess, hparams, model_path=hparams.model_path)\n",
        "           \n",
        "        with tf.variable_scope('root', reuse=True):\n",
        "            infer_model = ChatbotInferenceModel(sess, hparams,\n",
        "                                                model_path=hparams.model_path)\n",
        "            restored = model.restore()\n",
        "            if not restored:\n",
        "                if force_restore:\n",
        "                    raise Exception(\"Oops, couldn't restore\")\n",
        "                else:\n",
        "                    sess.run(tf.global_variables_initializer())\n",
        "        return model, infer_model\n",
        "\n",
        "\n",
        "def create_train_infer_models_in_graphs(train_graph, train_sess, infer_graph,\n",
        "                                        infer_sess, hparams):\n",
        "    with train_graph.as_default():\n",
        "        with tf.variable_scope('root'):\n",
        "            model = ChatbotModel(train_sess, hparams, model_path=hparams.model_path)\n",
        "            if not model.restore():\n",
        "                train_sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # note that infer_model is not sharing variable with training model.\n",
        "    with infer_graph.as_default():\n",
        "        with tf.variable_scope('root'):\n",
        "            infer_model = ChatbotInferenceModel(infer_sess, hparams,\n",
        "                                                model_path=hparams.model_path)\n",
        "\n",
        "    return model, infer_model\n",
        "\n",
        "\n",
        "def test_multiple_models_training():\n",
        "    first_tweet, train_encoder_inputs, train_encoder_inputs_length, training_target_labels, training_decoder_inputs = make_test_training_data(\n",
        "        test_hparams)\n",
        "    \n",
        "    hparams1 = copy.deepcopy(test_hparams).override_from_dict({'model_path': p(ModelDirectory.test_multiple1.value)})\n",
        "    hparams2 = copy.deepcopy(test_hparams).override_from_dict({'model_path': p(ModelDirectory.test_multiple2.value)})\n",
        "    \n",
        "\n",
        "    graph1 = tf.Graph()\n",
        "    sess1 = tf.Session(graph=graph1)\n",
        "    model, infer_model = create_train_infer_models(graph1, sess1, hparams1)\n",
        "    test_training(test_hparams, model, infer_model)\n",
        "\n",
        "    graph2 = tf.Graph()\n",
        "    sess2 = tf.Session(graph=graph2)\n",
        "    model2, infer_model2 = create_train_infer_models(graph2, sess2, hparams2)\n",
        "\n",
        "    test_training(test_hparams, model2, infer_model2)\n",
        "    dull_responses = [[4, 6, 6], [5, 5]]\n",
        "    model2.train_with_reward(infer_model2, infer_model, train_encoder_inputs,\n",
        "                             train_encoder_inputs_length,\n",
        "                             training_target_labels, training_decoder_inputs,\n",
        "                             np.ones((test_hparams.batch_size),\n",
        "                                     dtype=int) * test_hparams.decoder_length,\n",
        "                             dull_responses)\n",
        "\n",
        "    # comment out until https://github.com/tensorflow/tensorflow/issues/10731 is fixed\n",
        "    graph3 = tf.Graph()\n",
        "    sess3 = tf.Session(graph=graph3)\n",
        "#    hparams3 = copy.deepcopy(test_hparams).override_from_dict({'model_path': p(ModelDirectory.test_multiple3.value), 'use_attention': True})\n",
        "#    model3, infer_model3 = create_train_infer_models(graph3, sess3, hparams3)    \n",
        "#    test_training(test_attention_hparams, model3, infer_model3)        \n",
        "\n",
        "\n",
        "def test_save_restore_multiple_models_training():\n",
        "  \n",
        "    for d in [ModelDirectory.test_multiple1, ModelDirectory.test_multiple2, ModelDirectory.test_multiple3]:\n",
        "      shutil.rmtree(p(d.value))\n",
        "      os.makedirs(p(d.value), exist_ok=True)\n",
        "\n",
        "    # Fresh model\n",
        "    test_multiple_models_training()\n",
        "\n",
        "    # Saved model\n",
        "    test_multiple_models_training()\n",
        "\n",
        "\n",
        "def test_distributed_pattern(hparams):\n",
        "  \n",
        "    for d in [hparams.model_path]:\n",
        "      shutil.rmtree(p(d), ignore_errors=True)\n",
        "      os.makedirs(p(d), exist_ok=True)\n",
        "\n",
        "    print('==== test_distributed_pattern[{} {}] ===='.format('attention' if hparams.use_attention else '', 'beam' if hparams.beam_width > 0 else ''))\n",
        "        \n",
        "    first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs = make_test_training_data(\n",
        "        hparams)\n",
        "\n",
        "    train_graph = tf.Graph()\n",
        "    infer_graph = tf.Graph()\n",
        "    train_sess = tf.Session(graph=train_graph)\n",
        "    infer_sess = tf.Session(graph=infer_graph)\n",
        "    \n",
        "\n",
        "    model, infer_model = create_train_infer_models_in_graphs(train_graph,\n",
        "                                                             train_sess,\n",
        "                                                             infer_graph,\n",
        "                                                             infer_sess,\n",
        "                                                             hparams)\n",
        "\n",
        "    for i in range(hparams.num_train_steps):\n",
        "        _ = model.train(train_encoder_inputs,\n",
        "                        train_encoder_inputs_lengths,\n",
        "                        training_target_labels,\n",
        "                        training_decoder_inputs,\n",
        "                        np.ones(hparams.batch_size,\n",
        "                                dtype=int) * hparams.decoder_length)\n",
        "\n",
        "    model.save()\n",
        "\n",
        "    inference_encoder_inputs = np.empty((hparams.encoder_length, 1),\n",
        "                                        dtype=np.int)\n",
        "    inference_encoder_inputs_lengths = np.empty(1, dtype=np.int)\n",
        "\n",
        "    inference_encoder_inputs[:, 0] = first_tweet\n",
        "    inference_encoder_inputs_lengths[0] = len(first_tweet)\n",
        "\n",
        "    infer_model.restore()\n",
        "    if hparams.beam_width == 0:\n",
        "      replies = infer_model.infer(inference_encoder_inputs,\n",
        "                                  inference_encoder_inputs_lengths)\n",
        "      print(\"Inferred replies\", replies[0])\n",
        "\n",
        "    if hparams.beam_width > 0:\n",
        "      beam_replies = infer_model.infer_beam_search(inference_encoder_inputs,\n",
        "                                                   inference_encoder_inputs_lengths)\n",
        "      print(\"Inferred replies candidate0\", beam_replies[0][:, 0])\n",
        "      print(\"Inferred replies candidate1\", beam_replies[0][:, 1])\n",
        "    print(\"Expected replies\", training_target_labels[0])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W_ciBCflZq5o",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "test_save_restore_multiple_models_training()\n",
        "\n",
        "def test_distributed_one(enable_beam, enable_attention):\n",
        "  hparams = copy.deepcopy(test_hparams).override_from_dict({\n",
        "      'model_path': p(ModelDirectory.test_distributed.value),\n",
        "      'use_attention': enable_attention,\n",
        "      'beam_width': 2 if enable_beam else 0\n",
        "  })\n",
        "  test_distributed_pattern(hparams)\n",
        "  \n",
        "test_distributed_one(enable_beam=False, enable_attention=False)\n",
        "test_distributed_one(enable_beam=False, enable_attention=True)\n",
        "test_distributed_one(enable_beam=True, enable_attention=False)\n",
        "test_distributed_one(enable_beam=False, enable_attention=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kxeWpXO5FThm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def download_file_if_necessary(file_name):\n",
        "    if os.path.exists(file_name):\n",
        "        return\n",
        "    print(\"downloading {}...\".format(file_name))\n",
        "    content = read_file_from_drive(file_name)\n",
        "    f = open(file_name, 'w')\n",
        "    f.write(content)\n",
        "    f.close()\n",
        "    print(\"downloaded\")\n",
        "\n",
        "\n",
        "def read_file_from_drive(file_name):\n",
        "    seq2seq_data_dir_id = GoogleDriveFolder.root.value\n",
        "    file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(\n",
        "        seq2seq_data_dir_id)}).GetList()\n",
        "    found = [file for file in file_list if file['title'] == file_name]\n",
        "    if found:\n",
        "        downloaded = drive.CreateFile({'id': found[0]['id']})\n",
        "        return downloaded.GetContentString()\n",
        "    else:\n",
        "        raise ValueError(\"file {} not found.\".format(file_name))\n",
        "\n",
        "\n",
        "def read_file(file_name):\n",
        "    f = open(file_name)\n",
        "    data = f.read()\n",
        "    f.close()\n",
        "    return data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W3nUhj80H6BE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def download_model_data_if_necessary(drive, model_folder_in_drive, model_path):\n",
        "    if drive is None:\n",
        "        return\n",
        "    file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(\n",
        "        model_folder_in_drive)}).GetList()\n",
        "    if not os.path.exists(model_path):\n",
        "        os.makedirs(model_path)\n",
        "    for file in file_list:\n",
        "        print(\"Downloading \", file['title'], \"...\", end='')\n",
        "        target_file = \"{}/{}\".format(model_path, file['title'])\n",
        "        if not os.path.exists(target_file):\n",
        "            file.GetContentFile(\"{}/{}\".format(model_path, file['title']))\n",
        "        print(\"done\")\n",
        "\n",
        "def generic_train_loop(train_feed_data, val_feed_data, local_vocab, local_rev_vocab,\n",
        "                       hparams, generate_models_func,\n",
        "                       inference_hook_func, tweets, drive=None, short_loop=False):\n",
        "    def save_and_infer():\n",
        "        model.save()\n",
        "        inference_hook_func(infer_model)\n",
        "        helper = InferenceHelper(infer_model, local_vocab, local_rev_vocab)\n",
        "        print(\"==== {} ====\".format(global_step))\n",
        "        for tweet in tweets:\n",
        "          helper.print_inferences(tweet)\n",
        "        \n",
        "\n",
        "    print(\"generic train loop {}:{}\".format(drive, hparams.model_folder_in_drive))\n",
        "    download_model_data_if_necessary(drive, hparams.model_folder_in_drive, hparams.model_path)\n",
        "\n",
        "    graph, sess, model, infer_model = generate_models_func(hparams)\n",
        "    writer = tf.summary.FileWriter(hparams.model_path, graph)\n",
        "\n",
        "    with graph.as_default():\n",
        "        train_data_iterator = train_feed_data.make_one_shot_iterator()\n",
        "        val_data_iterator = val_feed_data.make_one_shot_iterator()\n",
        "\n",
        "        last_saved_time = datetime.datetime.now()\n",
        "        last_time = datetime.datetime.now()\n",
        "        x = []\n",
        "        y = []\n",
        "        for i in range(hparams.num_train_steps):\n",
        "            train_data = sess.run(train_data_iterator.get_next())\n",
        "            enc_input_index = 0\n",
        "            enc_input_length_index = 1\n",
        "            dec_input_index = 3\n",
        "            dec_input_length_index = 4\n",
        "            \n",
        "            global_step, learning_rate, summary = model.train(train_data[enc_input_index], train_data[enc_input_length_index],\n",
        "                                               train_data[2], train_data[dec_input_index],\n",
        "                                               train_data[dec_input_length_index])\n",
        "            writer.add_summary(summary, global_step)\n",
        "\n",
        "\n",
        "            if short_loop and i == 2:\n",
        "                save_and_infer()\n",
        "                break\n",
        "            elif i != 0 and i % 15 == 0:\n",
        "                save_and_infer()\n",
        "                val_data = sess.run(val_data_iterator.get_next())\n",
        "                val_loss, val_loss_log = model.batch_loss(val_data[enc_input_index],\n",
        "                                                          val_data[enc_input_length_index],\n",
        "                                                          val_data[2],\n",
        "                                                          val_data[dec_input_index],\n",
        "                                                          val_data[dec_input_length_index])\n",
        "                writer.add_summary(val_loss_log, global_step)\n",
        "                print(\"validation loss\", val_loss)\n",
        "                print(\"learning rate\", learning_rate)\n",
        "                delta = (\n",
        "                                datetime.datetime.now() - last_time).total_seconds() * 1000\n",
        "                print(\n",
        "                    \"{:.2f} msec/data\".format(delta / hparams.batch_size / 15))\n",
        "                last_time = datetime.datetime.now()\n",
        "                x.append(global_step)\n",
        "                y.append(val_loss)\n",
        "            else:\n",
        "                print('.', end='')\n",
        "            now = datetime.datetime.now()\n",
        "            if (now - last_saved_time).total_seconds() > 3600 and drive is not None:\n",
        "#            if (now - last_saved_time).total_seconds() > 120 and drive is not None:           \n",
        "                drive = make_drive()\n",
        "                last_saved_time = datetime.datetime.now()\n",
        "                save_model_in_drive(drive, hparams.model_folder_in_drive, hparams.model_path)\n",
        "\n",
        "            if i != 0 and i % 100 == 0:\n",
        "                plot_validation_loss(x, y)\n",
        "\n",
        "\n",
        "def plot_validation_loss(x, y):\n",
        "  if colab():\n",
        "    plt.plot(x, y, label=\"Validation Loss\")\n",
        "    plt.plot()\n",
        "    plt.ylabel(\"Validation Loss\")\n",
        "    plt.xlabel(\"steps\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def train_loop(train_feed_data, val_feed_data, vocab, rev_vocab,\n",
        "               hparams, tweets, drive=None, short_loop=False):\n",
        "    def inference_hook(_):\n",
        "        None\n",
        "\n",
        "    def generate_models(local_hparams):\n",
        "        graph = tf.Graph()\n",
        "        sess = tf.Session(graph=graph)\n",
        "        model, infer_model = create_train_infer_models(graph, sess,\n",
        "                                                       local_hparams)\n",
        "        return graph, sess, model, infer_model\n",
        "\n",
        "    generic_train_loop(train_feed_data, val_feed_data, vocab, rev_vocab,\n",
        "                       hparams, generate_models, inference_hook, tweets,\n",
        "                       drive, short_loop)\n",
        "\n",
        "\n",
        "def train_loop_distributed_pattern(train_feed_data, val_feed_data, vocab,\n",
        "                                   rev_vocab, hparams, tweets, drive=None,\n",
        "                                   short_loop=False):\n",
        "    def inference_hook(infer_model):\n",
        "        # always restore from file, because it's in different graph.\n",
        "        restored = infer_model.restore()\n",
        "        assert restored\n",
        "\n",
        "    def generate_models(local_hparams):\n",
        "        train_graph = tf.Graph()\n",
        "        infer_graph = tf.Graph()\n",
        "        # See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
        "        config = tf.ConfigProto(log_device_placement=True)\n",
        "        config.gpu_options.allow_growth = True\n",
        "        train_sess = tf.Session(graph=train_graph, config=config)\n",
        "        print(\"train_sess=\", train_sess)\n",
        "        infer_sess = tf.Session(graph=infer_graph, config=config)\n",
        "\n",
        "        device = '/cpu:0'\n",
        "        if has_gpu0():\n",
        "            device = '/gpu:0'\n",
        "            print(\"!!!GPU ENABLED !!!\")\n",
        "        with tf.device(device):\n",
        "            model, infer_model = create_train_infer_models_in_graphs(\n",
        "                train_graph,\n",
        "                train_sess,\n",
        "                infer_graph,\n",
        "                infer_sess,\n",
        "                local_hparams)\n",
        "        return train_graph, train_sess, model, infer_model\n",
        "\n",
        "    generic_train_loop(train_feed_data, val_feed_data, vocab, rev_vocab,\n",
        "                       hparams, generate_models, inference_hook,\n",
        "                       tweets, drive, short_loop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qml25n4hsmFZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "81_wjAUyCVBp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class TrainDataGenerator:\n",
        "    def __init__(self, source_path, hparams):\n",
        "        self.source_path = source_path\n",
        "        self.hparams = hparams\n",
        "        basename, extension = os.path.splitext(self.source_path)\n",
        "        self.enc_path = \"{}_enc{}\".format(basename, extension)\n",
        "        self.dec_path = \"{}_dec{}\".format(basename, extension)\n",
        "        self.enc_idx_path = \"{}_enc_idx{}\".format(basename, extension)\n",
        "        self.dec_idx_path = \"{}_dec_idx{}\".format(basename, extension)\n",
        "        self.dec_idx_eos_path = \"{}_dec_idx_eos{}\".format(basename, extension)\n",
        "        self.dec_idx_sos_path = \"{}_dec_idx_sos{}\".format(basename, extension)\n",
        "        self.dec_idx_len_path = \"{}_dec_idx_len{}\".format(basename, extension)\n",
        "\n",
        "        self.enc_idx_padded_path = \"{}_enc_idx_padded{}\".format(basename,\n",
        "                                                                extension)\n",
        "        self.enc_idx_len_path = \"{}_enc_idx_len{}\".format(basename, extension)\n",
        "\n",
        "        self.vocab_path = \"{}_vocab{}\".format(basename, extension)\n",
        "        self.max_vocab_size = hparams.vocab_size\n",
        "        self.start_vocabs = [hparams.sos_token, hparams.eos_token, hparams.pad_token, hparams.unk_token]\n",
        "        self.tagger = MeCab.Tagger(\"-Owakati\")\n",
        "\n",
        "    def generate(self):\n",
        "        print(\"generating enc and dec files...\")\n",
        "        self._generate_enc_dec()\n",
        "        print(\"generating vocab file...\")\n",
        "        self._generate_vocab()\n",
        "        print(\"loading vocab...\")\n",
        "        vocab, _ = self._load_vocab()\n",
        "        print(\"generating id files...\")\n",
        "        self._generate_id_file(self.enc_path, self.enc_idx_path, vocab)\n",
        "        self._generate_id_file(self.dec_path, self.dec_idx_path, vocab)\n",
        "        print(\"generating padded input file...\")\n",
        "        self._generate_enc_idx_padded(self.enc_idx_path,\n",
        "                                      self.enc_idx_padded_path,\n",
        "                                      self.enc_idx_len_path,\n",
        "                                      self.hparams.encoder_length)\n",
        "        print(\"generating dec eos/sos files...\")\n",
        "        self._generate_dec_idx_eos(self.dec_idx_path, self.dec_idx_eos_path,\n",
        "                                   self.hparams.decoder_length)\n",
        "        self._generate_dec_idx_sos(self.dec_idx_path, self.dec_idx_sos_path,\n",
        "                                   self.dec_idx_len_path,\n",
        "                                   self.hparams.decoder_length)\n",
        "        print(\"done\")\n",
        "        return self._create_dataset()\n",
        "\n",
        "    def _generate_id_file(self, source_path, dest_path, vocab):\n",
        "        if gfile.Exists(dest_path):\n",
        "            return\n",
        "        with gfile.GFile(source_path, mode=\"rb\") as f, gfile.GFile(dest_path,\n",
        "                                                                   mode=\"wb\") as of:\n",
        "            for line in f:\n",
        "                line = line.decode('utf-8')\n",
        "                words = self.tagger.parse(line).split()\n",
        "                ids = [vocab.get(w, self.hparams.unk_id) for w in words]\n",
        "                of.write(\" \".join([str(id) for id in ids]) + \"\\n\")\n",
        "\n",
        "    def _load_vocab(self):\n",
        "        rev_vocab = []\n",
        "        with gfile.GFile(self.vocab_path, mode=\"r\") as f:\n",
        "            rev_vocab.extend(f.readlines())\n",
        "            rev_vocab = [line.strip() for line in rev_vocab]\n",
        "            # Dictionary of (word, idx)\n",
        "            vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
        "            return vocab, rev_vocab\n",
        "\n",
        "    def _generate_vocab(self):\n",
        "        if gfile.Exists(self.vocab_path):\n",
        "            return\n",
        "        vocab_dic = self._build_vocab_dic(self.enc_path)\n",
        "        vocab_dic = self._build_vocab_dic(self.dec_path, vocab_dic)\n",
        "        vocab_list = self.start_vocabs + sorted(vocab_dic, key=vocab_dic.get,\n",
        "                                                reverse=True)\n",
        "        if len(vocab_list) > self.max_vocab_size:\n",
        "            vocab_list = vocab_list[:self.max_vocab_size]\n",
        "        with gfile.GFile(self.vocab_path, mode=\"w\") as vocab_file:\n",
        "            for w in vocab_list:\n",
        "                vocab_file.write(w + \"\\n\")\n",
        "\n",
        "    def _generate_enc_dec(self):\n",
        "        if gfile.Exists(self.enc_path) and gfile.Exists(self.dec_path):\n",
        "            return\n",
        "        with gfile.GFile(self.source_path, mode=\"rb\") as f, gfile.GFile(\n",
        "                self.enc_path, mode=\"w+\") as ef, gfile.GFile(self.dec_path,\n",
        "                                                             mode=\"w+\") as df:\n",
        "            tweet = None\n",
        "            reply = None\n",
        "            for i, line in enumerate(f):\n",
        "                line = line.decode('utf-8')\n",
        "                line = self.sanitize_line(line)\n",
        "                if i % 2 == 0:\n",
        "                  tweet = line\n",
        "                else:\n",
        "                  reply = line\n",
        "                  if tweet and reply:\n",
        "                    ef.write(tweet)\n",
        "                    df.write(reply)\n",
        "                  tweet = None\n",
        "                  reply = None\n",
        "\n",
        "    def _generate_enc_idx_padded(self, source_path, dest_path, dest_len_path,\n",
        "                                 max_line_len):\n",
        "        if gfile.Exists(dest_path):\n",
        "            return\n",
        "        with open(source_path) as fin, open(dest_path,\n",
        "                                            \"w\") as fout, open(dest_len_path,\n",
        "                                                               \"w\") as flen:\n",
        "            line = fin.readline()\n",
        "            while line:\n",
        "                ids = [int(x) for x in line.split()]\n",
        "                if len(ids) > max_line_len:\n",
        "#                    ids = ids[:max_line_len]\n",
        "                    ids = ids[-max_line_len:]\n",
        "                flen.write(str(len(ids)))\n",
        "                flen.write(\"\\n\")\n",
        "                if len(ids) < max_line_len:\n",
        "                    ids.extend([self.hparams.pad_id] * (max_line_len - len(ids)))\n",
        "                ids = [str(x) for x in ids]\n",
        "                fout.write(\" \".join(ids))\n",
        "                fout.write(\"\\n\")\n",
        "                line = fin.readline()\n",
        "\n",
        "    # read decoder_idx file and append eos at the end of idx list.\n",
        "    def _generate_dec_idx_eos(self, source_path, dest_path, max_line_len):\n",
        "        if gfile.Exists(dest_path):\n",
        "            return\n",
        "        with open(source_path) as fin, open(dest_path, \"w\") as fout:\n",
        "            line = fin.readline()\n",
        "            while line:\n",
        "                ids = [int(x) for x in line.split()]\n",
        "                if len(ids) > max_line_len - 1:\n",
        "#                    ids = ids[:max_line_len - 1]\n",
        "                  ids = ids[-(max_line_len - 1):]\n",
        "                ids.append(self.hparams.eos_id)\n",
        "                if len(ids) < max_line_len:\n",
        "                    ids.extend([self.hparams.pad_id] * (max_line_len - len(ids)))\n",
        "                ids = [str(x) for x in ids]\n",
        "                fout.write(\" \".join(ids))\n",
        "                fout.write(\"\\n\")\n",
        "                line = fin.readline()\n",
        "\n",
        "    # read decoder_idx file and put sos at the beginning of the idx list.\n",
        "    # also write out length of index list.\n",
        "    def _generate_dec_idx_sos(self, source_path, dest_path, dest_len_path,\n",
        "                              max_line_len):\n",
        "        if gfile.Exists(dest_path):\n",
        "            return\n",
        "        with open(source_path) as fin, open(dest_path, \"w\") as fout, open(\n",
        "                dest_len_path, \"w\") as flen:\n",
        "            line = fin.readline()\n",
        "            while line:\n",
        "                ids = [self.hparams.sos_id]\n",
        "                ids.extend([int(x) for x in line.split()])\n",
        "                if len(ids) > max_line_len:\n",
        "                    ids = ids[:max_line_len]\n",
        "                flen.write(str(len(ids)))\n",
        "                flen.write(\"\\n\")\n",
        "                if len(ids) < max_line_len:\n",
        "                    ids.extend([self.hparams.pad_id] * (max_line_len - len(ids)))\n",
        "                ids = [str(x) for x in ids]\n",
        "                fout.write(\" \".join(ids))\n",
        "                fout.write(\"\\n\")\n",
        "                line = fin.readline()\n",
        "\n",
        "    @staticmethod\n",
        "    def sanitize_line(line):\n",
        "        # replace @username\n",
        "        # replacing @username had bad impace where USERNAME token shows up everywhere.\n",
        "#        line = re.sub(r\"@([A-Za-z0-9_]+)\", \"USERNAME\", line)\n",
        "        line = re.sub(r\"@([A-Za-z0-9_]+)\", \"\", line)\n",
        "        # Remove URL\n",
        "        line = re.sub(r'https?:\\/\\/.*', \"\", line)\n",
        "        line = line.lstrip()\n",
        "        return line\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_source_target_swapped(source_path):\n",
        "        basename, extension = os.path.splitext(source_path)\n",
        "        dest_path = \"{}_swapped{}\".format(basename, extension)\n",
        "        with gfile.GFile(source_path, mode=\"rb\") as fin, gfile.GFile(dest_path,\n",
        "                                                                     mode=\"w+\") as fout:\n",
        "            temp = None\n",
        "            for i, line in enumerate(fin):\n",
        "                if i % 2 == 0:\n",
        "                    temp = line\n",
        "                else:\n",
        "                    fout.write(line)\n",
        "                    fout.write(temp)\n",
        "                    temp = None\n",
        "        return dest_path\n",
        "\n",
        "    def _build_vocab_dic(self, source_path, vocab_dic={}):\n",
        "        with gfile.GFile(source_path, mode=\"r\") as f:\n",
        "            for line in f:\n",
        "                words = self.tagger.parse(line).split()\n",
        "                for word in words:\n",
        "                    if word in vocab_dic:\n",
        "                        vocab_dic[word] += 1\n",
        "                    else:\n",
        "                        vocab_dic[word] = 1\n",
        "            return vocab_dic\n",
        "\n",
        "    @staticmethod\n",
        "    def _read_file(source_path):\n",
        "        f = open(source_path)\n",
        "        data = f.read()\n",
        "        f.close()\n",
        "        return data\n",
        "\n",
        "    def _read_vocab(self, source_path):\n",
        "        rev_vocab = []\n",
        "        rev_vocab.extend(self._read_file(source_path).splitlines())\n",
        "        rev_vocab = [line.strip() for line in rev_vocab]\n",
        "        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
        "        return vocab, rev_vocab\n",
        "\n",
        "    def text_line_split_dataset(self, filename):\n",
        "        return tf.data.TextLineDataset(filename).map(self.split_to_int_values)\n",
        "\n",
        "    @staticmethod\n",
        "    def split_to_int_values(x):\n",
        "        return tf.string_to_number(tf.string_split([x]).values, tf.int32)\n",
        "\n",
        "    def _create_dataset(self):\n",
        "\n",
        "        tweets_dataset = self.text_line_split_dataset(self.enc_idx_padded_path)\n",
        "        tweets_lengths_dataset = tf.data.TextLineDataset(\n",
        "            self.enc_idx_len_path)\n",
        "\n",
        "        replies_sos_dataset = self.text_line_split_dataset(\n",
        "            self.dec_idx_sos_path)\n",
        "        replies_eos_dataset = self.text_line_split_dataset(\n",
        "            self.dec_idx_eos_path)\n",
        "        replies_sos_lengths_dataset = tf.data.TextLineDataset(\n",
        "            self.dec_idx_len_path)\n",
        "\n",
        "        tweets_transposed = tweets_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(\n",
        "                self.hparams.batch_size)).map(\n",
        "            lambda x: tf.transpose(x))\n",
        "        tweets_lengths = tweets_lengths_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(self.hparams.batch_size))\n",
        "\n",
        "        replies_with_eos_suffix = replies_eos_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(self.hparams.batch_size))\n",
        "        replies_with_sos_prefix = replies_sos_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(\n",
        "                self.hparams.batch_size)).map(\n",
        "            lambda x: tf.transpose(x))\n",
        "        replies_with_sos_suffix_lengths = replies_sos_lengths_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(\n",
        "                self.hparams.batch_size))\n",
        "        vocab, rev_vocab = self._read_vocab(self.vocab_path)\n",
        "        return tf.data.Dataset.zip((tweets_transposed, tweets_lengths,\n",
        "                                    replies_with_eos_suffix,\n",
        "                                    replies_with_sos_prefix,\n",
        "                                    replies_with_sos_suffix_lengths)), vocab, rev_vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I0-o2wg0gzvA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def list_model_file(path):\n",
        "    f = open('{}/checkpoint'.format(path))\n",
        "    text = f.read()\n",
        "    f.close()\n",
        "    print(text)\n",
        "    m = re.match(r\".*ChatbotModel\\-(\\d+)\", text)\n",
        "    model_name = m.group(1)\n",
        "    all = [\"checkpoint\"]\n",
        "    all.extend([x for x in os.listdir(path) if re.search(model_name, x) or re.search('events.out', x)])\n",
        "    print(\"all=\", all)\n",
        "    return all\n",
        "\n",
        "\n",
        "def save_model_in_drive(drive, model_folder_in_drive, model_path):\n",
        "    file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(\n",
        "        model_folder_in_drive)}).GetList()\n",
        "    for model_file in list_model_file(model_path):\n",
        "        file = drive.CreateFile({'title': model_file, \"parents\": [\n",
        "            {\"kind\": \"drive#fileLink\", \"id\": model_folder_in_drive}]})\n",
        "        file.SetContentFile(\"{}/{}\".format(model_path, model_file))\n",
        "        print(\"Uploading \", model_file, \"...\", end=\"\")\n",
        "        file.Upload()\n",
        "        print(\"done\")\n",
        "    for file in file_list:\n",
        "        f = drive.CreateFile({'id': file['id']})\n",
        "        f.Delete()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g7qybKCEFgXE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "if colab():\n",
        "    !pip install pydrive\n",
        "    from pydrive.auth import GoogleAuth\n",
        "    from pydrive.drive import GoogleDrive\n",
        "    from google.colab import auth\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "\n",
        "    def make_drive():\n",
        "        # 1. Authenticate and create the PyDrive client.\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        drive = GoogleDrive(gauth)\n",
        "        return drive\n",
        "    drive = make_drive()\n",
        "else:        \n",
        "    drive = None\n",
        "    def make_drive():\n",
        "        return None\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PBo5SLAk3vza",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get -qq install -y mecab libmecab-dev mecab-ipadic mecab-ipadic-utf8\n",
        "!pip install mecab-python3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LPHzPckzWBQ8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls \n",
        "!wc -l tweets_small_vocab.txt\n",
        "!cat tweets_small_vocab.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JrvS_DURF5Pq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "\n",
        "\n",
        "def clean_model_path(model_path):\n",
        "    shutil.rmtree(model_path)\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "\n",
        "def print_header(text):\n",
        "    print(\"============== {} ==============\".format(text))\n",
        "\n",
        "\n",
        "def test_tweets_small(hparams):\n",
        "    print_header(\"train_loop_distributed_pattern\")\n",
        "    clean_model_path(hparams.model_path)\n",
        "\n",
        "    source_path = \"tweets_small.txt\"\n",
        "    download_file_if_necessary(source_path)\n",
        "    generator = TrainDataGenerator(source_path=source_path, hparams=hparams)\n",
        "    train_dataset, vocab, rev_vocab = generator.generate()\n",
        "\n",
        "    tweets = [\"おはようございます。寒いですね。\", \"さて帰ろう。明日は早い。\", \"今回もよろしくです。\"]\n",
        "\n",
        "    train_loop_distributed_pattern(train_dataset.repeat(100),\n",
        "                                   train_dataset.repeat(100),\n",
        "                                   vocab,\n",
        "                                   rev_vocab,\n",
        "                                   hparams, tweets, short_loop=False)\n",
        "\n",
        "\n",
        "def test_tweets_small_swapped(hparams):\n",
        "    print_header(\"train_loop_distributed_pattern swapped\")\n",
        "\n",
        "    clean_model_path(hparams.model_path)\n",
        "\n",
        "    source_path = \"tweets_small.txt\"\n",
        "    download_file_if_necessary(source_path)\n",
        "    TrainDataGenerator.generate_source_target_swapped(p(source_path))\n",
        "\n",
        "    swapped_path = \"tweets_small_swapped.txt\"\n",
        "\n",
        "    generator = TrainDataGenerator(source_path=swapped_path, hparams=hparams)\n",
        "    train_dataset, vocab, rev_vocab = generator.generate()\n",
        "\n",
        "    replies = [\"@higepon おはようございます！\", \"おつかれさまー。気をつけて。\", \"こちらこそよろしくお願いします。\"]\n",
        "\n",
        "    print_header(\"train_loop_distributed_pattern\")\n",
        "    train_loop_distributed_pattern(train_dataset.repeat(100),\n",
        "                                   train_dataset.repeat(100),\n",
        "                                   vocab,\n",
        "                                   rev_vocab,\n",
        "                                   hparams, replies, short_loop=False)\n",
        "\n",
        "# vocab size 変えたら動かなくなった\n",
        "tweet_small_hparams = copy.deepcopy(base_hparams).override_from_dict(\n",
        "    {\n",
        "        'batch_size': 6,  # of tweets should be dividable by batch_size\n",
        "        'encoder_length': 8,\n",
        "        'decoder_length': 8,\n",
        "        'num_units': 256,\n",
        "        'num_layers': 2,\n",
        "        'vocab_size': 34,\n",
        "        'embedding_size': 40,\n",
        "        'beam_width': 2,  # for faster iteration, this should be 10\n",
        "        'num_train_steps': 200,\n",
        "        'model_path': p(ModelDirectory.tweet_small.value),\n",
        "        'learning_rate': 0.05,\n",
        "        'use_attention': True,\n",
        "    })\n",
        "\n",
        "tweet_small_swapped_hparams = copy.deepcopy(\n",
        "    tweet_small_hparams).override_from_dict(\n",
        "    {'model_path': p(ModelDirectory.tweet_small_swapped.value)})\n",
        "\n",
        "!rm tweets_small*\n",
        "test_tweets_small(tweet_small_hparams)\n",
        "test_tweets_small_swapped(tweet_small_swapped_hparams)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EnmDxS2l4dDY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#!cat tweets_small_enc.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tzh2rhEPguJ9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "run_large = True\n",
        "run_large_swapped = False\n",
        "\n",
        "\n",
        "def test_tweets_large(hparams):\n",
        "    print_header(\"train_loop_distributed_pattern\")\n",
        "\n",
        "    source_path = \"tweets_conversation.txt\"\n",
        "    download_file_if_necessary(source_path)\n",
        "    generator = TrainDataGenerator(source_path=source_path, hparams=hparams)\n",
        "    train_dataset, vocab, rev_vocab = generator.generate()\n",
        "\n",
        "    tweets = [\"さて福岡行ってきます！\", \"誰か飲みに行こう\", \"熱でてるけど、でもなんか食べなきゃーと思ってアイス買おうとしたの\",\"今日のドラマ面白そう！\",\"お腹すいたー\", \"おやすみ～\", \"おはようございます。寒いですね。\", \"さて帰ろう。明日は早い。\", \"今回もよろしくです。\", \"ばいとおわ！\"]\n",
        "\n",
        "    train_loop_distributed_pattern(train_dataset.repeat().shuffle(1024),\n",
        "                                   train_dataset.repeat().shuffle(1024, seed=1234),\n",
        "                                   vocab,\n",
        "                                   rev_vocab,\n",
        "                                   hparams, tweets, short_loop=False,\n",
        "                                   drive=drive)\n",
        "\n",
        "\n",
        "def test_tweets_large_swapped(hparams):\n",
        "    print_header(\"train_loop_distributed_pattern\")\n",
        "\n",
        "    source_path = \"tweets_large.txt\"\n",
        "    download_file_if_necessary(source_path)\n",
        "    swapped_source_path = TrainDataGenerator.generate_source_target_swapped(\n",
        "        source_path)\n",
        "\n",
        "    generator = TrainDataGenerator(source_path=swapped_source_path,\n",
        "                                   hparams=hparams)\n",
        "    train_dataset, vocab, rev_vocab = generator.generate()\n",
        "\n",
        "    tweets = [ \"今日のドラマ面白そう！\",\"お腹すいたー\", \"おやすみ～\", \"おはようございます。寒いですね。\", \"さて帰ろう。明日は早い。\", \"今回もよろしくです。\", \"ばいとおわ！\"]\n",
        "\n",
        "\n",
        "    train_loop_distributed_pattern(train_dataset.repeat().shuffle(1024),\n",
        "                                   train_dataset.repeat().shuffle(1024, seed=1234),\n",
        "                                   vocab,\n",
        "                                   rev_vocab,\n",
        "                                   hparams, tweets, short_loop=False,\n",
        "                                   drive=drive)\n",
        "\n",
        "\n",
        "tweet_large_hparams = copy.deepcopy(base_hparams).override_from_dict(\n",
        "    {\n",
        "        # In typical seq2seq chatbot\n",
        "        # num_layers=3, learning_rate=0.5, batch_size=64, vocab=20000-100000, learning_rate decay is 0.99, which is taken care as default parameter in AdamOptimizer.\n",
        "        'batch_size': 64,  # of tweets should be dividable by batch_size\n",
        "        'encoder_length': 30,\n",
        "        'decoder_length': 30,\n",
        "        'num_units': 1024,\n",
        "        'num_layers': 3,\n",
        "        'vocab_size': 50000,\n",
        "        'embedding_size': 1024,\n",
        "        'beam_width': 2,  # for faster iteration, this should be 10\n",
        "        'num_train_steps': 1000000,\n",
        "        'model_folder_in_drive': GoogleDriveFolder.seq2seq.value,\n",
        "        'model_path': p(ModelDirectory.tweet_large.value),\n",
        "        'learning_rate': 0.5, # For vocab_size 50000, num_layers 3, num_units 1024, tweet_large, starting learning_rate 0.05 works well, change it t0 0.01 at perplexity 800, changed it to 0.005 at 200.\n",
        "        'learning_rate_decay': 0.99,\n",
        "        'use_attention': True,\n",
        "        # testing new restore learning rate and no USERNAME TOKEN\n",
        "    })\n",
        "\n",
        "tweet_large_swapped_hparams = copy.deepcopy(\n",
        "    tweet_large_hparams).override_from_dict(\n",
        "    {\n",
        "        'model_folder_in_drive': GoogleDriveFolder.seq2seq_swapped.value,\n",
        "        'model_path': p(ModelDirectory.tweet_large_swapped.value)\n",
        "    })\n",
        "\n",
        "if run_large:\n",
        "    test_tweets_large(tweet_large_hparams)\n",
        "if run_large_swapped:\n",
        "    test_tweets_large_swapped(tweet_large_swapped_hparams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dv4xb9avyi88",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#!rm tweets_large* \n",
        "#!rm -rf model/tweet_large\n",
        "!ls\n",
        "!grep  -in '^$' tweets_large_dec.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wQcU64vOFTEp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 7
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "5551c247-9645-4d38-b510-0c26dbf797c0"
      },
      "cell_type": "code",
      "source": [
        "def train_rl_loop_distributed_pattern(train_feed_data,\n",
        "                                      val_feed_data,\n",
        "                                      vocab,\n",
        "                                      rev_vocab,\n",
        "                                      src_hparams,\n",
        "                                      dst_hparams,\n",
        "                                      tweets,\n",
        "                                      drive=None,\n",
        "                                      short_loop=False):\n",
        "\n",
        "    download_model_data_if_necessary(drive, src_hparams.model_folder_in_drive, src_hparams.model_path)\n",
        "    download_model_data_if_necessary(drive, dst_hparams.model_folder_in_drive, dst_hparams.model_path)\n",
        "\n",
        "    seq2seq_graph = tf.Graph()\n",
        "    rl_graph = tf.Graph()\n",
        "\n",
        "    seq2seq_sess = tf.Session(graph=seq2seq_graph)\n",
        "    rl_sess = tf.Session(graph=rl_graph)\n",
        "\n",
        "    with seq2seq_graph.as_default():\n",
        "        with tf.variable_scope('root'):\n",
        "            seq2seq_infer_model = ChatbotInferenceModel(seq2seq_sess, src_hparams,\n",
        "                                                        model_path=src_hparams.model_path)\n",
        "            restored = seq2seq_infer_model.restore()\n",
        "            assert restored\n",
        "\n",
        "    model, infer_model = create_train_infer_models(rl_graph, rl_sess, dst_hparams)\n",
        "\n",
        "    # todo todo\n",
        "    writer = tf.summary.FileWriter(dst_hparams.model_path, rl_graph)\n",
        "\n",
        "    with rl_graph.as_default():\n",
        "        train_data_iterator = train_feed_data.make_one_shot_iterator()\n",
        "        val_data_iterator = val_feed_data.make_one_shot_iterator()\n",
        "\n",
        "    last_saved_time = datetime.datetime.now()\n",
        "    last_time = datetime.datetime.now()\n",
        "    x = []\n",
        "    y = []\n",
        "    helper = InferenceHelper(infer_model, vocab, rev_vocab)\n",
        "    dull_responses = map(lambda words: helper.words_to_ids(words),\n",
        "                         [[\"おはよう\"], [\"おつかれ\"]])\n",
        "    print(dull_responses)\n",
        "\n",
        "    \n",
        "    for i in range(dst_hparams.num_train_steps):\n",
        "        train_data = rl_sess.run(train_data_iterator.get_next())\n",
        "        global_step, _, summary = model.train_with_reward(infer_model,\n",
        "                                                          seq2seq_infer_model,\n",
        "                                                          train_data[0],\n",
        "                                                          train_data[1],\n",
        "                                                          train_data[2],\n",
        "                                                          train_data[3],\n",
        "                                                          train_data[4],\n",
        "                                                          dull_responses)\n",
        "        writer.add_summary(summary, global_step)\n",
        "\n",
        "\n",
        "\n",
        "        if short_loop and i == 2:\n",
        "            model.save()\n",
        "            print(\"==== {} ====\".format(global_step))\n",
        "            helper.print_inferences(\"お疲れ様ー\")            \n",
        "            break\n",
        "        elif i != 0 and i % 15 == 0:\n",
        "            model.save(dst_hparams.model_path)\n",
        "            print(\"==== {} ====\".format(global_step))\n",
        "            for tweet in tweets:\n",
        "              helper.print_inferences(tweet)            \n",
        "    \n",
        "            val_data = rl_sess.run(val_data_iterator.get_next())\n",
        "            val_loss, val_loss_log = model.batch_loss(val_data[0],\n",
        "                                                      val_data[1],\n",
        "                                                      val_data[2],\n",
        "                                                      val_data[3],\n",
        "                                                      val_data[4])\n",
        "            writer.add_summary(val_loss_log, global_step)\n",
        "            print(\"validation loss\", val_loss)\n",
        "            delta = (datetime.datetime.now() - last_time).total_seconds() * 1000\n",
        "            print(\"{:.2f} msec/data\".format(delta / dst_hparams.batch_size / 15))\n",
        "            last_time = datetime.datetime.now()\n",
        "            x.append(global_step)\n",
        "            y.append(val_loss)\n",
        "\n",
        "            now = datetime.datetime.now()\n",
        "            if (\n",
        "                    now - last_saved_time).total_seconds() > 900 and drive is not None:\n",
        "                save_model_in_drive(drive, dst_hparams.model_folder_in_drive, dst_hparams.model_path)\n",
        "                last_saved_time = datetime.datetime.now()\n",
        "\n",
        "            if i != 0 and i % 100 == 0:\n",
        "                plot_validation_loss(x, y)\n",
        "        else:\n",
        "            print('.', end='')\n",
        "            \n",
        "def train_large_rl(src_hparams, dst_hparams):\n",
        "\n",
        "  source_path = \"tweets_large.txt\"\n",
        "  download_file_if_necessary(source_path)\n",
        "  generator = TrainDataGenerator(source_path=source_path, hparams=dst_hparams)\n",
        "  train_dataset, vocab, rev_vocab = generator.generate()\n",
        "\n",
        "  tweets = [\"さて福岡行ってきます！\", \"誰か飲みに行こう\", \"熱でてるけど、でもなんか食べなきゃーと思ってアイス買おうとしたの\",\"今日のドラマ面白そう！\",\"お腹すいたー\", \"おやすみ～\", \"おはようございます。寒いですね。\", \"さて帰ろう。明日は早い。\", \"今回もよろしくです。\", \"ばいとおわ！\"]\n",
        "\n",
        "    \n",
        "  train_rl_loop_distributed_pattern(train_dataset.repeat().shuffle(1024),\n",
        "                                    train_dataset.repeat().shuffle(1024, seed=1234), \n",
        "                                    vocab,\n",
        "                                    rev_vocab,\n",
        "                                    src_hparams,\n",
        "                                    dst_hparams,\n",
        "                                    tweets,\n",
        "                                    drive=drive)    \n",
        "\n",
        "\n",
        "rl_src_hparams = copy.deepcopy(tweet_large_hparams).override_from_dict({'beam_width': 0})\n",
        "rl_dst_hparams = copy.deepcopy(rl_src_hparams).override_from_dict({\n",
        "    'model_folder_in_drive': GoogleDriveFolder.seq2seq_rl.value,\n",
        "    'model_path': p(ModelDirectory.tweet_large_rl.value),\n",
        "})\n",
        "\n",
        "train_large_rl(rl_src_hparams, rl_dst_hparams)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== 108 ====\n",
            "さて福岡行ってきます！\n",
            "    [0]はててててててててててててててててててててててててててててて\n",
            "誰か飲みに行こう\n",
            "    [0]はててててててててててててててててててててててててててててて\n",
            "熱でてるけど、でもなんか食べなきゃーと思ってアイス買おうとしたの\n",
            "    [0]はははててててててててててててててててててててててててててて\n",
            "今日のドラマ面白そう！\n",
            "    [0]はててててててててててててててててててててててててててててて\n",
            "お腹すいたー\n",
            "    [0]てててててててててててててててててててててててててててててて\n",
            "おやすみ～\n",
            "    [0]てててててててててててててててててててててててててててててて\n",
            "おはようございます。寒いですね。\n",
            "    [0]ははてててててててててててててててててててててててててててて\n",
            "さて帰ろう。明日は早い。\n",
            "    [0]ははてててててててててててててててててててててててててててて\n",
            "今回もよろしくです。\n",
            "    [0]はててててててててててててててててててててててててててててて\n",
            "ばいとおわ！\n",
            "    [0]てててててててててててててててててててててててててててててて\n",
            "validation loss 162.82904\n",
            "38.57 msec/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3jxF0tWHoBL-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "|def test_mutual_information(hparams, swapped_hparams):\n",
        "    graph = tf.Graph()\n",
        "    swapped_graph = tf.Graph()\n",
        "    config = tf.ConfigProto(log_device_placement=True)\n",
        "    config.gpu_options.allow_growth = True\n",
        "\n",
        "    drive = make_drive()\n",
        "    download_model_data_if_necessary(drive, hparams.model_folder_in_drive,\n",
        "                                     p(ModelDirectory.tweet.value))\n",
        "    download_model_data_if_necessary(drive,\n",
        "                                     swapped_hparams.model_folder_in_drive,\n",
        "                                     p(ModelDirectory.tweet_swapped.value))\n",
        "\n",
        "    with graph.as_default():\n",
        "        infer_sess = tf.Session(graph=graph, config=config)\n",
        "        with tf.variable_scope('root'):\n",
        "            model = ChatbotInferenceModel(infer_sess, hparams,\n",
        "                                          model_path=p(ModelDirectory.tweet.value))\n",
        "            model.restore()\n",
        "\n",
        "    with swapped_graph.as_default():\n",
        "        swap_sess = tf.Session(graph=swapped_graph, config=config)\n",
        "        with tf.variable_scope('root'):\n",
        "            smodel = ChatbotInferenceModel(swap_sess, swapped_hparams,\n",
        "                                           model_path=p(\n",
        "                                               ModelDirectory.tweet_swapped.value))\n",
        "            smodel.restore()\n",
        "            helper = InferenceHelper(model, vocab, rev_vocab)\n",
        "            helper.print_inferences(\"疲れた\")\n",
        "\n",
        "            shelper = InferenceHelper(smodel, vocab, rev_vocab)\n",
        "            shelper.print_inferences(\"お疲れ様\")\n",
        "\n",
        "\n",
        "#large_beam_hparams = copy.deepcopy(large_hparams)\n",
        "#large_beam_hparams.beam_width = 20\n",
        "#test_mutual_information(large_beam_hparams, large_swapped_hparams)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FGFWGrQv_kZ-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "ｑ"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
