{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/higepon/tensorflow_seq2seq_chatbot/blob/master/seq2seq.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "HWOxK9T5I8sb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq Beam Search + Attention + RL\n",
        "- Tensorflow 1.4.0+ is required.\n",
        "- This is based on [NMT Tutorial](https://github.com/tensorflow/nmt).\n",
        "- Experiment [notes](https://github.com/higepon/tensorflow_seq2seq_chatbot/wiki).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kK1r053SI2f9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WE9v1UerJMRo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KapXwLNkJtH-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os   \n",
        "import mmap \n",
        "import json\n",
        "import hashlib\n",
        "\n",
        "def colab():\n",
        "    return '/tools/node/bin/forever' == os.environ['_']\n",
        "\n",
        "def client_id():\n",
        "  clients = {'dfc1d5b22ba03430800179d23e522f6f': 'client1',\n",
        "                   'f8e857a2d792038820ebb2ae8d803f7c': 'client2',\n",
        "                   '7628f983785173edabbde501ef8f781d': 'client3'}\n",
        "  with open('/content/datalab/adc.json') as json_data:\n",
        "      d = json.load(json_data)\n",
        "      email =d['id_token']['email'].encode('utf-8')\n",
        "      return clients[hashlib.md5(email).hexdigest()]\n",
        "    \n",
        "print(client_id())\n",
        "current_client_id = client_id()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mM1uEwbYJPJK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate creds for the Drive FUSE library.\n",
        "if not os.path.exists('drive'):\n",
        "  from oauth2client.client import GoogleCredentials\n",
        "  creds = GoogleCredentials.get_application_default()\n",
        "  import getpass\n",
        "  !echo \"Auth\"\n",
        "  !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "  vcode = getpass.getpass()\n",
        "  !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M4eRP5c6JSko",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if not os.path.exists('drive'):\n",
        "  !mkdir -p drive\n",
        "  !google-drive-ocamlfuse drive\n",
        "\n",
        "drive_path = 'drive/seq2seq_data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bPLkjCHPSyGx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip -q install git+https://github.com/mrahtz/easy-tf-log#egg=easy-tf-log[tf]\n",
        "from easy_tf_log import tflog, Logger\n",
        "import easy_tf_log\n",
        "\n",
        "# Hack to reset the logger state\n",
        "#Logger.DEFAULT = None\n",
        "\n",
        "from __future__ import print_function\n",
        "from enum import Enum, auto\n",
        "\n",
        "class Mode(Enum):\n",
        "    Test = auto()\n",
        "    TrainSeq2Seq = auto()\n",
        "    TrainSeq2SeqSwapped = auto()\n",
        "    TrainRL = auto()\n",
        "    TweetBot = auto()\n",
        "\n",
        "mode = Mode.Test\n",
        "#mode = Mode.TrainSeq2Seq\n",
        "#mode = Mode.TrainSeq2SeqSwapped\n",
        "#mode = Mode.TrainRL\n",
        "#mode = Mode.TweetBot\n",
        "\n",
        "\n",
        "# @formatter:off\n",
        "import copy as copy\n",
        "from google.colab import files\n",
        "import datetime\n",
        "import re\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.layers import core as layers_core\n",
        "from tensorflow.python.platform import gfile\n",
        "\n",
        "#@formatter:on\n",
        "\n",
        "if colab():\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "# Note for myself.\n",
        "# You've summarized Seq2Seq\n",
        "# at http://d.hatena.ne.jp/higepon/20171210/1512887715.\n",
        "\n",
        "# If you see following error, it means your max(len(tweets of training set)) <  decoder_length.\n",
        "# This should be a bug somewhere in build_decoder, but couldn't find one yet.\n",
        "# You can workaround by setting hparams.decoder_length=max len of tweet in training set.\n",
        "# InvalidArgumentError: logits and labels must have the same first dimension, got logits shape [48,50] and labels shape [54]\n",
        "#\t [[Node: root/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, \n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "def data_dir():\n",
        "    target_dir = \"{}/chatbot_data\".format(str(Path.home()))\n",
        "    if not colab() and not os.path.exists(target_dir):\n",
        "        raise Exception(\"{} not found, you may create\".format(target_dir))\n",
        "    return target_dir\n",
        "\n",
        "\n",
        "def info(message, hparams):\n",
        "    if hparams.debug_verbose:\n",
        "        print(message)\n",
        "\n",
        "\n",
        "def p(path):\n",
        "    if colab():\n",
        "        return path\n",
        "    else:\n",
        "        return \"{}/{}\".format(data_dir(), path)\n",
        "\n",
        "\n",
        "def has_gpu0():\n",
        "    return tf.test.gpu_device_name() == \"/device:GPU:0\"\n",
        "\n",
        "\n",
        "class ModelDirectory(Enum):\n",
        "    tweet_large = 'model/tweet_large'\n",
        "    tweet_large_rl = 'model/tweet_large_rl'\n",
        "    tweet_large_swapped = 'model/tweet_large_swapped'\n",
        "    tweet_small = 'model/tweet_small'\n",
        "    tweet_small_swapped = 'model/tweet_small_swapped'\n",
        "    tweet_small_rl = 'model/tweet_small_rl'\n",
        "    test_multiple1 = 'model/test_multiple1'\n",
        "    test_multiple2 = 'model/test_multiple2'\n",
        "    test_multiple3 = 'model/test_multiple3'\n",
        "    test_distributed = 'model/test_distributed'\n",
        "\n",
        "    @staticmethod\n",
        "    def create_all_directories():\n",
        "        for d in ModelDirectory:\n",
        "            os.makedirs(p(d.value), exist_ok=True)\n",
        "\n",
        "\n",
        "# todo\n",
        "# collect all initializer\n",
        "ModelDirectory.create_all_directories()  \n",
        "\n",
        "base_hparams = tf.contrib.training.HParams(\n",
        "    machine=current_client_id,\n",
        "    batch_size=3,\n",
        "    num_units=6,\n",
        "    num_layers=2,\n",
        "    vocab_size=9,\n",
        "    embedding_size=8,\n",
        "    learning_rate=0.01,\n",
        "    learning_rate_decay=0.99,\n",
        "    use_attention=False,\n",
        "    encoder_length=5,\n",
        "    decoder_length=5,    \n",
        "    max_gradient_norm=5.0,\n",
        "    beam_width=2,\n",
        "    num_train_steps=100,\n",
        "    debug_verbose=False,\n",
        "    model_path='Please override model_directory',\n",
        "    sos_id=0,\n",
        "    eos_id=1,\n",
        "    pad_id=2,\n",
        "    unk_id=3,\n",
        "    sos_token=\"[SOS]\",\n",
        "    eos_token=\"[EOS]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    unk_token=\"[UNK]\",\n",
        ")\n",
        "\n",
        "test_hparams = copy.deepcopy(base_hparams).override_from_dict(\n",
        "    {'beam_width': 0, 'num_train_steps': 100, 'learning_rate': 0.5})\n",
        "\n",
        "\n",
        "\n",
        "test_attention_hparams = copy.deepcopy(test_hparams).override_from_dict(\n",
        "    {'use_attention': True})\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jPAsDmlILC-y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_hparams(hparams):\n",
        "  result = {}\n",
        "  for key in ['machine', 'batch_size', 'num_units', 'num_layers', 'vocab_size',\n",
        "              'embedding_size', 'learning_rate', 'learning_rate_decay',\n",
        "              'use_attention', 'encoder_length', 'decoder_length',\n",
        "              'max_gradient_norm', 'beam_width', 'num_train_steps',\n",
        "              'model_path']:\n",
        "    result[key] = hparams.get(key)\n",
        "  print(result)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DFEYKvBwL3Nm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For debug purpose.\n",
        "tf.reset_default_graph()\n",
        "\n",
        "\n",
        "class ChatbotModel:\n",
        "    def __init__(self, sess, hparams, model_path, scope='ChatbotModel'):\n",
        "        self.sess = sess\n",
        "        # todo remove\n",
        "        self.hparams = hparams\n",
        "\n",
        "        # todo\n",
        "        self.model_path = model_path\n",
        "        self.name = scope\n",
        "\n",
        "        self.encoder_inputs, self.encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder = self._build_encoder(\n",
        "            hparams, scope)\n",
        "        self.decoder_inputs, self.decoder_target_lengths, self.logits = self._build_decoder(\n",
        "            hparams, self.encoder_inputs_lengths, embedding_encoder,\n",
        "            encoder_state, encoder_outputs)\n",
        "\n",
        "        self.reward = tf.placeholder(tf.float32, name=\"reward\")\n",
        "        self.target_labels, self.loss, self.global_step, self.learning_rate, self.train_op = self._build_optimizer(\n",
        "            hparams, self.logits)\n",
        "        self.train_loss_summary = tf.summary.scalar(\"loss\", self.loss)\n",
        "        self.reward_summary = tf.summary.scalar(\"reward\", self.reward)\n",
        "        self.valiation_loss_summary = tf.summary.scalar(\"validation_loss\", self.loss)\n",
        "        self.merged_summary = tf.summary.merge_all()\n",
        "\n",
        "        # Initialize saver after model created\n",
        "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
        "\n",
        "    def restore(self):\n",
        "        ckpt = tf.train.get_checkpoint_state(self.model_path)\n",
        "        if ckpt:\n",
        "            last_model = ckpt.model_checkpoint_path\n",
        "            self.saver.restore(self.sess, last_model)\n",
        "            return True\n",
        "        else:\n",
        "            print(\"Created fresh model.\")\n",
        "            return False\n",
        "\n",
        "    def train(self, encoder_inputs, encoder_inputs_lengths, target_labels,\n",
        "              decoder_inputs, decoder_target_lengths, reward=1.0):\n",
        "        feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "            self.target_labels: target_labels,\n",
        "            self.decoder_inputs: decoder_inputs,\n",
        "            self.decoder_target_lengths: decoder_target_lengths,\n",
        "            # For normal Seq2Seq reward is always 1.\n",
        "            self.reward: reward\n",
        "        }\n",
        "        _, global_step, summary, reward_summary = self.sess.run(\n",
        "            [self.train_op, self.global_step, self.train_loss_summary, self.reward_summary], feed_dict=feed_dict)\n",
        "\n",
        "        return global_step, self.learning_rate, summary, reward_summary, reward\n",
        "\n",
        "    def batch_loss(self, encoder_inputs, encoder_inputs_lengths, target_labels,\n",
        "                   decoder_inputs, decoder_target_lengths):\n",
        "        feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "            self.target_labels: target_labels,\n",
        "            self.decoder_inputs: decoder_inputs,\n",
        "            self.decoder_target_lengths: decoder_target_lengths,\n",
        "            # For normal Seq2Seq reward is always 1.\n",
        "            self.reward: 1\n",
        "        }\n",
        "        return self.sess.run([self.loss, self.valiation_loss_summary],\n",
        "                             feed_dict=feed_dict)\n",
        "      \n",
        "    def input_length(self, input):\n",
        "      try :\n",
        "        l = input.index(self.hparams.eos_id)\n",
        "        return l + 1\n",
        "      except ValueError:\n",
        "        return self.hparams.encoder_length\n",
        "\n",
        "    def train_with_reward(self, infer_model, standard_seq2seq_model,\n",
        "                          encoder_inputs, encoder_inputs_lengths, target_labels,\n",
        "                          decoder_inputs, decoder_target_lengths,\n",
        "                          dull_responses):\n",
        "        infered_replies = infer_model.infer(encoder_inputs,\n",
        "                                            encoder_inputs_lengths)\n",
        "#        print(infered_replies)\n",
        "        standard_seq2seq_encoder_inputs = []\n",
        "        standard_seq2seq_encoder_inputs_lengths = []\n",
        "        for reply in infered_replies:\n",
        "            l = self.input_length(reply.tolist())\n",
        "            standard_seq2seq_encoder_inputs_lengths.append(l)\n",
        "            if l <= self.hparams.encoder_length:\n",
        "                standard_seq2seq_encoder_inputs.append(np.append(reply, (\n",
        "                        [self.hparams.pad_id] * (self.hparams.encoder_length - len(reply)))))\n",
        "                None\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    \"Inferred\"\n",
        "                    \" reply shouldn't be longer than encoder_input\")\n",
        "        standard_seq2seq_encoder_inputs = np.transpose(\n",
        "            np.array(standard_seq2seq_encoder_inputs))\n",
        "        if True:\n",
        "#           print(standard_seq2seq_encoder_inputs_lengths)\n",
        "           reward1 = np.mean(standard_seq2seq_encoder_inputs_lengths) / self.hparams.encoder_length\n",
        "        else:\n",
        "          reward1 = standard_seq2seq_model.reward_ease_of_answering(\n",
        "              standard_seq2seq_encoder_inputs,\n",
        "              standard_seq2seq_encoder_inputs_lengths, dull_responses)\n",
        "\n",
        "\n",
        "        reward2 = 0  # todo\n",
        "        reward3 = 0  # todo\n",
        "        reward = 0.25 * reward1 + 0.25 * reward2 + 0.5 * reward3\n",
        "        \n",
        "        return self.train(encoder_inputs, encoder_inputs_lengths, target_labels,\n",
        "                          decoder_inputs, decoder_target_lengths, reward)\n",
        "\n",
        "    def save(self, model_path=None):\n",
        "        if model_path is None:\n",
        "            model_path = self.model_path\n",
        "        model_dir = \"{}/{}\".format(model_path, self.name)\n",
        "        self.saver.save(self.sess, model_dir, global_step=self.global_step)\n",
        "\n",
        "    @staticmethod\n",
        "    def _softmax(x):\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    def _build_optimizer(self, hparams, logits):\n",
        "        # Target labels\n",
        "        #   As described in doc for sparse_softmax_cross_entropy_with_logits,\n",
        "        #   labels should be [batch_size, decoder_target_lengths]\n",
        "        #   instead of [batch_size, decoder_target_lengths, vocab_size].\n",
        "        #   So labels should have indices instead of vocab_size classes.\n",
        "        target_labels = tf.placeholder(tf.int32, shape=(\n",
        "            hparams.batch_size, hparams.decoder_length), name=\"target_labels\")\n",
        "        \n",
        "        # Loss\n",
        "        #   target_labels: [batch_size, decoder_length]\n",
        "        #   logits: [batch_size, decoder_length, vocab_size]\n",
        "        #   crossent: [batch_size, decoder_length]\n",
        "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            labels=target_labels, logits=logits)\n",
        "        \n",
        "        target_weights = tf.sequence_mask(self.decoder_target_lengths, hparams.decoder_length, dtype=logits.dtype)\n",
        "\n",
        "        loss = tf.reduce_sum(\n",
        "          crossent * target_weights) / tf.to_float(hparams.batch_size)\n",
        "        \n",
        "        # Adjust loss with reward.\n",
        "        loss = tf.multiply(loss, self.reward)\n",
        "\n",
        "        # Train\n",
        "        global_step = tf.get_variable(name=\"global_step\", shape=[],\n",
        "                                      dtype=tf.int32,\n",
        "                                      initializer=tf.constant_initializer(0),\n",
        "                                      trainable=False)\n",
        "        \n",
        "        learning_rate = hparams.learning_rate # tf.train.exponential_decay(hparams.learning_rate, global_step, 100, hparams.learning_rate_decay) \n",
        "\n",
        "        # Calculate and clip gradients\n",
        "        params = tf.trainable_variables()\n",
        "        for param in params:\n",
        "          info(\"  {}, {}, {}\".format(param.name, str(param.get_shape()),\n",
        "                                        param.op.device), hparams)\n",
        "        \n",
        "        gradients = tf.gradients(loss, params)\n",
        "        clipped_gradients, _ = tf.clip_by_global_norm(\n",
        "            gradients, hparams.max_gradient_norm)\n",
        "\n",
        "        # Optimization\n",
        "#        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "        device = '/cpu:0'\n",
        "        if has_gpu0():\n",
        "            device = '/gpu:0'\n",
        "            print(\"!!!GPU ENABLED !!!\")\n",
        "        with tf.device(device):\n",
        "            train_op = optimizer.apply_gradients(\n",
        "                zip(clipped_gradients, params), global_step=global_step)\n",
        "        return target_labels, loss, global_step, learning_rate, train_op\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_encoder(hparams, scope):\n",
        "        # Encoder\n",
        "        #   encoder_inputs: [encoder_length, batch_size]\n",
        "        #   This is time major where encoder_length comes\n",
        "        #   first instead of batch_size.\n",
        "        #   encoder_inputs_lengths: [batch_size]\n",
        "        encoder_inputs = tf.placeholder(tf.int32, shape=(\n",
        "            hparams.encoder_length, hparams.batch_size), name=\"encoder_inputs\")\n",
        "        encoder_inputs_lengths = tf.placeholder(tf.int32,\n",
        "                                                shape=hparams.batch_size,\n",
        "                                                name=\"encoder_inputs_lengtsh\")\n",
        "\n",
        "        # Embedding\n",
        "        #   We originally didn't share embedding between encoder and decoder.\n",
        "        #   But now we share it. It makes much easier to calculate rewards.\n",
        "        #   Matrix for embedding: [vocab_size, embedding_size]\n",
        "        #   Should be shared between training and inference.\n",
        "        with tf.variable_scope(scope):\n",
        "            embedding_encoder = tf.get_variable(\"embedding_encoder\",\n",
        "                                                [hparams.vocab_size,\n",
        "                                                 hparams.embedding_size])\n",
        "\n",
        "        # Look up embedding:\n",
        "        #   encoder_inputs: [encoder_length, batch_size]\n",
        "        #   encoder_emb_inputs: [encoder_length, batch_size, embedding_size]\n",
        "        encoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder,\n",
        "                                                    encoder_inputs)\n",
        "\n",
        "        # LSTM cell.\n",
        "        with tf.variable_scope(scope):\n",
        "            # Should be shared between training and inference.\n",
        "            cell_list = []\n",
        "            for _ in range(hparams.num_layers):\n",
        "               cell_list.append(tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
        "            encoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
        "\n",
        "        # Run Dynamic RNN\n",
        "        #   encoder_outputs: [encoder_length, batch_size, num_units]\n",
        "        #   encoder_state: [batch_size, num_units],\n",
        "        #   this is final state of the cell for each batch.\n",
        "        with tf.variable_scope(scope):\n",
        "            encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell,\n",
        "                                                               encoder_emb_inputs,\n",
        "                                                               time_major=True,\n",
        "                                                               dtype=tf.float32,\n",
        "                                                               sequence_length=encoder_inputs_lengths)\n",
        "\n",
        "        return encoder_inputs, encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_training_decoder(hparams, encoder_inputs_lengths,\n",
        "                                encoder_state, encoder_outputs, decoder_cell,\n",
        "                                decoder_emb_inputs, decoder_target_lengths,\n",
        "                                projection_layer):\n",
        "        # Decoder with helper:\n",
        "        #   decoder_emb_inputs: [decoder_length, batch_size, embedding_size]\n",
        "        #   decoder_target_lengths: [batch_size] vector,\n",
        "        #   which represents each target sequence length.\n",
        "        training_helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inputs,\n",
        "                                                            decoder_target_lengths,\n",
        "                                                            time_major=True)\n",
        "\n",
        "        # See https://github.com/tensorflow/tensorflow/issues/11904\n",
        "        if hparams.use_attention:\n",
        "            # Attention\n",
        "            # encoder_outputs is time major, so transopse it to batch major.\n",
        "            # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
        "            attention_encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2])\n",
        "\n",
        "            # Create an attention mechanism\n",
        "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "                hparams.num_units,\n",
        "                attention_encoder_outputs,\n",
        "                memory_sequence_length=encoder_inputs_lengths)\n",
        "\n",
        "            wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "                decoder_cell, attention_mechanism,\n",
        "                attention_layer_size=hparams.num_units)\n",
        "\n",
        "            initial_state = wrapped_decoder_cell.zero_state(hparams.batch_size,\n",
        "                                                            tf.float32).clone(\n",
        "                cell_state=encoder_state)\n",
        "        else:\n",
        "            wrapped_decoder_cell = decoder_cell\n",
        "            initial_state = encoder_state\n",
        "\n",
        "            # Decoder and decode\n",
        "        training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "            wrapped_decoder_cell, training_helper, initial_state,\n",
        "            output_layer=projection_layer)\n",
        "\n",
        "        # Dynamic decoding\n",
        "        #   final_outputs.rnn_output: [batch_size, decoder_length,\n",
        "        #                             vocab_size], list of RNN state.\n",
        "        #   final_outputs.sample_id: [batch_size, decoder_length],\n",
        "        #                            list of argmax of rnn_output.\n",
        "        #   final_state: [batch_size, num_units],\n",
        "        #                list of final state of RNN on decode process.\n",
        "        #   final_sequence_lengths: [batch_size], list of each decoded sequence. \n",
        "        final_outputs, _final_state, _final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n",
        "            training_decoder)\n",
        "\n",
        "        if hparams.debug_verbose:\n",
        "            print(\"rnn_output.shape=\", final_outputs.rnn_output.shape)\n",
        "            print(\"sample_id.shape=\", final_outputs.sample_id.shape)\n",
        "            print(\"final_state=\", _final_state)\n",
        "            print(\"final_sequence_lengths.shape=\",\n",
        "                  _final_sequence_lengths.shape)\n",
        "\n",
        "        logits = final_outputs.rnn_output\n",
        "        return logits, wrapped_decoder_cell, initial_state\n",
        "\n",
        "    def _build_decoder(self, hparams, encoder_inputs_lengths, embedding_encoder,\n",
        "                       encoder_state, encoder_outputs):\n",
        "        # Decoder input\n",
        "        #   decoder_inputs: [decoder_length, batch_size]\n",
        "        #   decoder_target_lengths: [batch_size]\n",
        "        #   This is grand truth target inputs for training.\n",
        "        decoder_inputs = tf.placeholder(tf.int32, shape=(\n",
        "            hparams.decoder_length, hparams.batch_size), name=\"decoder_inputs\")\n",
        "        decoder_target_lengths = tf.placeholder(tf.int32,\n",
        "                                                shape=hparams.batch_size,\n",
        "                                                name=\"decoder_target_lengths\")\n",
        "\n",
        "        # Look up embedding:\n",
        "        #   decoder_inputs: [decoder_length, batch_size]\n",
        "        #   decoder_emb_inp: [decoder_length, batch_size, embedding_size]\n",
        "        decoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder,\n",
        "                                                    decoder_inputs)\n",
        "\n",
        "        # https://stackoverflow.com/questions/39573188/output-projection-in-seq2seq-model-tensorflow\n",
        "        # Internally, a neural network operates on dense vectors of some size,\n",
        "        # often 256, 512 or 1024 floats (let's say 512 for here). \n",
        "        # But at the end it needs to predict a word\n",
        "        # from the vocabulary which is often much larger,\n",
        "        # e.g., 40000 words. Output projection is the final linear layer\n",
        "        # that converts (projects) from the internal representation\n",
        "        #  to the larger one.\n",
        "        # So, for example, it can consist of a 512 x 40000 parameter matrix\n",
        "        # and a 40000 parameter for the bias vector.\n",
        "        projection_layer = layers_core.Dense(hparams.vocab_size, use_bias=False)\n",
        "\n",
        "        # We share this between training and inference.\n",
        "        cell_list = []\n",
        "        for _ in range(hparams.num_layers):\n",
        "           cell_list.append(tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
        "        decoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
        "        \n",
        "#        decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
        "\n",
        "        # Training graph\n",
        "        logits, wrapped_decoder_cell, initial_state = self._build_training_decoder(\n",
        "            hparams, encoder_inputs_lengths, encoder_state, encoder_outputs,\n",
        "            decoder_cell, decoder_emb_inputs, decoder_target_lengths,\n",
        "            projection_layer)\n",
        "\n",
        "        return decoder_inputs, decoder_target_lengths, logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JzDknaQZV-iU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ChatbotInferenceModel:\n",
        "    def __init__(self, sess, hparams, model_path, scope='ChatbotModel'):\n",
        "        self.sess = sess\n",
        "        # todo remove\n",
        "        self.hparams = hparams\n",
        "\n",
        "        # todo\n",
        "        self.model_path = model_path\n",
        "        self.name = scope\n",
        "\n",
        "        self.encoder_inputs, self.encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder = self._build_encoder(\n",
        "            hparams, scope)\n",
        "        self.decoder_inputs, self.decoder_target_lengths, self.replies, self.beam_replies, self.infer_logits = self._build_decoder(\n",
        "            hparams, self.encoder_inputs_lengths, embedding_encoder,\n",
        "            encoder_state, encoder_outputs)\n",
        "\n",
        "        self.reward = tf.placeholder(tf.float32, name=\"reward\")\n",
        "\n",
        "        # we can't use variable length here, \n",
        "        # because tiled_batch requires constant length.\n",
        "        self.batch_size = 1\n",
        "\n",
        "        # Initialize saver after model created\n",
        "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
        "\n",
        "    def restore(self):\n",
        "        ckpt = tf.train.get_checkpoint_state(self.model_path)\n",
        "        if ckpt:\n",
        "            last_model = ckpt.model_checkpoint_path\n",
        "            self.saver.restore(self.sess, last_model)\n",
        "            return True\n",
        "        else:\n",
        "            print(\"Created fresh model.\")\n",
        "            return False\n",
        "\n",
        "    def infer(self, encoder_inputs, encoder_inputs_lengths):\n",
        "        # Should not call this when beam search enabled.\n",
        "        assert(self.hparams.beam_width == 0)\n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "        }\n",
        "        replies = self.sess.run(self.replies, feed_dict=inference_feed_dict)\n",
        "        return replies\n",
        "\n",
        "    def infer_beam_search(self, encoder_inputs, encoder_inputs_lengths):\n",
        "        # Should not call this when beam search disabled.\n",
        "        assert(self.hparams.beam_width > 0)      \n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "        }\n",
        "        replies = self.sess.run(self.beam_replies,\n",
        "                                feed_dict=inference_feed_dict)\n",
        "        return replies\n",
        "     \n",
        "    # imakoko\n",
        "    def infer_mi(self, swapped_model, encoder_inputs, encoder_inputs_lengths):\n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "        }\n",
        "        beam_replies = self.sess.run(self.beam_replies, feed_dict=inference_feed_dict)\n",
        "        # beam_replis [batch_size, length, , batch_width]\n",
        "        # for now we assume encoder_inputs is batch_size = 1\n",
        "        \n",
        "        swapped_encoder_inputs = beam_replies[0]\n",
        "        # beam_width = batch_size\n",
        "        swapped_batch_size = swapped_encoder_inputs.shape[1]\n",
        "        \n",
        "        # beam_replies can be shorten less than decoder_output_legth, so we pad them.\n",
        "        paddings = tf.constant([[0, self.hparams.encoder_length - swapped_encoder_inputs.shape[0],], [0, 0]])\n",
        "        swapped_encoder_inputs = swapped_model.sess.run(tf.pad(swapped_encoder_inputs, paddings, \"CONSTANT\", constant_values=self.hparams.pad_id))\n",
        "        swapped_encoder_inputs_lengths = np.empty(swapped_batch_size, dtype=np.int)\n",
        "        for i in range(swapped_batch_size):\n",
        "          swapped_encoder_inputs_lengths[i] = swapped_encoder_inputs.shape[0]\n",
        "        \n",
        "        return swapped_model.infer_beam_search(swapped_encoder_inputs, swapped_encoder_inputs_lengths)\n",
        "        # todo make correct length\n",
        "#        for repy in beam_replies:\n",
        "          # logits from swapped_model for this reply\n",
        "          # cals prob for in original encoder_input\n",
        "      \n",
        "\n",
        "    def log_prob(self, encoder_inputs, encoder_inputs_lengths, expected_output):\n",
        "        \"\"\"Return sum of log probability of given\n",
        "           one specific expected_output for encoder_inputs.\n",
        "    \n",
        "        Args:\n",
        "            encoder_inputs: [encoder_length, batch_size], eg) tweets\n",
        "            expected_output: [1, decoder_length or less than decoder_length],\n",
        "            eg) One reply.\n",
        "    \n",
        "        Returns:\n",
        "            Return log probablity of expected output for given encoder inputs.\n",
        "            eg) sum of log probability of reply \"Good\" when given [\"How are you?\",\n",
        "             \"What's up?\"]\n",
        "        \"\"\"\n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths\n",
        "        }\n",
        "\n",
        "        # Logits\n",
        "        #   logits_value: [batch_size, actual_decoder_length, vocab_size]\n",
        "        logits_batch_value = self.sess.run(self.infer_logits,\n",
        "                                           feed_dict=inference_feed_dict)\n",
        "\n",
        "        sum_p = []\n",
        "        # For each batch: [actual_decoder_length, vocab_size]\n",
        "        for logits in logits_batch_value:\n",
        "            p = 1\n",
        "            # Note that expected_output and logits don't always have\n",
        "            # same length, but zip takes care of the case.\n",
        "            for word_id, logit in zip(expected_output, logits):\n",
        "                # Apply softmax first, see definition of softmax.\n",
        "                norm = (self._softmax(logit))[word_id]\n",
        "                p *= norm\n",
        "            p = np.log(p)\n",
        "            sum_p.append(p)\n",
        "        ret = np.sum(sum_p) / len(sum_p)\n",
        "        return ret\n",
        "\n",
        "    def reward_ease_of_answering(self, encoder_inputs, encoder_inputs_lengths,\n",
        "                                 expected_outputs):\n",
        "        \"\"\" Return reward for ease of answering. \n",
        "            See Deep Reinforcement Learning for Dialogue Generation\n",
        "            for more details.\n",
        "    \n",
        "        Args:\n",
        "            encoder_inputs: [encoder_length, batch_size], eg) tweets\n",
        "            expected_outputs: [number of pre-defined dull responses,\n",
        "            decoder_length or less than decoder_length].\n",
        "            eg) [[\"I'm\", \"Good\"], [\"fine\"]]\n",
        "    \n",
        "        Returns:\n",
        "            Return reward for ease of answering.\n",
        "            Note that this can be calculated\n",
        "            by calling log_prob function for each dull response,\n",
        "            but this function is more efficient\n",
        "            because this calculated the reward at once.\n",
        "        \"\"\"\n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths\n",
        "        }\n",
        "\n",
        "        # Logits\n",
        "        #   logits_value: [batch_size, actual_decoder_length, vocab_size]\n",
        "        logits_batch_value = self.sess.run(self.infer_logits,\n",
        "                                           feed_dict=inference_feed_dict)\n",
        "\n",
        "        batch_sum_p = []\n",
        "        # For each batch: [actual_decoder_length, vocab_size]\n",
        "        for logits in logits_batch_value:\n",
        "            sum_p = []\n",
        "            for expected_output in expected_outputs:\n",
        "                p = 1\n",
        "                # Note that expected_output and logits don't\n",
        "                # always have same length, but zip takes care of the case.\n",
        "                for word_id, logit in zip(expected_output, logits):\n",
        "                    # Apply softmax first, see definition of softmax.\n",
        "                    norm = (self._softmax(logit))[word_id]\n",
        "                    p *= norm\n",
        "                p = np.log(p) / len(expected_output)\n",
        "                sum_p.append(p)\n",
        "            one_batch_p = np.sum(sum_p)\n",
        "            batch_sum_p.append(one_batch_p)\n",
        "        ret = np.sum(batch_sum_p) / len(batch_sum_p)\n",
        "        return -ret\n",
        "\n",
        "    @staticmethod\n",
        "    def _softmax(x):\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_encoder(hparams, scope):\n",
        "        # Encoder\n",
        "        #   encoder_inputs: [encoder_length, batch_size]\n",
        "        #   This is time major where encoder_length\n",
        "        #   comes first instead of batch_size.\n",
        "        #   encoder_inputs_lengths: [batch_size]\n",
        "        encoder_inputs = tf.placeholder(tf.int32,\n",
        "                                        shape=[hparams.encoder_length, None],\n",
        "                                        name=\"encoder_inputs\")\n",
        "        encoder_inputs_lengths = tf.placeholder(tf.int32, shape=[None],\n",
        "                                                name=\"encoder_inputs_lengths\")\n",
        "\n",
        "        # Embedding\n",
        "        #   We originally didn't share embedding between encoder and decoder.\n",
        "        #   But now we share it. It makes much easier to calculate rewards.\n",
        "        #   Matrix for embedding: [vocab_size, embedding_size]\n",
        "        #   Should be shared between training and inference.\n",
        "        with tf.variable_scope(scope):\n",
        "            embedding_encoder = tf.get_variable(\"embedding_encoder\",\n",
        "                                                [hparams.vocab_size,\n",
        "                                                 hparams.embedding_size])\n",
        "\n",
        "        # Look up embedding:\n",
        "        #   encoder_inputs: [encoder_length, batch_size]\n",
        "        #   encoder_emb_inputs: [encoder_length, batch_size, embedding_size]\n",
        "        encoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder,\n",
        "                                                    encoder_inputs)\n",
        "\n",
        "        # LSTM cell.\n",
        "        with tf.variable_scope(scope):\n",
        "            # Should be shared between training and inference.\n",
        "#            encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
        "            cell_list = []\n",
        "            for _ in range(hparams.num_layers):\n",
        "               cell_list.append(tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
        "            encoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
        "\n",
        "        # Run Dynamic RNN\n",
        "        #   encoder_outputs: [encoder_length, batch_size, num_units]\n",
        "        #   encoder_state: [batch_size, num_units],\n",
        "        #   this is final state of the cell for each batch.\n",
        "        with tf.variable_scope(scope):\n",
        "            encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell,\n",
        "                                                               encoder_emb_inputs,\n",
        "                                                               time_major=True,\n",
        "                                                               dtype=tf.float32,\n",
        "                                                               sequence_length=encoder_inputs_lengths)\n",
        "\n",
        "        return encoder_inputs, encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_greedy_inference(hparams, embedding_encoder, encoder_state,\n",
        "                                encoder_inputs_lengths, encoder_outputs,\n",
        "                                decoder_cell, projection_layer):\n",
        "        if hparams.beam_width > 0:\n",
        "          return None, None\n",
        "        \n",
        "        # Greedy decoder\n",
        "        dynamic_batch_size = tf.shape(encoder_inputs_lengths)[0]\n",
        "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
        "            embedding_encoder,\n",
        "            tf.fill([dynamic_batch_size], hparams.sos_id), hparams.eos_id)\n",
        "\n",
        "        # See https://github.com/tensorflow/tensorflow/issues/11904\n",
        "        if hparams.use_attention:\n",
        "            # Attention\n",
        "            # encoder_outputs is time major, so transopse it to batch major.\n",
        "            # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
        "            attention_encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2])\n",
        "\n",
        "            # Create an attention mechanism\n",
        "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "                hparams.num_units,\n",
        "                attention_encoder_outputs,\n",
        "                memory_sequence_length=encoder_inputs_lengths)\n",
        "\n",
        "            wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "                decoder_cell, attention_mechanism,\n",
        "                attention_layer_size=hparams.num_units)\n",
        "\n",
        "            initial_state = wrapped_decoder_cell.zero_state(dynamic_batch_size,\n",
        "                                                            tf.float32).clone(\n",
        "                cell_state=encoder_state)\n",
        "        else:\n",
        "            wrapped_decoder_cell = decoder_cell\n",
        "            initial_state = encoder_state\n",
        "\n",
        "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "            wrapped_decoder_cell, inference_helper, initial_state,\n",
        "            output_layer=projection_layer)\n",
        "\n",
        "        # len(inferred_reply) is lte encoder_length,\n",
        "        # because we are targeting tweet (140 for each tweet)\n",
        "        # Also by doing this,\n",
        "        # we can pass the reply to other seq2seq w/o shorten it.\n",
        "        maximum_iterations = hparams.encoder_length\n",
        "\n",
        "        # Dynamic decoding\n",
        "        outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "            inference_decoder, maximum_iterations=maximum_iterations)\n",
        "        replies = outputs.sample_id\n",
        "\n",
        "        # We use infer_logits instead of logits when calculating log_prob,\n",
        "        # because infer_logits doesn't require decoder_target_lengths input.\n",
        "        infer_logits = outputs.rnn_output\n",
        "        return infer_logits, replies\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_beam_search_inference(hparams, encoder_inputs_lengths,\n",
        "                                     embedding_encoder, encoder_state,\n",
        "                                     encoder_outputs, decoder_cell,\n",
        "                                     projection_layer):\n",
        "      \n",
        "        if hparams.beam_width == 0:\n",
        "          return None\n",
        "      \n",
        "        dynamic_batch_size = tf.shape(encoder_inputs_lengths)[0]\n",
        "        # https://github.com/tensorflow/tensorflow/issues/11904\n",
        "        if hparams.use_attention:\n",
        "            # Attention\n",
        "            # encoder_outputs is time major, so transopse it to batch major.\n",
        "            # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
        "            attention_encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2])\n",
        "\n",
        "            tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(\n",
        "                attention_encoder_outputs, multiplier=hparams.beam_width)\n",
        "            tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(\n",
        "                encoder_state, multiplier=hparams.beam_width)\n",
        "            tiled_encoder_inputs_lengths = tf.contrib.seq2seq.tile_batch(\n",
        "                encoder_inputs_lengths, multiplier=hparams.beam_width)\n",
        "\n",
        "            # Create an attention mechanism\n",
        "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "                hparams.num_units, tiled_encoder_outputs,\n",
        "                memory_sequence_length=tiled_encoder_inputs_lengths)\n",
        "\n",
        "            wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "                decoder_cell, attention_mechanism,\n",
        "                attention_layer_size=hparams.num_units)\n",
        "\n",
        "            decoder_initial_state = wrapped_decoder_cell.zero_state(\n",
        "                dtype=tf.float32,\n",
        "                batch_size=dynamic_batch_size * hparams.beam_width)\n",
        "            decoder_initial_state = decoder_initial_state.clone(\n",
        "                cell_state=tiled_encoder_final_state)\n",
        "        else:\n",
        "            wrapped_decoder_cell = decoder_cell\n",
        "            decoder_initial_state = tf.contrib.seq2seq.tile_batch(encoder_state,\n",
        "                                                                  multiplier=hparams.beam_width)\n",
        "\n",
        "        # len(inferred_reply) is lte encoder_length,\n",
        "        # because we are targeting tweet (140 for each tweet)\n",
        "        # Also by doing this,\n",
        "        # we can pass the reply to other seq2seq w/o shorten it.\n",
        "        maximum_iterations = hparams.encoder_length\n",
        "\n",
        "        inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
        "            cell=wrapped_decoder_cell,\n",
        "            embedding=embedding_encoder,\n",
        "            start_tokens=tf.fill([dynamic_batch_size], hparams.sos_id),\n",
        "            end_token=hparams.eos_id,\n",
        "            initial_state=decoder_initial_state,\n",
        "            beam_width=hparams.beam_width,\n",
        "            output_layer=projection_layer,\n",
        "            length_penalty_weight=0.0)\n",
        "\n",
        "        # Dynamic decoding\n",
        "        beam_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "            inference_decoder, maximum_iterations=maximum_iterations)\n",
        "        beam_replies = beam_outputs.predicted_ids\n",
        "        return beam_replies\n",
        "\n",
        "    def _build_decoder(self, hparams, encoder_inputs_lengths, embedding_encoder,\n",
        "                       encoder_state, encoder_outputs):\n",
        "        # Decoder input\n",
        "        #   decoder_inputs: [decoder_length, batch_size]\n",
        "        #   decoder_target_lengths: [batch_size]\n",
        "        #   This is grand truth target inputs for training.\n",
        "        decoder_inputs = tf.placeholder(tf.int32,\n",
        "                                        shape=[hparams.decoder_length, None],\n",
        "                                        name=\"decoder_inputs\")\n",
        "        decoder_target_lengths = tf.placeholder(tf.int32, shape=[None],\n",
        "                                                name=\"decoder_target_lengths\")\n",
        "\n",
        "        # https://stackoverflow.com/questions/39573188/output-projection-in-seq2seq-model-tensorflow\n",
        "        # Internally, a neural network operates on dense vectors of some size,\n",
        "        # often 256, 512 or 1024 floats (let's say 512 for here). \n",
        "        # But at the end it needs to predict a word\n",
        "        # from the vocabulary which is often much larger,\n",
        "        # e.g., 40000 words. Output projection is the final linear\n",
        "        # layer that converts (projects) from the internal\n",
        "        # representation to the larger one.\n",
        "        # So, for example, it can consist of a 512 x 40000 parameter\n",
        "        # matrix and a 40000 parameter for the bias vector.\n",
        "        projection_layer = layers_core.Dense(hparams.vocab_size, use_bias=False)\n",
        "\n",
        "        # We share this between training and inference.\n",
        "#        decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
        "        cell_list = []\n",
        "        for _ in range(hparams.num_layers):\n",
        "           cell_list.append(tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
        "        decoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
        "        \n",
        "\n",
        "        # Greedy Inference graph\n",
        "        infer_logits, replies = self._build_greedy_inference(hparams,\n",
        "                                                             embedding_encoder,\n",
        "                                                             encoder_state,\n",
        "                                                             encoder_inputs_lengths,\n",
        "                                                             encoder_outputs,\n",
        "                                                             decoder_cell,\n",
        "                                                             projection_layer)\n",
        "\n",
        "        # Beam Search Inference graph\n",
        "        beam_replies = self._build_beam_search_inference(hparams,\n",
        "                                                         encoder_inputs_lengths,\n",
        "                                                         embedding_encoder,\n",
        "                                                         encoder_state,\n",
        "                                                         encoder_outputs,\n",
        "                                                         decoder_cell,\n",
        "                                                         projection_layer)\n",
        "\n",
        "        return decoder_inputs, decoder_target_lengths, replies, beam_replies, infer_logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ul5WBjSF3vy9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class InferenceHelper:\n",
        "    def __init__(self, model, vocab, rev_vocab):\n",
        "        self.model = model\n",
        "        self.vocab = vocab\n",
        "        self.rev_vocab = rev_vocab\n",
        "\n",
        "    def inferences(self, tweet):\n",
        "        encoder_inputs, encoder_inputs_lengths = self.create_inference_input(\n",
        "            tweet)\n",
        "        if self.model.hparams.beam_width == 0:\n",
        "          replies = self.model.infer(encoder_inputs, encoder_inputs_lengths)\n",
        "          ids = replies[0].tolist()\n",
        "          return [self.sanitize_text(self.ids_to_words(ids))]\n",
        "        else:\n",
        "          beam_replies = self.model.infer_beam_search(encoder_inputs,\n",
        "                                                      encoder_inputs_lengths)\n",
        "\n",
        "          return [self.sanitize_text(self.ids_to_words(beam_replies[0][:, i])) for i in range(self.model.hparams.beam_width)]\n",
        "        \n",
        "    def sanitize_text(self, line):\n",
        "      line = re.sub(r\"\\[EOS\\]\", \" \", line)\n",
        "      line = re.sub(r\"\\[UNK\\]\", \"💩\", line)\n",
        "      return line\n",
        "\n",
        "    def print_inferences(self, tweet):\n",
        "        print(tweet)\n",
        "        for i, reply in enumerate(self.inferences(tweet)):\n",
        "            print(\"    [{}]{}\".format(i, reply))\n",
        "\n",
        "    def words_to_ids(self, words):\n",
        "        ids = []\n",
        "        for word in words:\n",
        "            if word in self.vocab:\n",
        "                ids.append(self.vocab[word])\n",
        "            else:\n",
        "                ids.append(self.model.hparams.unk_id)\n",
        "        return ids\n",
        "\n",
        "    def ids_to_words(self, ids):\n",
        "        words = \"\"\n",
        "        for id in ids:\n",
        "            words += self.rev_vocab[id]\n",
        "        return words\n",
        "\n",
        "    def create_inference_input(self, text):\n",
        "        inference_encoder_inputs = np.empty((self.model.hparams.encoder_length, 1),\n",
        "                                            dtype=np.int)\n",
        "        inference_encoder_inputs_lengths = np.empty(1, dtype=np.int)\n",
        "        text = TrainDataGenerator.sanitize_line(text)\n",
        "        tagger = MeCab.Tagger(\"-Owakati\")\n",
        "        words = tagger.parse(text).split()\n",
        "        ids = self.words_to_ids(words)\n",
        "        ids = ids[:self.model.hparams.encoder_length]\n",
        "        len_ids = len(ids)\n",
        "        ids.extend([self.model.hparams.pad_id] * (self.model.hparams.encoder_length - len(ids)))\n",
        "        for i in range(1):\n",
        "            inference_encoder_inputs[:, i] = np.array(ids, dtype=np.int)\n",
        "            inference_encoder_inputs_lengths[i] = len_ids\n",
        "        return inference_encoder_inputs, inference_encoder_inputs_lengths\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4U1o8gMTP1mA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TrainDataGenerator:\n",
        "    def __init__(self, source_path, hparams):\n",
        "        self.source_path = source_path\n",
        "        self.hparams = hparams\n",
        "        basename, extension = os.path.splitext(self.source_path)\n",
        "        self.enc_path = \"{}_enc{}\".format(basename, extension)\n",
        "        self.dec_path = \"{}_dec{}\".format(basename, extension)\n",
        "        self.enc_idx_path = \"{}_enc_idx{}\".format(basename, extension)\n",
        "        self.dec_idx_path = \"{}_dec_idx{}\".format(basename, extension)\n",
        "        self.dec_idx_eos_path = \"{}_dec_idx_eos{}\".format(basename, extension)\n",
        "        self.dec_idx_sos_path = \"{}_dec_idx_sos{}\".format(basename, extension)\n",
        "        self.dec_idx_len_path = \"{}_dec_idx_len{}\".format(basename, extension)\n",
        "\n",
        "        self.enc_idx_padded_path = \"{}_enc_idx_padded{}\".format(basename,\n",
        "                                                                extension)\n",
        "        self.enc_idx_len_path = \"{}_enc_idx_len{}\".format(basename, extension)\n",
        "\n",
        "        self.vocab_path = \"{}_vocab{}\".format(basename, extension)\n",
        "        \n",
        "        self.generated_files = [self.enc_path, self.dec_path, self.enc_idx_path, self.dec_idx_path, self.dec_idx_eos_path, self.dec_idx_sos_path, self.dec_idx_len_path, self.enc_idx_padded_path, self.vocab_path, self.enc_idx_len_path]\n",
        "        self.max_vocab_size = hparams.vocab_size\n",
        "        self.start_vocabs = [hparams.sos_token, hparams.eos_token, hparams.pad_token, hparams.unk_token]\n",
        "        self.tagger = MeCab.Tagger(\"-Owakati\")\n",
        "        \n",
        "    def remove_generated(self):\n",
        "      for f in self.generated_files:\n",
        "        if os.path.exists(f):\n",
        "          os.remove(f)\n",
        "\n",
        "    def generate(self):\n",
        "        print(\"generating enc and dec files...\")\n",
        "        self._generate_enc_dec()\n",
        "        print(\"generating vocab file...\")\n",
        "        self._generate_vocab()\n",
        "        print(\"loading vocab...\")\n",
        "        vocab, _ = self._load_vocab()\n",
        "        print(\"generating id files...\")\n",
        "        self._generate_id_file(self.enc_path, self.enc_idx_path, vocab)\n",
        "        self._generate_id_file(self.dec_path, self.dec_idx_path, vocab)\n",
        "        print(\"generating padded input file...\")\n",
        "        self._generate_enc_idx_padded(self.enc_idx_path,\n",
        "                                      self.enc_idx_padded_path,\n",
        "                                      self.enc_idx_len_path,\n",
        "                                      self.hparams.encoder_length)\n",
        "        print(\"generating dec eos/sos files...\")\n",
        "        self._generate_dec_idx_eos(self.dec_idx_path, self.dec_idx_eos_path,\n",
        "                                   self.hparams.decoder_length)\n",
        "        self._generate_dec_idx_sos(self.dec_idx_path, self.dec_idx_sos_path,\n",
        "                                   self.dec_idx_len_path,\n",
        "                                   self.hparams.decoder_length)\n",
        "        print(\"done\")\n",
        "        return self._create_dataset()\n",
        "\n",
        "    def _generate_id_file(self, source_path, dest_path, vocab):\n",
        "        if gfile.Exists(dest_path):\n",
        "            return\n",
        "        with gfile.GFile(source_path, mode=\"rb\") as f, gfile.GFile(dest_path,\n",
        "                                                                   mode=\"wb\") as of:\n",
        "            for line in f:\n",
        "                line = line.decode('utf-8')\n",
        "                words = self.tagger.parse(line).split()\n",
        "                ids = [vocab.get(w, self.hparams.unk_id) for w in words]\n",
        "                of.write(\" \".join([str(id) for id in ids]) + \"\\n\")\n",
        "\n",
        "    def _load_vocab(self):\n",
        "        rev_vocab = []\n",
        "        with gfile.GFile(self.vocab_path, mode=\"r\") as f:\n",
        "            rev_vocab.extend(f.readlines())\n",
        "            rev_vocab = [line.strip() for line in rev_vocab]\n",
        "            # Dictionary of (word, idx)\n",
        "            vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
        "            return vocab, rev_vocab\n",
        "\n",
        "    def _generate_vocab(self):\n",
        "        if gfile.Exists(self.vocab_path):\n",
        "            return\n",
        "        vocab_dic = self._build_vocab_dic(self.enc_path)\n",
        "        vocab_dic = self._build_vocab_dic(self.dec_path, vocab_dic)\n",
        "        vocab_list = self.start_vocabs + sorted(vocab_dic, key=vocab_dic.get,\n",
        "                                                reverse=True)\n",
        "        if len(vocab_list) > self.max_vocab_size:\n",
        "            vocab_list = vocab_list[:self.max_vocab_size]\n",
        "        with gfile.GFile(self.vocab_path, mode=\"w\") as vocab_file:\n",
        "            for w in vocab_list:\n",
        "                vocab_file.write(w + \"\\n\")\n",
        "\n",
        "    def _generate_enc_dec(self):\n",
        "        if gfile.Exists(self.enc_path) and gfile.Exists(self.dec_path):\n",
        "            return\n",
        "        with gfile.GFile(self.source_path, mode=\"rb\") as f, gfile.GFile(\n",
        "                self.enc_path, mode=\"w+\") as ef, gfile.GFile(self.dec_path,\n",
        "                                                             mode=\"w+\") as df:\n",
        "            tweet = None\n",
        "            reply = None\n",
        "            for i, line in enumerate(f):\n",
        "                line = line.decode('utf-8')\n",
        "                line = self.sanitize_line(line)\n",
        "                if i % 2 == 0:\n",
        "                  tweet = line\n",
        "                else:\n",
        "                  reply = line\n",
        "                  if tweet and reply:\n",
        "                    ef.write(tweet)\n",
        "                    df.write(reply)\n",
        "                  tweet = None\n",
        "                  reply = None\n",
        "\n",
        "    def _generate_enc_idx_padded(self, source_path, dest_path, dest_len_path,\n",
        "                                 max_line_len):\n",
        "        if gfile.Exists(dest_path):\n",
        "            return\n",
        "        with open(source_path) as fin, open(dest_path,\n",
        "                                            \"w\") as fout, open(dest_len_path,\n",
        "                                                               \"w\") as flen:\n",
        "            line = fin.readline()\n",
        "            while line:\n",
        "                ids = [int(x) for x in line.split()]\n",
        "                if len(ids) > max_line_len:\n",
        "#                    ids = ids[:max_line_len]\n",
        "                    ids = ids[-max_line_len:]\n",
        "                flen.write(str(len(ids)))\n",
        "                flen.write(\"\\n\")\n",
        "                if len(ids) < max_line_len:\n",
        "                    ids.extend([self.hparams.pad_id] * (max_line_len - len(ids)))\n",
        "                ids = [str(x) for x in ids]\n",
        "                fout.write(\" \".join(ids))\n",
        "                fout.write(\"\\n\")\n",
        "                line = fin.readline()\n",
        "\n",
        "    # read decoder_idx file and append eos at the end of idx list.\n",
        "    def _generate_dec_idx_eos(self, source_path, dest_path, max_line_len):\n",
        "        if gfile.Exists(dest_path):\n",
        "            return\n",
        "        with open(source_path) as fin, open(dest_path, \"w\") as fout:\n",
        "            line = fin.readline()\n",
        "            while line:\n",
        "                ids = [int(x) for x in line.split()]\n",
        "                if len(ids) > max_line_len - 1:\n",
        "#                    ids = ids[:max_line_len - 1]\n",
        "                  ids = ids[-(max_line_len - 1):]\n",
        "                ids.append(self.hparams.eos_id)\n",
        "                if len(ids) < max_line_len:\n",
        "                    ids.extend([self.hparams.pad_id] * (max_line_len - len(ids)))\n",
        "                ids = [str(x) for x in ids]\n",
        "                fout.write(\" \".join(ids))\n",
        "                fout.write(\"\\n\")\n",
        "                line = fin.readline()\n",
        "\n",
        "    # read decoder_idx file and put sos at the beginning of the idx list.\n",
        "    # also write out length of index list.\n",
        "    def _generate_dec_idx_sos(self, source_path, dest_path, dest_len_path,\n",
        "                              max_line_len):\n",
        "        if gfile.Exists(dest_path):\n",
        "            return\n",
        "        with open(source_path) as fin, open(dest_path, \"w\") as fout, open(\n",
        "                dest_len_path, \"w\") as flen:\n",
        "            line = fin.readline()\n",
        "            while line:\n",
        "                ids = [self.hparams.sos_id]\n",
        "                ids.extend([int(x) for x in line.split()])\n",
        "                if len(ids) > max_line_len:\n",
        "                    ids = ids[:max_line_len]\n",
        "                flen.write(str(len(ids)))\n",
        "                flen.write(\"\\n\")\n",
        "                if len(ids) < max_line_len:\n",
        "                    ids.extend([self.hparams.pad_id] * (max_line_len - len(ids)))\n",
        "                ids = [str(x) for x in ids]\n",
        "                fout.write(\" \".join(ids))\n",
        "                fout.write(\"\\n\")\n",
        "                line = fin.readline()\n",
        "\n",
        "    @staticmethod\n",
        "    def sanitize_line(line):\n",
        "        # replace @username\n",
        "        # replacing @username had bad impace where USERNAME token shows up everywhere.\n",
        "#        line = re.sub(r\"@([A-Za-z0-9_]+)\", \"USERNAME\", line)\n",
        "        line = re.sub(r\"@([A-Za-z0-9_]+)\", \"\", line)\n",
        "        # Remove URL\n",
        "        line = re.sub(r'https?:\\/\\/.*', \"\", line)\n",
        "        line = line.lstrip()\n",
        "        return line\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_source_target_swapped(source_path):\n",
        "        basename, extension = os.path.splitext(source_path)\n",
        "        dest_path = \"{}_swapped{}\".format(basename, extension)\n",
        "        with gfile.GFile(source_path, mode=\"rb\") as fin, gfile.GFile(dest_path,\n",
        "                                                                     mode=\"w+\") as fout:\n",
        "            temp = None\n",
        "            for i, line in enumerate(fin):\n",
        "                if i % 2 == 0:\n",
        "                    temp = line\n",
        "                else:\n",
        "                    fout.write(line)\n",
        "                    fout.write(temp)\n",
        "                    temp = None\n",
        "        return dest_path\n",
        "\n",
        "    def _build_vocab_dic(self, source_path, vocab_dic={}):\n",
        "        with gfile.GFile(source_path, mode=\"r\") as f:\n",
        "            for line in f:\n",
        "                words = self.tagger.parse(line).split()\n",
        "                for word in words:\n",
        "                    if word in vocab_dic:\n",
        "                        vocab_dic[word] += 1\n",
        "                    else:\n",
        "                        vocab_dic[word] = 1\n",
        "            return vocab_dic\n",
        "\n",
        "    @staticmethod\n",
        "    def _read_file(source_path):\n",
        "        f = open(source_path)\n",
        "        data = f.read()\n",
        "        f.close()\n",
        "        return data\n",
        "\n",
        "    def _read_vocab(self, source_path):\n",
        "        rev_vocab = []\n",
        "        rev_vocab.extend(self._read_file(source_path).splitlines())\n",
        "        rev_vocab = [line.strip() for line in rev_vocab]\n",
        "        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
        "        return vocab, rev_vocab\n",
        "\n",
        "    def text_line_split_dataset(self, filename):\n",
        "        return tf.data.TextLineDataset(filename).map(self.split_to_int_values)\n",
        "\n",
        "    @staticmethod\n",
        "    def split_to_int_values(x):\n",
        "        return tf.string_to_number(tf.string_split([x]).values, tf.int32)\n",
        "\n",
        "    def _create_dataset(self):\n",
        "\n",
        "        tweets_dataset = self.text_line_split_dataset(self.enc_idx_padded_path)\n",
        "        tweets_lengths_dataset = tf.data.TextLineDataset(\n",
        "            self.enc_idx_len_path)\n",
        "\n",
        "        replies_sos_dataset = self.text_line_split_dataset(\n",
        "            self.dec_idx_sos_path)\n",
        "        replies_eos_dataset = self.text_line_split_dataset(\n",
        "            self.dec_idx_eos_path)\n",
        "        replies_sos_lengths_dataset = tf.data.TextLineDataset(\n",
        "            self.dec_idx_len_path)\n",
        "\n",
        "        tweets_transposed = tweets_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(\n",
        "                self.hparams.batch_size)).map(\n",
        "            lambda x: tf.transpose(x))\n",
        "        tweets_lengths = tweets_lengths_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(self.hparams.batch_size))\n",
        "\n",
        "        replies_with_eos_suffix = replies_eos_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(self.hparams.batch_size))\n",
        "        replies_with_sos_prefix = replies_sos_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(\n",
        "                self.hparams.batch_size)).map(\n",
        "            lambda x: tf.transpose(x))\n",
        "        replies_with_sos_suffix_lengths = replies_sos_lengths_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(\n",
        "                self.hparams.batch_size))\n",
        "        vocab, rev_vocab = self._read_vocab(self.vocab_path)\n",
        "        return tf.data.Dataset.zip((tweets_transposed, tweets_lengths,\n",
        "                                    replies_with_eos_suffix,\n",
        "                                    replies_with_sos_prefix,\n",
        "                                    replies_with_sos_suffix_lengths)), vocab, rev_vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O5MLcyf9OVQZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TrainDataSource:\n",
        "  def __init__(self, source_path, hparams):\n",
        "    download_file_if_necessary(source_path)\n",
        "    generator = TrainDataGenerator(source_path=source_path,\n",
        "                                   hparams=hparams)\n",
        "    train_dataset, vocab, rev_vocab = generator.generate()\n",
        "    self.train_dataset = train_dataset.repeat().shuffle(1024)\n",
        "    # todo(higepon): Use actual validation dataset.\n",
        "    self.valid_dataset = train_dataset.repeat().shuffle(1024,\n",
        "                                                        seed=1234)\n",
        "    self.vocab = vocab\n",
        "    self.rev_vocab = rev_vocab\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "  def __init__(self):\n",
        "    self.loss_step = []\n",
        "    self.val_losses = []\n",
        "    self.reward_step = []\n",
        "    self.reward_average = []\n",
        "    self.last_saved_time = datetime.datetime.now()\n",
        "    self.last_stats_time = datetime.datetime.now()\n",
        "    self.num_stats_per = 20\n",
        "    self.reward_history = []\n",
        "\n",
        "  def train_rl_tweets(self, src_hparams, dst_hparams, source_path, tweets,\n",
        "                      dull_responses):\n",
        "    print(\"===== Train RL {} ====\".format(source_path))\n",
        "    print(\"src:\")\n",
        "    print_hparams(src_hparams)\n",
        "    print(\"dst\")\n",
        "    print_hparams(dst_hparams)\n",
        "    data_source = TrainDataSource(source_path, dst_hparams)\n",
        "\n",
        "    self._rl_train_loop(data_source, src_hparams, dst_hparams, tweets,\n",
        "                        dull_responses)\n",
        "\n",
        "  def train_seq2seq_swapped(self, hparams, tweets_path, validation_tweets,\n",
        "                            should_clean_saved_model=True):\n",
        "    download_file_if_necessary(tweets_path)\n",
        "    swapped_path = TrainDataGenerator.generate_source_target_swapped(\n",
        "      tweets_path)\n",
        "    self.train_seq2seq(hparams, swapped_path, validation_tweets,\n",
        "                       should_clean_saved_model)\n",
        "\n",
        "  def train_seq2seq(self, hparams, tweets_path, val_tweets,\n",
        "                    should_clean_saved_model=True):\n",
        "    print(\"===== Train Seq2Seq {} ====\".format(tweets_path))\n",
        "    print_hparams(hparams)\n",
        "    \n",
        "    if should_clean_saved_model:\n",
        "      clean_model_path(hparams.model_path)\n",
        "    data_source = TrainDataSource(tweets_path, hparams)\n",
        "    self._train_loop(data_source, hparams, val_tweets)\n",
        "\n",
        "  def _print_inferences(self, global_step, tweets, helper, beam_helper=None):\n",
        "    print(\"==== {} ====\".format(global_step))\n",
        "    len_array = []\n",
        "    for tweet in tweets:\n",
        "      len_array.append(len(helper.inferences(tweet)[0]))\n",
        "      helper.print_inferences(tweet)\n",
        "      if beam_helper is not None:\n",
        "        beam_helper.print_inferences(tweet)\n",
        "    self._print_log('avg_reply_len', np.mean(len_array))\n",
        "\n",
        "  # should this be public/private?\n",
        "  @staticmethod\n",
        "  def create_models(hparams):\n",
        "\n",
        "    # See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
        "    config = tf.ConfigProto(log_device_placement=True)\n",
        "    config.gpu_options.allow_growth = True\n",
        "\n",
        "    train_graph = tf.Graph()\n",
        "    train_sess = tf.Session(graph=train_graph, config=config)\n",
        "    with train_graph.as_default():\n",
        "      with tf.variable_scope('root'):\n",
        "        model = ChatbotModel(train_sess, hparams,\n",
        "                             model_path=hparams.model_path)\n",
        "        if not model.restore():\n",
        "          train_sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # note that infer_model is not sharing variable with training model.\n",
        "    infer_graph = tf.Graph()\n",
        "    infer_sess = tf.Session(graph=infer_graph, config=config)\n",
        "    with infer_graph.as_default():\n",
        "      with tf.variable_scope('root'):\n",
        "        infer_model = ChatbotInferenceModel(infer_sess, hparams,\n",
        "                                            model_path=hparams.model_path)\n",
        "\n",
        "    return model, infer_model\n",
        "\n",
        "  # should thi be public/private\n",
        "  def create_rl_models(self, dst_hparams, src_hparams):\n",
        "    model, infer_model = self.create_models(\n",
        "      dst_hparams)\n",
        "    rl_beam_hparams = copy.deepcopy(dst_hparams).override_from_dict(\n",
        "      {'beam_width': 2})\n",
        "    infer_sess = infer_model.sess\n",
        "    with infer_sess.graph.as_default():\n",
        "      with tf.variable_scope('root', reuse=True):\n",
        "        infer_beam_model = ChatbotInferenceModel(infer_sess,\n",
        "                                                 rl_beam_hparams,\n",
        "                                                 model_path=rl_beam_hparams.model_path)\n",
        "    restored = infer_model.restore()\n",
        "    if not restored:\n",
        "      with rl_infer_graph.as_default():\n",
        "        infer_sess.run(tf.global_variables_initializer())\n",
        "    seq2seq_graph = tf.Graph()\n",
        "    seq2seq_sess = tf.Session(graph=seq2seq_graph)\n",
        "    with seq2seq_graph.as_default():\n",
        "      with tf.variable_scope('root'):\n",
        "        seq2seq_infer_model = ChatbotInferenceModel(seq2seq_sess,\n",
        "                                                    src_hparams,\n",
        "                                                    model_path=src_hparams.model_path)\n",
        "        restored = seq2seq_infer_model.restore()\n",
        "        assert restored\n",
        "    return infer_beam_model, infer_model, model, seq2seq_infer_model\n",
        "\n",
        "  def _train_loop(self, data_source,\n",
        "                  hparams, tweets):\n",
        "    download_model_data_if_necessary(hparams.model_path)\n",
        "\n",
        "    device = self._available_device()\n",
        "    with tf.device(device):\n",
        "      model, infer_model = self.create_models(hparams)\n",
        "\n",
        "    def my_train(**kwargs):\n",
        "      data = kwargs['train_data']\n",
        "      return model.train(data[0], data[1], data[2], data[3], data[4])\n",
        "\n",
        "    self._generic_loop(data_source, hparams, infer_model, None,\n",
        "                       model,\n",
        "                       tweets, my_train)\n",
        "\n",
        "  def _rl_train_loop(self, data_source, src_hparams, dst_hparams, tweets,\n",
        "                     dull_responses):\n",
        "    download_model_data_if_necessary(src_hparams.model_path)\n",
        "    download_model_data_if_necessary(dst_hparams.model_path)\n",
        "\n",
        "    device = self._available_device()\n",
        "    with tf.device(device):\n",
        "      infer_beam_model, infer_model, model, seq2seq_infer_model = self.create_rl_models(\n",
        "        dst_hparams, src_hparams)\n",
        "\n",
        "    def my_train(**kwargs):\n",
        "      train_data = kwargs['train_data']\n",
        "      dull_responses_ids = kwargs['dull_responses_ids']\n",
        "      return model.train_with_reward(\n",
        "        infer_model,\n",
        "        seq2seq_infer_model,\n",
        "        train_data[0],\n",
        "        train_data[1],\n",
        "        train_data[2],\n",
        "        train_data[3],\n",
        "        train_data[4],\n",
        "        dull_responses_ids)\n",
        "\n",
        "    self._generic_loop(data_source, dst_hparams, infer_model,\n",
        "                       infer_beam_model, model,\n",
        "                       tweets, my_train,\n",
        "                       dull_responses)\n",
        "\n",
        "  @staticmethod\n",
        "  def _available_device():\n",
        "    device = '/cpu:0'\n",
        "    if has_gpu0():\n",
        "      device = '/gpu:0'\n",
        "      print(\"!!!GPU ENABLED !!!\")\n",
        "    return device\n",
        "\n",
        "  @staticmethod\n",
        "  def tokenize(infer_helper, text):\n",
        "    tagger = MeCab.Tagger(\"-Owakati\")\n",
        "    words = tagger.parse(text).split()\n",
        "    return infer_helper.words_to_ids(words)\n",
        "\n",
        "  def _generic_loop(self, data_source, hparams, infer_model,\n",
        "                    infer_beam_model, model,\n",
        "                    tweets, train_func,\n",
        "                    dull_responses=None):\n",
        "    vocab = data_source.vocab\n",
        "    rev_vocab = data_source.rev_vocab\n",
        "    infer_helper = InferenceHelper(infer_model, vocab, rev_vocab)\n",
        "    beam_helper = None if infer_beam_model is None else InferenceHelper(\n",
        "      infer_beam_model, vocab, rev_vocab)\n",
        "\n",
        "    if dull_responses is None:\n",
        "      dull_responses_ids = None\n",
        "    else:\n",
        "      dull_responses_ids = [self.tokenize(infer_helper, text)\n",
        "                            for text in\n",
        "                            dull_responses]\n",
        "    graph = model.sess.graph\n",
        "    with graph.as_default():\n",
        "      train_data_next = data_source.train_dataset.make_one_shot_iterator().get_next()\n",
        "      val_data_next = data_source.valid_dataset.make_one_shot_iterator().get_next()\n",
        "      easy_tf_log.set_dir(hparams.model_path)\n",
        "      writer = tf.summary.FileWriter(hparams.model_path, graph)\n",
        "      self.last_saved_time = datetime.datetime.now()\n",
        "      for i in range(hparams.num_train_steps):\n",
        "        train_data = model.sess.run(train_data_next)\n",
        "\n",
        "        step, learning_rate, summary, reward_summary, reward = train_func(\n",
        "          train_data=train_data, dull_responses_ids=dull_responses_ids)\n",
        "        writer.add_summary(summary, step)\n",
        "        writer.add_summary(reward_summary, step)\n",
        "        self._print_reward(step, reward)\n",
        "\n",
        "        if i != 0 and i % self.num_stats_per == 0:\n",
        "          model.save(hparams.model_path)\n",
        "          is_restored = infer_model.restore()\n",
        "          assert is_restored\n",
        "          self._print_inferences(step, tweets, infer_helper, beam_helper)\n",
        "          self._compute_val_loss(step, model, val_data_next, writer)\n",
        "          self._print_stats(hparams, learning_rate)\n",
        "          self._plot_if_necessary()\n",
        "          self._save_model_in_drive(hparams)\n",
        "        else:\n",
        "          print('.', end='')\n",
        "\n",
        "  def _plot_if_necessary(self):\n",
        "    if len(self.reward_average) > 0 and len(self.reward_average) % 5 == 0:\n",
        "      self._plot(self.reward_step, self.reward_average,\n",
        "                 y_label='reward average')\n",
        "      self._plot(self.loss_step, self.val_losses, y_label='validation_loss')\n",
        "\n",
        "  def _print_stats(self, hparams, learning_rate):\n",
        "    print(\"learning rate\", learning_rate)\n",
        "    delta = (\n",
        "                datetime.datetime.now() - self.last_stats_time).total_seconds() * 1000\n",
        "    self._print_log(\"msec/data\",\n",
        "                    delta / hparams.batch_size / self.num_stats_per)\n",
        "    self.last_stats_time = datetime.datetime.now()\n",
        "\n",
        "  def _save_model_in_drive(self, hparams):\n",
        "    now = datetime.datetime.now()\n",
        "    delta_in_min = (now - self.last_saved_time).total_seconds() / 60\n",
        "\n",
        "    if delta_in_min >= 45 :\n",
        "      self.last_saved_time = datetime.datetime.now()\n",
        "      save_model_in_drive(hparams.model_path)\n",
        "\n",
        "  @staticmethod\n",
        "  def _print_log(key, value):\n",
        "    tflog(key, value)\n",
        "    print(\"{}={}\".format(key, round(value, 1)))\n",
        "\n",
        "  def _print_reward(self, global_step, reward):\n",
        "    tflog(\"reward\", reward)\n",
        "    self.reward_history.append(reward)\n",
        "    if len(self.reward_history) == 20:\n",
        "      reward_average = np.mean(self.reward_history)\n",
        "      self._print_log(\"reward average\", reward_average)\n",
        "      self.reward_step.append(global_step)\n",
        "      self.reward_average.append(reward_average)\n",
        "      self.reward_history = []\n",
        "\n",
        "  @staticmethod\n",
        "  def _plot(x, y, x_label=\"step\", y_label='y'):\n",
        "    title = \"{}_{}\".format(current_client_id, y_label)\n",
        "    if colab():\n",
        "      plt.plot(x, y, label=title)\n",
        "      plt.plot()\n",
        "      plt.ylabel(y_label)\n",
        "      plt.xlabel(x_label)\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "  def _compute_val_loss(self, global_step, model, val_data_next,\n",
        "                        writer):\n",
        "    val_data = model.sess.run(val_data_next)\n",
        "    val_loss, val_loss_log = model.batch_loss(val_data[0],\n",
        "                                              val_data[1],\n",
        "                                              val_data[2],\n",
        "                                              val_data[3],\n",
        "                                              val_data[4])\n",
        "    # np.float64 to native float\n",
        "    val_loss = val_loss.item()\n",
        "    writer.add_summary(val_loss_log, global_step)\n",
        "    self._print_log(\"validation loss\", val_loss)\n",
        "    self.loss_step.append(global_step)\n",
        "    self.val_losses.append(val_loss)\n",
        "    return val_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DQg8kU-2Dr-q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Helper functions to test\n",
        "def make_test_training_data(hparams):\n",
        "    train_encoder_inputs = np.empty(\n",
        "        (hparams.encoder_length, hparams.batch_size), dtype=np.int)\n",
        "    train_encoder_inputs_lengths = np.empty(hparams.batch_size, dtype=np.int)\n",
        "    training_target_labels = np.empty(\n",
        "        (hparams.batch_size, hparams.decoder_length), dtype=np.int)\n",
        "    training_decoder_inputs = np.empty(\n",
        "        (hparams.decoder_length, hparams.batch_size), dtype=np.int)\n",
        "\n",
        "    # We keep first tweet to validate inference.\n",
        "    first_tweet = None\n",
        "\n",
        "    for i in range(hparams.batch_size):\n",
        "        # Tweet\n",
        "        tweet = np.random.randint(low=0, high=hparams.vocab_size,\n",
        "                                  size=hparams.encoder_length)\n",
        "        train_encoder_inputs[:, i] = tweet\n",
        "        train_encoder_inputs_lengths[i] = len(tweet)\n",
        "        # Reply\n",
        "        #   Note that low = 2, as 0 and 1 are reserved.\n",
        "        reply = np.random.randint(low=2, high=hparams.vocab_size,\n",
        "                                  size=hparams.decoder_length - 1)\n",
        "\n",
        "        training_target_label = np.concatenate((reply, np.array([hparams.eos_id])))\n",
        "        training_target_labels[i] = training_target_label\n",
        "\n",
        "        training_decoder_input = np.concatenate(([hparams.sos_id], reply))\n",
        "        training_decoder_inputs[:, i] = training_decoder_input\n",
        "\n",
        "        if i == 0:\n",
        "            first_tweet = tweet\n",
        "            info(\"0th tweet={}\".format(tweet), hparams)\n",
        "            info(\"0th reply_with_eos_suffix={}\".format(training_target_label),\n",
        "                 hparams)\n",
        "            info(\"0th reply_with_sos_prefix={}\".format(training_decoder_input),\n",
        "                 hparams)\n",
        "\n",
        "        info(\"Tweets\", hparams)\n",
        "        info(train_encoder_inputs, hparams)\n",
        "        info(\"Replies\", hparams)\n",
        "        info(training_target_labels, hparams)\n",
        "        info(training_decoder_inputs, hparams)\n",
        "    return first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs\n",
        "\n",
        "\n",
        "def test_training(test_hparams, model, infer_model):\n",
        "    if test_hparams.use_attention:\n",
        "        print(\"==== training model[attention] ====\")\n",
        "    else:\n",
        "        print(\"==== training model ====\")\n",
        "    first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs = make_test_training_data(\n",
        "        test_hparams)\n",
        "    for i in range(test_hparams.num_train_steps):\n",
        "        _ = model.train(train_encoder_inputs,\n",
        "                        train_encoder_inputs_lengths,\n",
        "                        training_target_labels,\n",
        "                        training_decoder_inputs,\n",
        "                        np.ones(test_hparams.batch_size,\n",
        "                                dtype=int) * test_hparams.decoder_length)\n",
        "        if i % 5 == 0 and test_hparams.debug_verbose:\n",
        "            print('.', end='')\n",
        "\n",
        "        if i % 15 == 0:\n",
        "            model.save()\n",
        "\n",
        "    inference_encoder_inputs = np.empty((test_hparams.encoder_length, 1),\n",
        "                                        dtype=np.int)\n",
        "    inference_encoder_inputs_lengths = np.empty(1, dtype=np.int)\n",
        "    for i in range(1):\n",
        "        inference_encoder_inputs[:, i] = first_tweet\n",
        "        inference_encoder_inputs_lengths[i] = len(first_tweet)\n",
        "\n",
        "    # testing \n",
        "    log_prob54 = infer_model.log_prob(inference_encoder_inputs,\n",
        "                                      inference_encoder_inputs_lengths,\n",
        "                                      np.array([5, 4]))\n",
        "    log_prob65 = infer_model.log_prob(inference_encoder_inputs,\n",
        "                                      inference_encoder_inputs_lengths,\n",
        "                                      np.array([6, 5]))\n",
        "    print(\"log_prob for 54\", log_prob54)\n",
        "    print(\"log_prob for 65\", log_prob65)\n",
        "\n",
        "    reward = infer_model.reward_ease_of_answering(inference_encoder_inputs,\n",
        "                                                  inference_encoder_inputs_lengths,\n",
        "                                                  np.array([[5], [6]]))\n",
        "    print(\"reward=\", reward)\n",
        "\n",
        "    if test_hparams.debug_verbose:\n",
        "        print(inference_encoder_inputs)\n",
        "    replies = infer_model.infer(inference_encoder_inputs,\n",
        "                                inference_encoder_inputs_lengths)\n",
        "    print(\"Infered replies\", replies[0])\n",
        "    print(\"Expected replies\", training_target_labels[0])\n",
        "\n",
        "# is this used?\n",
        "def create_train_infer_models(graph, sess, hparams, force_restore=False):\n",
        "    with graph.as_default():\n",
        "        with tf.variable_scope('root'):\n",
        "            model = ChatbotModel(sess, hparams, model_path=hparams.model_path)\n",
        "           \n",
        "        with tf.variable_scope('root', reuse=True):\n",
        "            infer_model = ChatbotInferenceModel(sess, hparams,\n",
        "                                                model_path=hparams.model_path)\n",
        "            restored = model.restore()\n",
        "            if not restored:\n",
        "                if force_restore:\n",
        "                    raise Exception(\"Oops, couldn't restore\")\n",
        "                else:\n",
        "                    sess.run(tf.global_variables_initializer())\n",
        "        return model, infer_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test_multiple_models_training():\n",
        "    first_tweet, train_encoder_inputs, train_encoder_inputs_length, training_target_labels, training_decoder_inputs = make_test_training_data(\n",
        "        test_hparams)\n",
        "    \n",
        "    hparams1 = copy.deepcopy(test_hparams).override_from_dict({'model_path': p(ModelDirectory.test_multiple1.value)})\n",
        "    hparams2 = copy.deepcopy(test_hparams).override_from_dict({'model_path': p(ModelDirectory.test_multiple2.value)})\n",
        "    \n",
        "\n",
        "    graph1 = tf.Graph()\n",
        "    sess1 = tf.Session(graph=graph1)\n",
        "    model, infer_model = create_train_infer_models(graph1, sess1, hparams1)\n",
        "    test_training(test_hparams, model, infer_model)\n",
        "\n",
        "    graph2 = tf.Graph()\n",
        "    sess2 = tf.Session(graph=graph2)\n",
        "    model2, infer_model2 = create_train_infer_models(graph2, sess2, hparams2)\n",
        "\n",
        "    test_training(test_hparams, model2, infer_model2)\n",
        "    dull_responses = [[4, 6, 6], [5, 5]]\n",
        "    model2.train_with_reward(infer_model2, infer_model, train_encoder_inputs,\n",
        "                             train_encoder_inputs_length,\n",
        "                             training_target_labels, training_decoder_inputs,\n",
        "                             np.ones((test_hparams.batch_size),\n",
        "                                     dtype=int) * test_hparams.decoder_length,\n",
        "                             dull_responses)\n",
        "\n",
        "    # comment out until https://github.com/tensorflow/tensorflow/issues/10731 is fixed\n",
        "    graph3 = tf.Graph()\n",
        "    sess3 = tf.Session(graph=graph3)\n",
        "#    hparams3 = copy.deepcopy(test_hparams).override_from_dict({'model_path': p(ModelDirectory.test_multiple3.value), 'use_attention': True})\n",
        "#    model3, infer_model3 = create_train_infer_models(graph3, sess3, hparams3)    \n",
        "#    test_training(test_attention_hparams, model3, infer_model3)        \n",
        "\n",
        "\n",
        "def test_save_restore_multiple_models_training():\n",
        "  \n",
        "    for d in [ModelDirectory.test_multiple1, ModelDirectory.test_multiple2, ModelDirectory.test_multiple3]:\n",
        "      shutil.rmtree(p(d.value))\n",
        "      os.makedirs(p(d.value), exist_ok=True)\n",
        "\n",
        "    # Fresh model\n",
        "    test_multiple_models_training()\n",
        "\n",
        "    # Saved model\n",
        "    test_multiple_models_training()\n",
        "\n",
        "\n",
        "def test_distributed_pattern(hparams):\n",
        "  \n",
        "    for d in [hparams.model_path]:\n",
        "      shutil.rmtree(p(d), ignore_errors=True)\n",
        "      os.makedirs(p(d), exist_ok=True)\n",
        "\n",
        "    print('==== test_distributed_pattern[{} {}] ===='.format('attention' if hparams.use_attention else '', 'beam' if hparams.beam_width > 0 else ''))\n",
        "        \n",
        "    first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs = make_test_training_data(\n",
        "        hparams)\n",
        "\n",
        "    train_graph = tf.Graph()\n",
        "    infer_graph = tf.Graph()\n",
        "    train_sess = tf.Session(graph=train_graph)\n",
        "    infer_sess = tf.Session(graph=infer_graph)\n",
        "    \n",
        "\n",
        "    model, infer_model = Trainer().create_models(hparams)\n",
        "\n",
        "    for i in range(hparams.num_train_steps):\n",
        "        _ = model.train(train_encoder_inputs,\n",
        "                        train_encoder_inputs_lengths,\n",
        "                        training_target_labels,\n",
        "                        training_decoder_inputs,\n",
        "                        np.ones(hparams.batch_size,\n",
        "                                dtype=int) * hparams.decoder_length)\n",
        "\n",
        "    model.save()\n",
        "\n",
        "    inference_encoder_inputs = np.empty((hparams.encoder_length, 1),\n",
        "                                        dtype=np.int)\n",
        "    inference_encoder_inputs_lengths = np.empty(1, dtype=np.int)\n",
        "\n",
        "    inference_encoder_inputs[:, 0] = first_tweet\n",
        "    inference_encoder_inputs_lengths[0] = len(first_tweet)\n",
        "\n",
        "    infer_model.restore()\n",
        "    if hparams.beam_width == 0:\n",
        "      replies = infer_model.infer(inference_encoder_inputs,\n",
        "                                  inference_encoder_inputs_lengths)\n",
        "      print(\"Inferred replies\", replies[0])\n",
        "\n",
        "    if hparams.beam_width > 0:\n",
        "      beam_replies = infer_model.infer_beam_search(inference_encoder_inputs,\n",
        "                                                   inference_encoder_inputs_lengths)\n",
        "      print(\"Inferred replies candidate0\", beam_replies[0][:, 0])\n",
        "      print(\"Inferred replies candidate1\", beam_replies[0][:, 1])\n",
        "    print(\"Expected replies\", training_target_labels[0])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W_ciBCflZq5o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if mode == Mode.Test:\n",
        "  test_save_restore_multiple_models_training()\n",
        "\n",
        "def test_distributed_one(enable_beam, enable_attention):\n",
        "  hparams = copy.deepcopy(test_hparams).override_from_dict({\n",
        "      'model_path': p(ModelDirectory.test_distributed.value),\n",
        "      'use_attention': enable_attention,\n",
        "      'beam_width': 2 if enable_beam else 0\n",
        "  })\n",
        "  test_distributed_pattern(hparams)\n",
        "  \n",
        "if mode == Mode.Test:\n",
        "  test_distributed_one(enable_beam=False, enable_attention=False)\n",
        "  test_distributed_one(enable_beam=False, enable_attention=True)\n",
        "  test_distributed_one(enable_beam=True, enable_attention=False)\n",
        "  test_distributed_one(enable_beam=False, enable_attention=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kxeWpXO5FThm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def download_file_if_necessary(file_name):\n",
        "    if os.path.exists(file_name):\n",
        "        return\n",
        "    print(\"downloading {}...\".format(file_name))\n",
        "    !cp $drive_path/$file_name $file_name\n",
        "    print(\"downloaded\")\n",
        "  \n",
        "def absoluteFilePaths(directory):\n",
        "   for dirpath,_,filenames in os.walk(directory):\n",
        "       for f in filenames:\n",
        "           yield os.path.abspath(os.path.join(dirpath, f))\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W3nUhj80H6BE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def download_model_data_if_necessary(model_path):\n",
        "    if not os.path.exists(model_path):\n",
        "        os.makedirs(model_path)\n",
        "    print(\"Downloading model files...\")\n",
        "    !cp $drive_path/$model_path/* $model_path/\n",
        "    print(\"done\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I0-o2wg0gzvA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "def list_model_file(path):\n",
        "    f = open('{}/checkpoint'.format(path))\n",
        "    text = f.read()\n",
        "    f.close()\n",
        "    print(text)\n",
        "    m = re.match(r\".*ChatbotModel\\-(\\d+)\", text)\n",
        "    model_name = m.group(1)\n",
        "    all = [\"checkpoint\"]\n",
        "    all.extend([x for x in os.listdir(path) if re.search(model_name, x) or re.search('events.out', x)])\n",
        "    print(\"all=\", all)\n",
        "    return all\n",
        "  \n",
        "def save_model_in_drive(model_path):\n",
        "    !mkdir -p $drive_path/$model_path\n",
        "    !rm -f $drive_path/$model_path/*\n",
        "    print(\"Saving model in Google Drive...\")\n",
        "    for file in list_model_file(model_path):\n",
        "      print(\"Saving \", file)\n",
        "      shutil.copy2(\"{}/{}\".format(model_path, file), \"{}/{}/{}\".format(drive_path, model_path, file))\n",
        "    print(\"done\")\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PBo5SLAk3vza",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!apt-get -qq install -y mecab libmecab-dev mecab-ipadic mecab-ipadic-utf8\n",
        "!pip install mecab-python3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JrvS_DURF5Pq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "\n",
        "\n",
        "def clean_model_path(model_path):\n",
        "    shutil.rmtree(model_path)\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "\n",
        "def print_header(text):\n",
        "    print(\"============== {} ==============\".format(text))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test_tweets_small_swapped(hparams):\n",
        "  replies = [\"@higepon おはようございます！\", \"おつかれさまー。気をつけて。\", \"こちらこそよろしくお願いします。\"]\n",
        "  trainer = Trainer()\n",
        "  trainer.train_seq2seq_swapped(hparams, \"tweets_small.txt\", replies)\n",
        "\n",
        "# vocab size 変えたら動かなくなった\n",
        "tweet_small_hparams = copy.deepcopy(base_hparams).override_from_dict(\n",
        "    {\n",
        "        'batch_size': 6,  # of tweets should be dividable by batch_size\n",
        "        'encoder_length': 8,\n",
        "        'decoder_length': 8,\n",
        "        'num_units': 256,\n",
        "        'num_layers': 2,\n",
        "        'vocab_size': 34,\n",
        "        'embedding_size': 40,\n",
        "        'beam_width': 2,  # for faster iteration, this should be 10\n",
        "        'num_train_steps': 200,\n",
        "        'model_path': p(ModelDirectory.tweet_small.value),\n",
        "        'learning_rate': 0.05,\n",
        "        'use_attention': True,\n",
        "    })\n",
        "\n",
        "tweet_small_swapped_hparams = copy.deepcopy(\n",
        "    tweet_small_hparams).override_from_dict(\n",
        "    {'model_path': p(ModelDirectory.tweet_small_swapped.value)})\n",
        "\n",
        "if mode == Mode.Test:\n",
        "    tweets_path = \"tweets_small.txt\"\n",
        "    TrainDataGenerator(tweets_path, tweet_small_hparams).remove_generated()\n",
        "    trainer = Trainer()\n",
        "    trainer.train_seq2seq(tweet_small_hparams, tweets_path,\n",
        "                         [\"おはようございます。寒いですね。\", \"さて帰ろう。明日は早い。\", \"今回もよろしくです。\"])\n",
        "    test_tweets_small_swapped(tweet_small_swapped_hparams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tzh2rhEPguJ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_tweets_large(hparams):\n",
        "  tweets = [\"さて福岡行ってきます！\", \"誰か飲みに行こう\", \"熱でてるけど、でもなんか食べなきゃーと思ってアイス買おうとしたの\",\"今日のドラマ面白そう！\",\"お腹すいたー\", \"おやすみ～\", \"おはようございます。寒いですね。\", \"さて帰ろう。明日は早い。\", \"今回もよろしくです。\", \"ばいとおわ！\"]\n",
        "  trainer = Trainer()\n",
        "  trainer.train_seq2seq(hparams, \"tweets_conversation.txt\", tweets, should_clean_saved_model=False)\n",
        "\n",
        "\n",
        "def test_tweets_large_swapped(hparams):\n",
        "  tweets = [ \"今日のドラマ面白そう！\",\"お腹すいたー\", \"おやすみ～\", \"おはようございます。寒いですね。\", \"さて帰ろう。明日は早い。\", \"今回もよろしくです。\", \"ばいとおわ！\"]\n",
        "  trainer = Trainer()\n",
        "  trainer.train_seq2seq_swapped(hparams, \"tweets_large.txt\", tweets, should_clean_saved_model=False)\n",
        "\n",
        "\n",
        "tweet_large_hparams = copy.deepcopy(base_hparams).override_from_dict(\n",
        "    {\n",
        "        # In typical seq2seq chatbot\n",
        "        # num_layers=3, learning_rate=0.5, batch_size=64, vocab=20000-100000, learning_rate decay is 0.99, which is taken care as default parameter in AdamOptimizer.\n",
        "        'batch_size': 64,  # of tweets should be dividable by batch_size\n",
        "        'encoder_length': 30,\n",
        "        'decoder_length': 30,\n",
        "        'num_units': 1024,\n",
        "        'num_layers': 3,\n",
        "        'vocab_size': 100000,\n",
        "        'embedding_size': 1024,\n",
        "        'beam_width': 2,  # for faster iteration, this should be 10\n",
        "        'num_train_steps': 1000000,\n",
        "        'model_path': p(ModelDirectory.tweet_large.value),\n",
        "        'learning_rate': 0.5, # For vocab_size 50000, num_layers 3, num_units 1024, tweet_large, starting learning_rate 0.05 works well, change it t0 0.01 at perplexity 800, changed it to 0.005 at 200.\n",
        "        'learning_rate_decay': 0.99,\n",
        "        'use_attention': True,\n",
        "        # testing new restore learning rate and no USERNAME TOKEN\n",
        "    })\n",
        "\n",
        "tweet_large_swapped_hparams = copy.deepcopy(\n",
        "    tweet_large_hparams).override_from_dict(\n",
        "    {\n",
        "        'model_path': p(ModelDirectory.tweet_large_swapped.value)\n",
        "    })\n",
        "\n",
        "if mode == Mode.TrainSeq2Seq:\n",
        "    print(\"train seq2seq\")\n",
        "    print(tweet_large_hparams)\n",
        "    test_tweets_large(tweet_large_hparams)\n",
        "elif mode == Mode.TrainSeq2SeqSwapped:\n",
        "    print(\"train seq2seq swapped\")\n",
        "    test_tweets_large_swapped(tweet_large_swapped_hparams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cAItqdYv8RA0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rl_src_hparams = copy.deepcopy(tweet_large_hparams).override_from_dict({'beam_width': 0})\n",
        "rl_dst_hparams = copy.deepcopy(rl_src_hparams).override_from_dict({\n",
        "    'learning_rate': 0.1,\n",
        "    'beam_width': 0,\n",
        "    'num_train_steps': 300000,\n",
        "    'model_path': p(ModelDirectory.tweet_large_rl.value),\n",
        "})\n",
        "\n",
        "tweets = [\"さて福岡行ってきます！\", \"誰か飲みに行こう\", \"熱でてるけど、でもなんか食べなきゃーと思ってアイス買おうとしたの\",\n",
        "          \"今日のドラマ面白そう！\", \"お腹すいたー\", \"おやすみ～\", \"おはようございます。寒いですね。\",\n",
        "          \"さて帰ろう。明日は早い。\", \"今回もよろしくです。\", \"ばいとおわ！\"]\n",
        "\n",
        "if mode == Mode.TrainRL:\n",
        "    train_large_rl(rl_src_hparams,\n",
        "                   rl_dst_hparams,\n",
        "                   source_path=\"tweets_large.txt\",\n",
        "                   tweets=tweets)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4vViISnYEzCr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def download_logs(path):\n",
        "  for f in absoluteFilePaths(path):\n",
        "    if re.match('.*events', f):\n",
        "      files.download(f)\n",
        "      \n",
        "def remove_saved_model(hparams):\n",
        "  !mkdir -p $hparams.model_path\n",
        "  !rm $hparams.model_path/*  \n",
        "  \n",
        "def copy_saved_model(src_hparams, dst_hparams):\n",
        "  !cp $src_hparams.model_path/* $dst_hparams.model_path/\n",
        "  # rm tf.logs from source so that it wouldn't be mixed in dest tf.logs.\n",
        "  !rm $dst_hparams.model_path/events*\n",
        "  \n",
        "  \n",
        "# This is designed to have small clean RL iterations many times.\n",
        "def easy_train_rl_tweets(seq2seq_hparams, rl_src_hparams, rl_dst_hparams, tweets_path, validation_tweets, dull_responses):\n",
        "  \n",
        "  TrainDataGenerator(tweets_path, seq2seq_hparams).remove_generated()\n",
        "  \n",
        "  # train normal seq2seq.\n",
        "  remove_saved_model(seq2seq_hparams)\n",
        "  trainer = Trainer()\n",
        "  trainer.train_seq2seq(seq2seq_hparams, tweets_path, validation_tweets)\n",
        "  \n",
        "  # Copy trained seq2seq as initial state of rl model.\n",
        "  remove_saved_model(rl_dst_hparams)\n",
        "  copy_saved_model(seq2seq_hparams, rl_dst_hparams)\n",
        "\n",
        "  trainer = Trainer()\n",
        "  trainer.train_rl_tweets(rl_src_hparams,   rl_dst_hparams, tweets_path, validation_tweets, dull_responses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zpJEEtL68Uo_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tweet_small_hparams = copy.deepcopy(\n",
        "    tweet_small_hparams).override_from_dict(\n",
        "    {'num_train_steps': 120,\n",
        "      'decoder_length': 10,\n",
        "      'encoder_length': 10\n",
        "    })\n",
        "\n",
        "rl_src_hparams = copy.deepcopy(tweet_small_hparams).override_from_dict({'beam_width': 0})\n",
        "rl_dst_hparams = copy.deepcopy(rl_src_hparams).override_from_dict({\n",
        "    'learning_rate': 0.1,\n",
        "    'beam_width': 0,\n",
        "    'num_train_steps':1000,\n",
        "    'model_path': p(ModelDirectory.tweet_small_rl.value),\n",
        "})\n",
        "\n",
        "if mode == Mode.Test:  \n",
        "  dull_responses = ['これ', \"それ\", \"ありがとうございます\", \"おつかれ\", \"それなwww\",\n",
        "                            \"おはよ(˙-˙)\",\n",
        "                            \"おはよ！\", \"おはようございます！\", \"おつかれさまです\"]\n",
        "#  easy_train_rl_tweets(tweet_small_hparams,  rl_src_hparams, rl_dst_hparams, \"tweets_small.txt\", [ \"おやすみ～\", \"おはようございます。寒いですね。\", \"今回もよろしくです。\", \"ばいとおわ！\"], dull_responses)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yimhfKo9BXG5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if mode == Mode.Test:\n",
        "  download_logs(ModelDirectory.tweet_small_rl.value)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R9GmdB_gEhqz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_tweets = 10000\n",
        "batch_size =64\n",
        "steps = 2 *int(num_tweets/batch_size)\n",
        "tweet_large_hparams = copy.deepcopy(\n",
        "    tweet_large_hparams).override_from_dict(\n",
        "    {'num_train_steps': steps * 20,\n",
        "        'vocab_size': 5000,\n",
        "        'embedding_size': 256,\n",
        "        'num_units': 512,\n",
        "        'num_layers': 2,     \n",
        "       'batch_size': batch_size,\n",
        "    })\n",
        "\n",
        "rl_large_src_hparams = copy.deepcopy(tweet_large_hparams).override_from_dict({'beam_width': 0})\n",
        "rl_large_dst_hparams = copy.deepcopy(rl_large_src_hparams).override_from_dict({\n",
        "    'learning_rate': 0.1,\n",
        "    'beam_width': 0,\n",
        "    'num_train_steps': steps * 20,\n",
        "    'model_path': p(ModelDirectory.tweet_large_rl.value),\n",
        "})\n",
        "\n",
        "\n",
        "if mode == Mode.Test:  \n",
        "  dull_responses = ['これ', \"それ\", \"ありがとうございます\", \"おつかれ\", \"それなwww\",\n",
        "                            \"おはよ(˙-˙)\",\n",
        "                            \"おはよ！\", \"おはようございます！\", \"おつかれさまです\"]\n",
        "  easy_train_rl_tweets(tweet_large_hparams,  rl_large_src_hparams, rl_large_dst_hparams, \"tweets_medium.txt\", [ \"おやすみ～\", \"おはようございます。寒いですね。\", \"今回もよろしくです。\", \"ばいとおわ！\"], dull_responses)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EP9wMbVkGlCa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if mode == Mode.Test:\n",
        "  download_logs(rl_large_src_hparams.model_path)\n",
        "  download_logs(rl_large_dst_hparams.model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MaKLchVHfaHl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T_q-Ns9hiHMB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# N.B: This would fail if we try to download logs in the previous cell.\n",
        "# My guess is tflog is somehow locking the log file when running the cell.\n",
        "#download_logs()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3jxF0tWHoBL-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_mutual_information(hparams, swapped_hparams):\n",
        "    graph = tf.Graph()\n",
        "    swapped_graph = tf.Graph()\n",
        "    config = tf.ConfigProto(log_device_placement=True)\n",
        "    config.gpu_options.allow_growth = True\n",
        "\n",
        "    download_model_data_if_necessary(p(ModelDirectory.tweet.value))\n",
        "    download_model_data_if_necessary(p(ModelDirectory.tweet_swapped.value))\n",
        "\n",
        "    with graph.as_default():\n",
        "        infer_sess = tf.Session(graph=graph, config=config)\n",
        "        with tf.variable_scope('root'):\n",
        "            model = ChatbotInferenceModel(infer_sess, hparams,\n",
        "                                          model_path=p(ModelDirectory.tweet.value))\n",
        "            model.restore()\n",
        "\n",
        "    with swapped_graph.as_default():\n",
        "        swap_sess = tf.Session(graph=swapped_graph, config=config)\n",
        "        with tf.variable_scope('root'):\n",
        "            smodel = ChatbotInferenceModel(swap_sess, swapped_hparams,\n",
        "                                           model_path=p(\n",
        "                                               ModelDirectory.tweet_swapped.value))\n",
        "            smodel.restore()\n",
        "            helper = InferenceHelper(model, vocab, rev_vocab)\n",
        "            helper.print_inferences(\"疲れた\")\n",
        "\n",
        "            shelper = InferenceHelper(smodel, vocab, rev_vocab)\n",
        "            shelper.print_inferences(\"お疲れ様\")\n",
        "\n",
        "\n",
        "#large_beam_hparams = copy.deepcopy(large_hparams)\n",
        "#large_beam_hparams.beam_width = 20\n",
        "#test_mutual_information(large_beam_hparams, large_swapped_hparams)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FGFWGrQv_kZ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tweepy pyyaml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hxPwNy-70_9X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import yaml\n",
        "import random\n",
        "\n",
        "class StreamListener(tweepy.StreamListener):\n",
        "    def __init__(self, api, helper):\n",
        "        self.api = api\n",
        "        self.helper = helper\n",
        "\n",
        "    def on_status(self, status):\n",
        "        # done hanlde @reply only\n",
        "        # done print reply\n",
        "        # add model paramerer\n",
        "        # direct reply\n",
        "        # unk reply\n",
        "        # shuffle beam search\n",
        "        print(\"{0}: {1}\".format(status.text, status.author.screen_name))\n",
        "\n",
        "        screen_name = status.author.screen_name\n",
        "        # ignore my tweets\n",
        "        if screen_name == self.api.me().screen_name:\n",
        "          print(\"Ignored my tweet\")\n",
        "          return True\n",
        "        elif status.text.startswith(\"@{0}\".format(self.api.me().screen_name)):\n",
        "          \n",
        "          replies = self.helper.inferences(status.text)\n",
        "          reply = random.choice(replies)\n",
        "          reply = \"@\" + status.author.screen_name + \" \" + reply\n",
        "          print(reply)\n",
        "          self.api.update_status(status=reply,\n",
        "                                 in_reply_to_status_id=status.id)\n",
        "          \n",
        "          return True        \n",
        "\n",
        "    @staticmethod\n",
        "    def on_error(status_code):\n",
        "        print(status_code)\n",
        "        return True\n",
        "\n",
        "\n",
        "def listener(hparams):\n",
        "  download_model_data_if_necessary(hparams.model_path)\n",
        "\n",
        "  rl_train_graph = tf.Graph()\n",
        "  rl_infer_graph = tf.Graph()\n",
        "  rl_train_sess = tf.Session(graph=rl_train_graph)\n",
        "  rl_infer_sess = tf.Session(graph=rl_infer_graph)\n",
        "    \n",
        "  _, infer_model = create_train_infer_models_in_graphs(rl_train_graph,\n",
        "                                                       rl_train_sess,\n",
        "                                                       rl_infer_graph,\n",
        "                                                       rl_infer_sess,\n",
        "                                                       hparams)\n",
        "    \n",
        "  source_path = \"tweets_large.txt\"\n",
        "  download_file_if_necessary(source_path)\n",
        "  generator = TrainDataGenerator(source_path=source_path, hparams=hparams)\n",
        "  _, vocab, rev_vocab = generator.generate() \n",
        "  infer_model.restore()\n",
        "  helper = InferenceHelper(infer_model, vocab, rev_vocab)\n",
        "\n",
        "  config_path = 'config.yml'\n",
        "  download_file_if_necessary(config_path)\n",
        "  f = open(config_path, 'rt')\n",
        "  cfg = yaml.load(f)['twitter']\n",
        "\n",
        "  consumer_key = cfg['consumer_key']\n",
        "  consumer_secret = cfg['consumer_secret']\n",
        "  access_token = cfg['access_token']\n",
        "  access_token_secret = cfg['access_token_secret']\n",
        "\n",
        "  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "  auth.set_access_token(access_token, access_token_secret)\n",
        "  api = tweepy.API(auth)\n",
        "\n",
        "  while True:\n",
        "#    try:\n",
        "      stream = tweepy.Stream(auth=api.auth,\n",
        "                             listener=StreamListener(api, helper))\n",
        "      print(\"listener starting...\")\n",
        "      stream.userstream()\n",
        "#    except Exception as e:\n",
        " #     print(e.__doc__)\n",
        "\n",
        "           \n",
        "tweet_hparams = copy.deepcopy(rl_dst_hparams).override_from_dict({'beam_width': 50})        \n",
        "if mode == Mode.TweetBot:\n",
        "  listener(tweet_hparams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bdZ4kH6SRnhd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b9zaxi8yRn2e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}