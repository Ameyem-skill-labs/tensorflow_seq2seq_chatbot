{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "FnS-GXJOJOY2",
        "colab_type": "text"
      },
      "source": [
        "Tensorflow 1.4.0 is required.\n",
        "This is based on [NMT Tutorial](https://github.com/tensorflow/nmt)."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "caxRbRVkDdhp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from __future__ import print_function\n",
        "from tensorflow.python.layers import core as layers_core\n",
        "from tensorflow.python.platform import gfile\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "\n",
        "## Note for me. You've summarized Seq2Seq at http://d.hatena.ne.jp/higepon/20171210/1512887715."
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g-ZP7_08WtPv",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "tf.__version__\n",
        "!mkdir \"./saved_model2\"\n",
        "!mkdir \"./saved_model\"\n",
        "\n",
        "\n",
        "!ls -la ./saved_model2"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C16WwvYwGVBm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# TODO\n",
        "# Make 2 models possible\n",
        "# Make them as methods\n",
        "# Change first part to use small hparams for debug\n",
        "# We could pass sequence_length to tf.nn.dynamic_rnn for better performance.\n",
        "# Maybe extract hparams\n",
        "# モデル どうやって reload するか？\n",
        "# support beam infer\n",
        "# attention default\n",
        "# model_path による reload が動いて得ない。\n",
        "\n",
        "test_hparams = tf.contrib.training.HParams(\n",
        "    batch_size=3,\n",
        "    encoder_length=5,\n",
        "    decoder_length=7,\n",
        "    num_units=6,\n",
        "    src_vocab_size=9,\n",
        "    embedding_size=8,\n",
        "    tgt_vocab_size=11,\n",
        "    learning_rate = 0.01,\n",
        "    max_gradient_norm = 5.0,\n",
        "    beam_width =9,\n",
        "    use_attention = False,\n",
        "    num_train_steps = 150,\n",
        "    debug_verbose = False\n",
        ")\n",
        "\n",
        "real_hparams = tf.contrib.training.HParams(\n",
        "    batch_size=25, # of tweets should be devidable by batch_size\n",
        "    encoder_length=20,\n",
        "    decoder_length=20,\n",
        "    num_units=1024,\n",
        "    src_vocab_size=500,\n",
        "    embedding_size=256,\n",
        "    tgt_vocab_size=500,\n",
        "    learning_rate = 0.01,\n",
        "    max_gradient_norm = 5.0,\n",
        "    beam_width =9,\n",
        "    use_attention = False,\n",
        "    num_train_steps = 100,\n",
        "    debug_verbose = True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Model path\n",
        "model_path = \"./saved_model/twitter\"\n",
        "\n",
        "# Symbol for start decode process.\n",
        "tgt_sos_id = 0\n",
        "\n",
        "# Symbol for end of decode process.\n",
        "tgt_eos_id = 1\n",
        "\n",
        "pad_id = 2\n",
        "\n",
        "unk_id = 3"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DFEYKvBwL3Nm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# For debug purpose.\n",
        "tf.reset_default_graph()\n",
        "\n",
        "class ChatbotModel:\n",
        "  def __init__(self, sess, hparams, model_path, scope='ChatbotModel'):\n",
        "    self.sess = sess\n",
        "    # todo remove\n",
        "#    self.hparams = hparams\n",
        "    \n",
        "    # todo\n",
        "    self.model_path = model_path\n",
        "    self.name = scope\n",
        "\n",
        "    self.encoder_inputs, encoder_outputs, encoder_state = self._build_encoder(hparams, scope)\n",
        "    self.decoder_inputs, self.decoder_lengths, self.replies, self.beam_replies, logits = self._build_decoder(hparams, encoder_state, encoder_outputs, scope)\n",
        "    \n",
        "    self.target_labels, self.loss, self.global_step, self.train_op = self._build_optimizer(hparams, logits)\n",
        "\n",
        "\n",
        "    # Initialize saver after model created\n",
        "    self.saver = tf.train.Saver(tf.global_variables())\n",
        "\n",
        "    ckpt = tf.train.get_checkpoint_state(self.model_path)\n",
        "    if ckpt:\n",
        "      last_model = ckpt.model_checkpoint_path\n",
        "      self.saver.restore(self.sess, last_model)\n",
        "      print(\"loaded \" + last_model)\n",
        "    else:\n",
        "      self.sess.run(tf.global_variables_initializer())\n",
        "      print(\"created fresh model.\")\n",
        "      \n",
        "  def train(self, encoder_inputs, target_labels, decoder_inputs, decoder_lengths):\n",
        "    feed_dict = {\n",
        "        self.encoder_inputs: encoder_inputs,\n",
        "        self.target_labels: target_labels,\n",
        "        self.decoder_inputs: decoder_inputs,\n",
        "        self.decoder_lengths: decoder_lengths\n",
        "    }    \n",
        "    _, loss_value, global_step = self.sess.run([self.train_op, self.loss, self.global_step], feed_dict=feed_dict)\n",
        "    return loss_value, global_step\n",
        "\n",
        "  def infer(self, encoder_inputs):\n",
        "    inference_feed_dict = {\n",
        "        self.encoder_inputs: encoder_inputs,\n",
        "    }\n",
        "    replies = self.sess.run([self.replies], feed_dict=inference_feed_dict)\n",
        "    return replies\n",
        "  \n",
        "  def infer_beam_search(self, encoder_inputs):\n",
        "    inference_feed_dict = {\n",
        "        self.encoder_inputs: encoder_inputs,\n",
        "    }    \n",
        "    replies = self.sess.run([self.beam_replies], feed_dict=inference_feed_dict)\n",
        "    return replies\n",
        "  \n",
        "  ## todo model_path\n",
        "  def save(self):\n",
        "      self.saver.save(self.sess, \"{}/{}\".format(self.model_path, self.name), global_step=self.global_step)\n",
        "      \n",
        "  def _build_optimizer(self, hparams, logits):\n",
        "    # Target labels\n",
        "    #   As described in doc for sparse_softmax_cross_entropy_with_logits,\n",
        "    #   labels should be [batch_size, decoder_lengths] instead of [batch_size, decoder_lengths, tgt_vocab_size].\n",
        "    #   So labels should have indices instead of tgt_vocab_size classes.\n",
        "    target_labels = tf.placeholder(tf.int32, shape=(hparams.batch_size, hparams.decoder_length))\n",
        "\n",
        "    # Loss\n",
        "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        labels=target_labels, logits=logits)\n",
        "\n",
        "    loss = tf.reduce_sum(crossent / tf.to_float(hparams.batch_size))    \n",
        "      \n",
        "    # Train\n",
        "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "\n",
        "    # Calculate and clip gradients\n",
        "    params = tf.trainable_variables()\n",
        "    gradients = tf.gradients(loss, params)\n",
        "    clipped_gradients, _ = tf.clip_by_global_norm(\n",
        "        gradients, hparams.max_gradient_norm)\n",
        "\n",
        "    # Optimization\n",
        "    optimizer = tf.train.AdamOptimizer(hparams.learning_rate)\n",
        "    train_op = optimizer.apply_gradients(\n",
        "        zip(clipped_gradients, params), global_step=global_step)\n",
        "    return target_labels, loss, global_step, train_op\n",
        "  \n",
        "  def _build_encoder(self, hparams, scope):\n",
        "    # Encoder\n",
        "    #   encoder_inputs: [encoder_length, batch_size]\n",
        "    #   This is time major where encoder_length comes first instead of batch_size.\n",
        "    encoder_inputs = tf.placeholder(tf.int32, shape=(hparams.encoder_length, hparams.batch_size), name=\"encoder_inputs\")\n",
        "    \n",
        "    # Embedding\n",
        "    #   Matrix for embedding: [src_vocab_size, embedding_size]\n",
        "    with tf.variable_scope(scope):\n",
        "      embedding_encoder = tf.get_variable(\"embedding_encoder\", [hparams.src_vocab_size, hparams.embedding_size])\n",
        "\n",
        "    # Look up embedding:\n",
        "    #   encoder_inputs: [encoder_length, batch_size]\n",
        "    #   encoder_emb_inputs: [encoder_length, batch_size, embedding_size]\n",
        "    encoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder, encoder_inputs)\n",
        "\n",
        "    # LSTM cell.\n",
        "    with tf.variable_scope(scope):\n",
        "      encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
        "\n",
        "    # Run Dynamic RNN\n",
        "    #   encoder_outputs: [encoder_length, batch_size, num_units]\n",
        "    #   encoder_state: [batch_size, num_units], this is final state of the cell for each batch.\n",
        "    with tf.variable_scope(scope):\n",
        "      encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell, encoder_emb_inputs, time_major=True, dtype=tf.float32)\n",
        "      \n",
        "    return encoder_inputs, encoder_outputs, encoder_state\n",
        "  \n",
        "  def _build_decoder(self, hparams, encoder_state, encoder_output, scope):\n",
        "    # Decoder input\n",
        "    #   decoder_inputs: [decoder_length, batch_size]\n",
        "    #   decoder_lengths: [batch_size]\n",
        "    #   This is grand truth target inputs for training.\n",
        "    decoder_inputs = tf.placeholder(tf.int32, shape=(hparams.decoder_length, hparams.batch_size), name=\"decoder_inputs\")\n",
        "    decoder_lengths = tf.placeholder(tf.int32, shape=(hparams.batch_size), name=\"decoer_length\")\n",
        "\n",
        "    # EmbeddingDecoder:\n",
        "    #    Embedding for decoder.\n",
        "    #    This is used to convert encode training target texts to list of ids.\n",
        "    with tf.variable_scope(scope):\n",
        "      embedding_decoder = tf.get_variable(\"embedding_decoder\", [hparams.tgt_vocab_size, hparams.embedding_size])\n",
        "\n",
        "    # Look up embedding:\n",
        "    #   decoder_inputs: [decoder_length, batch_size]\n",
        "    #   decoder_emb_inp: [decoder_length, batch_size, embedding_size]\n",
        "    decoder_emb_inputs = tf.nn.embedding_lookup(embedding_decoder, decoder_inputs)   \n",
        "    \n",
        "    # https://stackoverflow.com/questions/39573188/output-projection-in-seq2seq-model-tensorflow\n",
        "    # Internally, a neural network operates on dense vectors of some size,\n",
        "    # often 256, 512 or 1024 floats (let's say 512 for here). \n",
        "    # But at the end it needs to predict a word from the vocabulary which is often much larger,\n",
        "    # e.g., 40000 words. Output projection is the final linear layer that converts (projects) from the internal representation to the larger one.\n",
        "    # So, for example, it can consist of a 512 x 40000 parameter matrix and a 40000 parameter for the bias vector.\n",
        "    projection_layer = layers_core.Dense(hparams.tgt_vocab_size, use_bias=False)\n",
        "    \n",
        "    helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inputs, decoder_lengths, time_major=True)\n",
        "\n",
        "    # Decoder with helper:\n",
        "    #   decoder_emb_inputs: [decoder_length, batch_size, embedding_size]\n",
        "    #   decoder_length: [batch_size] vector, which represents each target sequence length.\n",
        "    decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
        "\n",
        "    if hparams.use_attention:\n",
        "      # Attention\n",
        "      # attention_states: [batch_size, max_time, num_units]\n",
        "      attention_states = tf.transpose(self.encoder_outputs, [1, 0, 2])\n",
        "\n",
        "      # Create an attention mechanism\n",
        "      attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "          hparams.num_units, attention_states,\n",
        "          memory_sequence_length=None)\n",
        "\n",
        "      decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "          decoder_cell, attention_mechanism,\n",
        "          attention_layer_size=hparams.num_units)\n",
        "\n",
        "      initial_state = decoder_cell.zero_state(hparams.batch_size, tf.float32).clone(cell_state=encoder_state)\n",
        "    else:\n",
        "      initial_state = encoder_state    \n",
        "      \n",
        "    # Decoder and decode\n",
        "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "        decoder_cell, helper, initial_state,\n",
        "        output_layer=projection_layer)\n",
        "\n",
        "    # Dynamic decoding\n",
        "    #   final_outputs.rnn_output: [batch_size, decoder_length, tgt_vocab_size], list of RNN state.\n",
        "    #   final_outputs.sample_id: [batch_size, decoder_length], list of argmax of rnn_output.\n",
        "    #   final_state: [batch_size, num_units], list of final state of RNN on decode process.\n",
        "    #   final_sequence_lengths: [batch_size], list of each decoded sequence. \n",
        "    final_outputs, _final_state, _final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
        "\n",
        "    if hparams.debug_verbose:\n",
        "      print(\"rnn_output.shape=\", final_outputs.rnn_output.shape)\n",
        "      print(\"sample_id.shape=\", final_outputs.sample_id.shape)\n",
        "      print(\"final_state=\", _final_state)\n",
        "      print(\"final_sequence_lengths.shape=\", _final_sequence_lengths.shape)\n",
        "\n",
        "    logits = final_outputs.rnn_output    \n",
        "    \n",
        "    # Inference\n",
        "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
        "        embedding_decoder,\n",
        "        tf.fill([hparams.batch_size], tgt_sos_id), tgt_eos_id)\n",
        "\n",
        "    # Inference Decoder\n",
        "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "        decoder_cell, inference_helper, initial_state,\n",
        "        output_layer=projection_layer)\n",
        "\n",
        "    # We should specify maximum_iterations, it can't stop otherwise.\n",
        "    source_sequence_length = hparams.encoder_length\n",
        "    maximum_iterations = tf.round(tf.reduce_max(source_sequence_length) * 2)\n",
        "\n",
        "    # Dynamic decoding\n",
        "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "        inference_decoder, maximum_iterations=maximum_iterations)\n",
        "    replies = outputs.sample_id\n",
        "    \n",
        "    # Beam Search\n",
        "    # Replicate encoder infos beam_width times\n",
        "    decoder_initial_state = tf.contrib.seq2seq.tile_batch(\n",
        "        initial_state, multiplier=hparams.beam_width)\n",
        "\n",
        "    # Define a beam-search decoder\n",
        "    inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
        "            cell=decoder_cell,\n",
        "            embedding=embedding_decoder,\n",
        "            start_tokens=tf.fill([hparams.batch_size], tgt_sos_id),\n",
        "            end_token=tgt_eos_id,\n",
        "            initial_state=decoder_initial_state,\n",
        "            beam_width=hparams.beam_width,\n",
        "            output_layer=projection_layer,\n",
        "            length_penalty_weight=0.0)\n",
        "\n",
        "    # Dynamic decoding\n",
        "    beam_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "        inference_decoder, maximum_iterations=maximum_iterations)\n",
        "    beam_replies = beam_outputs.predicted_ids    \n",
        "\n",
        "    return decoder_inputs, decoder_lengths, replies, beam_replies, logits"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DQg8kU-2Dr-q",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# Helper functions to test\n",
        "def make_test_training_data(hparams):\n",
        "  train_encoder_inputs = np.empty((hparams.encoder_length, hparams.batch_size), dtype=np.int)\n",
        "  training_target_labels = np.empty((hparams.batch_size, hparams.decoder_length), dtype=np.int)\n",
        "  training_decoder_inputs = np.empty((hparams.decoder_length, hparams.batch_size), dtype=np.int)\n",
        "\n",
        "  # We keep first tweet to validate inference.\n",
        "  first_tweet = None\n",
        "\n",
        "  for i in range(hparams.batch_size):\n",
        "    # Tweet\n",
        "    tweet = np.random.randint(low=0, high=hparams.src_vocab_size, size=hparams.encoder_length)\n",
        "    train_encoder_inputs[:, i] = tweet  \n",
        "  \n",
        "    # Reply\n",
        "    #   Note that low = 2, as 0 and 1 are reserved.\n",
        "    reply = np.random.randint(low=2, high=hparams.tgt_vocab_size, size=hparams.decoder_length - 1)\n",
        "  \n",
        "    training_target_label = np.concatenate((reply, np.array([tgt_eos_id])))\n",
        "    training_target_labels[i] = training_target_label\n",
        "  \n",
        "    training_decoder_input = np.concatenate(([tgt_sos_id], reply))\n",
        "    training_decoder_inputs[:, i] = training_decoder_input\n",
        "  \n",
        "    if i == 0:\n",
        "      first_tweet = tweet\n",
        "      if hparams.debug_verbose:\n",
        "        print(\"0th tweet={}\".format(tweet))\n",
        "        print(\"0th reply_with_eos_suffix={}\".format(training_target_label))\n",
        "        print(\"0th reply_with_sos_prefix={}\".format(training_decoder_input))\n",
        "\n",
        "    if hparams.debug_verbose:\n",
        "      print(\"Tweets\")\n",
        "      print(train_encoder_inputs)\n",
        "      print(\"Replies\")\n",
        "      print(training_target_labels)\n",
        "      print(training_decoder_inputs)\n",
        "  return first_tweet, train_encoder_inputs, training_target_labels, training_decoder_inputs\n",
        "\n",
        "def test_training(test_hparams, model):\n",
        "  print(\"==== training model ====\")\n",
        "  first_tweet, train_encoder_inputs, training_target_labels, training_decoder_inputs = make_test_training_data(test_hparams)\n",
        "  # Train\n",
        "  x = []\n",
        "  y = []\n",
        "  for i in range(test_hparams.num_train_steps):\n",
        "    loss_value, global_step = model.train(train_encoder_inputs, training_target_labels, training_decoder_inputs, np.ones((test_hparams.batch_size), dtype=int) * test_hparams.decoder_length)\n",
        "    if i % 5 == 0 and test_hparams.debug_verbose:\n",
        "      print('.', end='')\n",
        "\n",
        "    if i % 15 == 0:\n",
        "      model.save()\n",
        "      x.append(global_step)\n",
        "      y.append(loss_value)\n",
        "      if test_hparams.debug_verbose:\n",
        "        print(\"loss={} step={}\".format(loss_value, global_step))\n",
        "  inference_encoder_inputs = np.empty((test_hparams.encoder_length, test_hparams.batch_size), dtype=np.int)\n",
        "  for i in range(test_hparams.batch_size):\n",
        "    inference_encoder_inputs[:, i] = first_tweet\n",
        "  if test_hparams.debug_verbose:\n",
        "    print(inference_encoder_inputs)\n",
        "  replies = model.infer(inference_encoder_inputs)\n",
        "  print(\"Infered replies\", replies[0][0])\n",
        "  print(\"Expected replies\", training_target_labels[0])\n",
        "  \n",
        "  beam_replies = model.infer_beam_search(inference_encoder_inputs)\n",
        "  print(\"Infered replies candidate0\", beam_replies[0][0][:,0])\n",
        "  print(\"Infered replies candidate1\", beam_replies[0][0][:,1])\n",
        "\n",
        "  if test_hparams.debug_verbose:    \n",
        "    plt.plot(x, y, label=\"Loss\")\n",
        "    plt.plot()\n",
        "    plt.xlabel(\"Loss\")\n",
        "    plt.ylabel(\"steps\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "  \n",
        "def test_two_models_training():\n",
        "  first_tweet, train_encoder_inputs, training_target_labels, training_decoder_inputs = make_test_training_data(test_hparams)\n",
        "\n",
        "  graph1= tf.Graph()\n",
        "  graph2 = tf.Graph()\n",
        "\n",
        "  with graph1.as_default():\n",
        "    sess1 = tf.Session(graph=graph1)\n",
        "    model = ChatbotModel(sess1, test_hparams, model_path=\"./saved_model/hige\")\n",
        "    test_training(test_hparams, model)  \n",
        "\n",
        "  with graph2.as_default():\n",
        "    sess2 = tf.Session(graph=graph2)\n",
        "    model2 = ChatbotModel(sess2, test_hparams, model_path=\"./saved_model2/hige\")\n",
        "    test_training(test_hparams, model2)  \n",
        "  \n"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "St07WCfSBJoG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "! rm -rf ./saved_model\n",
        "! mkdir ./saved_model\n",
        "! rm -rf ./saved_model2\n",
        "! mkdir ./saved_model2\n",
        "\n",
        "# Fresh model\n",
        "test_two_models_training()\n",
        "\n",
        "# Saved model\n",
        "test_two_models_training()\n"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SB627B3UGIac",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kxeWpXO5FThm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "def read_file_from_drive(file_name):\n",
        "  seq2seq_data_dir_id = \"146ZLldWXLDH0l9WbSUNFKi3nVK_HV0Sz\"\n",
        "  file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(seq2seq_data_dir_id)}).GetList()\n",
        "  found = [file for file in file_list if file['title'] == file_name]\n",
        "  if found != []:\n",
        "    downloaded = drive.CreateFile({'id': found[0]['id']})\n",
        "    return downloaded.GetContentString()\n",
        "  else:\n",
        "    raise ValueError(\"file {} not found.\".format(file_name))\n",
        "\n",
        "def read_vocabulary_drive(vocabulary_path):\n",
        "  rev_vocab = []\n",
        "  rev_vocab.extend(read_file_from_drive(vocabulary_path).splitlines())\n",
        "  print(rev_vocab)\n",
        "  rev_vocab = [line.strip() for line in rev_vocab]\n",
        "  vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
        "  return vocab, rev_vocab  \n",
        "  \n",
        "print(read_vocabulary_drive('vocab_dec.txt'))"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Np6GIJATTGg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "def read_training_data_from_drive(file_name, max_line_len, pad_value):\n",
        "  ret = []\n",
        "  for line in read_file_from_drive(file_name).splitlines():\n",
        "    # padding\n",
        "    ids = [int(x) for x in line.split()]\n",
        "    if len(ids) > max_line_len:\n",
        "      ids = ids[:max_line_len]\n",
        "    else:\n",
        "      ids.extend([pad_value] * (max_line_len - len(ids)))\n",
        "    ret.append(ids)\n",
        "  return ret\n",
        "\n",
        "def words_to_ids(words, vocab):\n",
        "  ids = []\n",
        "  for word in words:\n",
        "    if word in vocab:\n",
        "      ids.append(vocab[word])\n",
        "    else:\n",
        "      ids.append(unk_id)\n",
        "  return ids\n",
        "\n",
        "def ids_to_words(ids, rev_vocab):\n",
        "  words = \"\"\n",
        "  for id in ids:\n",
        "    words += rev_vocab[id]\n",
        "  return words\n",
        "# For replies, we use decoder_lenght - 1, because we need to add eos/sos.\n",
        "replies = read_training_data_from_drive('tweets_train_dec_idx.txt', real_hparams.decoder_length - 1, pad_id)\n",
        "tweets = read_training_data_from_drive('tweets_train_enc_idx.txt', real_hparams.encoder_length, pad_id)\n",
        "print(\"tweets_shape=\", len(tweets))\n",
        "\n",
        "src_vocab, rev_src_vocab = read_vocabulary_drive('vocab_enc.txt')\n",
        "tgt_vocab, rev_tgt_vocab = read_vocabulary_drive('vocab_dec.txt')\n",
        "\n"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9STF9lA6UVCG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "sess = tf.Session()\n",
        "# Note that tweets data should be a matrix where each line has exact same length.\n",
        "tweets_dataset = tf.data.Dataset.from_tensor_slices(tweets)\n",
        "replies_dataset = tf.data.Dataset.from_tensor_slices(replies)\n",
        "\n",
        "tweets_transposed = tweets_dataset.batch(real_hparams.batch_size).map(lambda x: tf.transpose(x))\n",
        "replies_with_eos_suffix = replies_dataset.map(lambda x: tf.concat([x, [tgt_eos_id]], axis=0)).batch(real_hparams.batch_size)\n",
        "replies_with_sos_prefix = replies_dataset.map(lambda x: tf.concat([[tgt_sos_id], x], axis=0)).batch(real_hparams.batch_size).map(lambda x: tf.transpose(x))\n",
        "\n",
        "print(\"tweets_example:\", sess.run(tweets_transposed.make_one_shot_iterator().get_next()))\n",
        "print(\"reply_with_eos_suffix_example:\", sess.run(replies_with_eos_suffix.make_one_shot_iterator().get_next()))\n",
        "print(\"reply_with_sos_prefix_example:\", sess.run(replies_with_sos_prefix.make_one_shot_iterator().get_next()))\n",
        "\n",
        "# Merge all using zip\n",
        "train_feed_data = tf.data.Dataset.zip((tweets_transposed, replies_with_eos_suffix, replies_with_sos_prefix))\n",
        "train_feed_data_value = sess.run(train_feed_data.make_one_shot_iterator().get_next())\n",
        "print(\"train_feed_data=\", train_feed_data_value[0])\n",
        "print(\"train_feed_data=\", train_feed_data_value[1])\n",
        "print(\"train_feed_data=\", train_feed_data_value[2])                                 \n",
        "                                 "
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dagyd_fKE8lX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# Train using real data.\n",
        "#! rm -rf ./saved_model/real\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "graph3= tf.Graph()\n",
        "with graph3.as_default():\n",
        "  sess3 = tf.Session(graph=graph3)\n",
        "  model3 = ChatbotModel(sess3, real_hparams, model_path=\"./saved_model/real\")\n",
        "  train_data_iterator = train_feed_data.repeat(1000).shuffle(500000).make_one_shot_iterator()\n",
        "    \n",
        "  for i in range(real_hparams.num_train_steps+25000):\n",
        "    train_data = sess3.run(train_data_iterator.get_next())\n",
        "    loss_value, global_step = model3.train(train_data[0], train_data[1], train_data[2], np.ones((real_hparams.batch_size), dtype=int) * real_hparams.decoder_length)\n",
        "\n",
        "    if i % 5 == 0 and real_hparams.debug_verbose:\n",
        "      print('.', end='')\n",
        "\n",
        "    if i % 15 == 0:\n",
        "      model3.save()\n",
        "      x.append(global_step)\n",
        "      y.append(loss_value)\n",
        "      if real_hparams.debug_verbose:\n",
        "        print(\"loss={} step={}\".format(loss_value, global_step))\n",
        "\n",
        "\n",
        "  "
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y4auRWoHk9RW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "\n",
        "plt.plot(x, y, label=\"Loss\")\n",
        "plt.plot()\n",
        "\n",
        "plt.xlabel(\"Loss\")\n",
        "plt.ylabel(\"steps\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FsvxQU26XsOd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "with graph3.as_default():\n",
        "  inference_encoder_inputs = np.empty((real_hparams.encoder_length, real_hparams.batch_size), dtype=np.int)\n",
        "  tweet = [\"おはよう\"]\n",
        "  tweet_ids = words_to_ids(tweet, src_vocab)\n",
        "  tweet_ids.extend([pad_id] * (real_hparams.encoder_length - len(tweet_ids)))\n",
        "  for i in range(real_hparams.batch_size):\n",
        "    inference_encoder_inputs[:, i] = np.array(tweet_ids, dtype=np.int) \n",
        "\n",
        "  replies = model3.infer(inference_encoder_inputs)\n",
        "  reply = replies[0][0].tolist()\n",
        "  print(\"Infered reply\", ids_to_words(reply, rev_tgt_vocab))\n",
        "  \n",
        "  beam_replies = model3.infer_beam_search(inference_encoder_inputs)\n",
        "  print(\"Infered replies candidate0\", ids_to_words(beam_replies[0][0][:,0], rev_tgt_vocab))\n",
        "  print(\"Infered replies candidate1\", ids_to_words(beam_replies[0][0][:,1], rev_tgt_vocab))"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y0JPF6FUb3Af",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        ""
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    }
  ]
}