{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_ipynb_保存版_learning_rate_(1)-5.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1xk_TGJm51bIgyGwIqmEiR6YI6fDSN6W3",
          "timestamp": 1520410824800
        },
        {
          "file_id": "1Os9oWOWM-thM7tlXMNYfNlAC_L-l7arY",
          "timestamp": 1515554387158
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "FnS-GXJOJOY2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Tensorflow 1.4.0 is required.\n",
        "This is based on [NMT Tutorial](https://github.com/tensorflow/nmt).\n",
        "\n",
        "## notes\n",
        "* Training w/o USERNAME token\n",
        "  * replacing @username with USERNAME token caused high frequency of appearence of USERNAME. The result was poor.\n",
        "  * So simply replacing it with \"\".\n",
        "  * [commit](https://github.com/higepon/tensorflow_seq2seq_chatbot/commit/a2d53ee1b3196bffc97c226230e41e25a1b46904)\n",
        "* Make Attention work\n",
        "  * Now beam/greedy can't work together.\n",
        "* Testing Attention if it coverges with learning rate\n",
        "  * Adam 0.05 and age decay 0.99 didn't work, perplexity stays around 50000-100000\n",
        "  * Adam 0.5, didn't work\n",
        "  * SGD with learning rate = 0.5 paramter no decay, worked very well.\n",
        "    * perplexity goes down to around 80\n",
        "    * おはよう -> おはようございます\n",
        "    * [commit](https://github.com/higepon/tensorflow_seq2seq_chatbot/commit/c9e230a4fbd485e89f838af9e33b1d9c9f7bc0d0)\n",
        "    * 'num_layers': 3, 'vocab_size': 50000, 'embedding_size': 1024\n",
        "  \n",
        "  \n",
        "  \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "bPLkjCHPSyGx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "\n",
        "# @formatter:off\n",
        "import copy as copy\n",
        "import datetime\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from enum import Enum\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.layers import core as layers_core\n",
        "from tensorflow.python.platform import gfile\n",
        "\n",
        "#@formatter:on\n",
        "\n",
        "\n",
        "def colab():\n",
        "    return '/tools/node/bin/forever' == os.environ['_']\n",
        "\n",
        "\n",
        "if colab():\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "# Note for myself.\n",
        "# You've summarized Seq2Seq\n",
        "# at http://d.hatena.ne.jp/higepon/20171210/1512887715.\n",
        "\n",
        "# If you see following error, it means your max(len(tweets of training set)) <  decoder_length.\n",
        "# This should be a bug somewhere in build_decoder, but couldn't find one yet.\n",
        "# You can workaround by setting hparams.decoder_length=max len of tweet in training set.\n",
        "# InvalidArgumentError: logits and labels must have the same first dimension, got logits shape [48,50] and labels shape [54]\n",
        "#\t [[Node: root/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, \n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "def data_dir():\n",
        "    target_dir = \"{}/chatbot_data\".format(str(Path.home()))\n",
        "    if not colab() and not os.path.exists(target_dir):\n",
        "        raise Exception(\"{} not found, you may create\".format(target_dir))\n",
        "    return target_dir\n",
        "\n",
        "\n",
        "def info(message, hparams):\n",
        "    if hparams.debug_verbose:\n",
        "        print(message)\n",
        "\n",
        "\n",
        "def p(path):\n",
        "    if colab():\n",
        "        return path\n",
        "    else:\n",
        "        return \"{}/{}\".format(data_dir(), path)\n",
        "\n",
        "\n",
        "def has_gpu0():\n",
        "    return tf.test.gpu_device_name() == \"/device:GPU:0\"\n",
        "\n",
        "\n",
        "class ModelDirectory(Enum):\n",
        "    tweet_large = 'model/tweet_large'\n",
        "    tweet_large_swapped = 'model/tweet_large_swapped'\n",
        "    tweet_small = 'model/tweet_small'\n",
        "    tweet_small_swapped = 'model/tweet_small_swapped'\n",
        "    tweet_small_rl = 'model/tweet_small_rl'\n",
        "    test_multiple1 = 'model/test_multiple1'\n",
        "    test_multiple2 = 'model/test_multiple2'\n",
        "    test_multiple3 = 'model/test_multiple3'\n",
        "    test_distributed = 'model/test_distributed'\n",
        "\n",
        "    @staticmethod\n",
        "    def create_all_directories():\n",
        "        for d in ModelDirectory:\n",
        "            os.makedirs(p(d.value), exist_ok=True)\n",
        "\n",
        "\n",
        "# todo\n",
        "# collect all initializer\n",
        "ModelDirectory.create_all_directories()\n",
        "\n",
        "if True:\n",
        "    class GoogleDriveFolder(Enum):\n",
        "        root = '146ZLldWXLDH0l9WbSUNFKi3nVK_HV0Sz'\n",
        "        seq2seq = '18lYBgKvX3AG1zhwJqP1tRYJU688U1N95'\n",
        "        seq2seq_swapped = '1w56FFoKStEfZNThA2Y_jx3NDTELcau52'\n",
        "        seq2seq_rl = '1pHnOuT_7JjD1TS8VQ4KN9oUiblBIABXJ'\n",
        "else:\n",
        "    class GoogleDriveFolder(Enum):\n",
        "        root = '15Z3wbaSjR34ziPgAVEFiXgM67ln1Z9Xt'\n",
        "        seq2seq = '1KdMwLNbUfI_PZ5QTyi2zvcS___ct399p'\n",
        "        seq2seq_swapped = '1OjvR4TXAVudSiI-A7EBjw3gLoY4mrO7i'\n",
        "        seq2seq_rl = '161ler0gTpsvFUPAvc4x1__jyClJ_8IyB'\n",
        "\n",
        "base_hparams = tf.contrib.training.HParams(\n",
        "    batch_size=3,\n",
        "    encoder_length=5,\n",
        "    decoder_length=5,\n",
        "    num_units=6,\n",
        "    num_layers=2,\n",
        "    vocab_size=9,\n",
        "    embedding_size=8,\n",
        "    learning_rate=0.01,\n",
        "    learning_rate_decay=0.99,\n",
        "    max_gradient_norm=5.0,\n",
        "    beam_width=2,\n",
        "    use_attention=False,\n",
        "    num_train_steps=100,\n",
        "    debug_verbose=False,\n",
        "    model_folder_in_drive=GoogleDriveFolder.seq2seq.value,\n",
        "    model_path='Please override model_directory',\n",
        "    sos_id=0,\n",
        "    eos_id=1,\n",
        "    pad_id=2,\n",
        "    unk_id=3,\n",
        "    sos_token=\"[SOS]\",\n",
        "    eos_token=\"[EOS]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    unk_token=\"[UNK]\",\n",
        ")\n",
        "\n",
        "test_hparams = copy.deepcopy(base_hparams).override_from_dict(\n",
        "    {'beam_width': 0, 'num_train_steps': 100, 'learning_rate': 0.5})\n",
        "\n",
        "\n",
        "\n",
        "test_attention_hparams = copy.deepcopy(test_hparams).override_from_dict(\n",
        "    {'use_attention': True})\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DFEYKvBwL3Nm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# For debug purpose.\n",
        "tf.reset_default_graph()\n",
        "\n",
        "\n",
        "class ChatbotModel:\n",
        "    def __init__(self, sess, hparams, model_path, scope='ChatbotModel'):\n",
        "        self.sess = sess\n",
        "        # todo remove\n",
        "        self.hparams = hparams\n",
        "\n",
        "        # todo\n",
        "        self.model_path = model_path\n",
        "        self.name = scope\n",
        "\n",
        "        self.encoder_inputs, self.encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder = self._build_encoder(\n",
        "            hparams, scope)\n",
        "        self.decoder_inputs, self.decoder_target_lengths, self.logits = self._build_decoder(\n",
        "            hparams, self.encoder_inputs_lengths, embedding_encoder,\n",
        "            encoder_state, encoder_outputs)\n",
        "\n",
        "        self.reward = tf.placeholder(tf.float32, name=\"reward\")\n",
        "        self.target_labels, self.loss, self.global_step, self.learning_rate, self.train_op = self._build_optimizer(\n",
        "            hparams, self.logits)\n",
        "        self.train_loss_summary = tf.summary.scalar(\"loss\", self.loss)\n",
        "        self.valiation_loss_summary = tf.summary.scalar(\"validation_loss\", self.loss)\n",
        "        self.merged_summary = tf.summary.merge_all()\n",
        "\n",
        "        # Initialize saver after model created\n",
        "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
        "\n",
        "    def restore(self):\n",
        "        ckpt = tf.train.get_checkpoint_state(self.model_path)\n",
        "        if ckpt:\n",
        "            last_model = ckpt.model_checkpoint_path\n",
        "            self.saver.restore(self.sess, last_model)\n",
        "            return True\n",
        "        else:\n",
        "            print(\"Created fresh model.\")\n",
        "            return False\n",
        "\n",
        "    def train(self, encoder_inputs, encoder_inputs_lengths, target_labels,\n",
        "              decoder_inputs, decoder_target_lengths, reward=1.0):\n",
        "        feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "            self.target_labels: target_labels,\n",
        "            self.decoder_inputs: decoder_inputs,\n",
        "            self.decoder_target_lengths: decoder_target_lengths,\n",
        "            # For normal Seq2Seq reward is always 1.\n",
        "            self.reward: reward\n",
        "        }\n",
        "        _, global_step, summary = self.sess.run(\n",
        "            [self.train_op, self.global_step, self.train_loss_summary], feed_dict=feed_dict)\n",
        "        return global_step, self.learning_rate, summary\n",
        "\n",
        "    def batch_loss(self, encoder_inputs, encoder_inputs_lengths, target_labels,\n",
        "                   decoder_inputs, decoder_target_lengths):\n",
        "        feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "            self.target_labels: target_labels,\n",
        "            self.decoder_inputs: decoder_inputs,\n",
        "            self.decoder_target_lengths: decoder_target_lengths,\n",
        "            # For normal Seq2Seq reward is always 1.\n",
        "            self.reward: 1\n",
        "        }\n",
        "        return self.sess.run([self.loss, self.valiation_loss_summary],\n",
        "                             feed_dict=feed_dict)\n",
        "\n",
        "    def train_with_reward(self, infer_model, standard_seq2seq_model,\n",
        "                          encoder_inputs, encoder_inputs_lengths, target_labels,\n",
        "                          decoder_inputs, decoder_target_lengths,\n",
        "                          dull_responses):\n",
        "        infered_replies = infer_model.infer(encoder_inputs,\n",
        "                                            encoder_inputs_lengths)\n",
        "        standard_seq2seq_encoder_inputs = []\n",
        "        standard_seq2seq_encoder_inputs_lengths = []\n",
        "        for reply in infered_replies:\n",
        "            standard_seq2seq_encoder_inputs_lengths.append(len(reply))\n",
        "            if len(reply) <= self.hparams.encoder_length:\n",
        "                standard_seq2seq_encoder_inputs.append(np.append(reply, (\n",
        "                        [self.hparams.pad_id] * (self.hparams.encoder_length - len(reply)))))\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    \"Inferred\"\n",
        "                    \" reply shouldn't be longer than encoder_input\")\n",
        "        standard_seq2seq_encoder_inputs = np.transpose(\n",
        "            np.array(standard_seq2seq_encoder_inputs))\n",
        "        reward1 = standard_seq2seq_model.reward_ease_of_answering(\n",
        "            standard_seq2seq_encoder_inputs,\n",
        "            standard_seq2seq_encoder_inputs_lengths, dull_responses)\n",
        "        reward2 = 0  # todo\n",
        "        reward3 = 0  # todo\n",
        "        reward = 0.25 * reward1 + 0.25 * reward2 + 0.5 * reward3\n",
        "        return self.train(encoder_inputs, encoder_inputs_lengths, target_labels,\n",
        "                          decoder_inputs, decoder_target_lengths, reward)\n",
        "\n",
        "    def save(self, model_path=None):\n",
        "        if model_path is None:\n",
        "            model_path = self.model_path\n",
        "        model_dir = \"{}/{}\".format(model_path, self.name)\n",
        "        self.saver.save(self.sess, model_dir, global_step=self.global_step)\n",
        "\n",
        "    @staticmethod\n",
        "    def _softmax(x):\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    def _build_optimizer(self, hparams, logits):\n",
        "        # Target labels\n",
        "        #   As described in doc for sparse_softmax_cross_entropy_with_logits,\n",
        "        #   labels should be [batch_size, decoder_target_lengths]\n",
        "        #   instead of [batch_size, decoder_target_lengths, vocab_size].\n",
        "        #   So labels should have indices instead of vocab_size classes.\n",
        "        target_labels = tf.placeholder(tf.int32, shape=(\n",
        "            hparams.batch_size, hparams.decoder_length), name=\"target_labels\")\n",
        "        \n",
        "        # Loss\n",
        "        #   target_labels: [batch_size, decoder_length]\n",
        "        #   logits: [batch_size, decoder_length, vocab_size]\n",
        "        #   crossent: [batch_size, decoder_length]\n",
        "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            labels=target_labels, logits=logits)\n",
        "        \n",
        "        target_weights = tf.sequence_mask(self.decoder_target_lengths, hparams.decoder_length, dtype=logits.dtype)\n",
        "\n",
        "        loss = tf.reduce_sum(\n",
        "          crossent * target_weights) / tf.to_float(hparams.batch_size)\n",
        "        \n",
        "        # Adjust loss with reward.\n",
        "        #loss = tf.multiply(loss, self.reward)\n",
        "\n",
        "        # Train\n",
        "        global_step = tf.get_variable(name=\"global_step\", shape=[],\n",
        "                                      dtype=tf.int32,\n",
        "                                      initializer=tf.constant_initializer(0),\n",
        "                                      trainable=False)\n",
        "        \n",
        "        learning_rate = hparams.learning_rate # tf.train.exponential_decay(hparams.learning_rate, global_step, 100, hparams.learning_rate_decay) \n",
        "\n",
        "        # Calculate and clip gradients\n",
        "        params = tf.trainable_variables()\n",
        "        for param in params:\n",
        "          info(\"  {}, {}, {}\".format(param.name, str(param.get_shape()),\n",
        "                                        param.op.device), hparams)\n",
        "        \n",
        "        gradients = tf.gradients(loss, params)\n",
        "        clipped_gradients, _ = tf.clip_by_global_norm(\n",
        "            gradients, hparams.max_gradient_norm)\n",
        "\n",
        "        # Optimization\n",
        "#        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "        device = '/cpu:0'\n",
        "        if has_gpu0():\n",
        "            device = '/gpu:0'\n",
        "            print(\"!!!GPU ENABLED !!!\")\n",
        "        with tf.device(device):\n",
        "            train_op = optimizer.apply_gradients(\n",
        "                zip(clipped_gradients, params), global_step=global_step)\n",
        "        return target_labels, loss, global_step, learning_rate, train_op\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_encoder(hparams, scope):\n",
        "        # Encoder\n",
        "        #   encoder_inputs: [encoder_length, batch_size]\n",
        "        #   This is time major where encoder_length comes\n",
        "        #   first instead of batch_size.\n",
        "        #   encoder_inputs_lengths: [batch_size]\n",
        "        encoder_inputs = tf.placeholder(tf.int32, shape=(\n",
        "            hparams.encoder_length, hparams.batch_size), name=\"encoder_inputs\")\n",
        "        encoder_inputs_lengths = tf.placeholder(tf.int32,\n",
        "                                                shape=hparams.batch_size,\n",
        "                                                name=\"encoder_inputs_lengtsh\")\n",
        "\n",
        "        # Embedding\n",
        "        #   We originally didn't share embedding between encoder and decoder.\n",
        "        #   But now we share it. It makes much easier to calculate rewards.\n",
        "        #   Matrix for embedding: [vocab_size, embedding_size]\n",
        "        #   Should be shared between training and inference.\n",
        "        with tf.variable_scope(scope):\n",
        "            embedding_encoder = tf.get_variable(\"embedding_encoder\",\n",
        "                                                [hparams.vocab_size,\n",
        "                                                 hparams.embedding_size])\n",
        "\n",
        "        # Look up embedding:\n",
        "        #   encoder_inputs: [encoder_length, batch_size]\n",
        "        #   encoder_emb_inputs: [encoder_length, batch_size, embedding_size]\n",
        "        encoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder,\n",
        "                                                    encoder_inputs)\n",
        "\n",
        "        # LSTM cell.\n",
        "        with tf.variable_scope(scope):\n",
        "            # Should be shared between training and inference.\n",
        "            cell_list = []\n",
        "            for _ in range(hparams.num_layers):\n",
        "               cell_list.append(tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
        "            encoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
        "\n",
        "        # Run Dynamic RNN\n",
        "        #   encoder_outputs: [encoder_length, batch_size, num_units]\n",
        "        #   encoder_state: [batch_size, num_units],\n",
        "        #   this is final state of the cell for each batch.\n",
        "        with tf.variable_scope(scope):\n",
        "            encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell,\n",
        "                                                               encoder_emb_inputs,\n",
        "                                                               time_major=True,\n",
        "                                                               dtype=tf.float32,\n",
        "                                                               sequence_length=encoder_inputs_lengths)\n",
        "\n",
        "        return encoder_inputs, encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_training_decoder(hparams, encoder_inputs_lengths,\n",
        "                                encoder_state, encoder_outputs, decoder_cell,\n",
        "                                decoder_emb_inputs, decoder_target_lengths,\n",
        "                                projection_layer):\n",
        "        # Decoder with helper:\n",
        "        #   decoder_emb_inputs: [decoder_length, batch_size, embedding_size]\n",
        "        #   decoder_target_lengths: [batch_size] vector,\n",
        "        #   which represents each target sequence length.\n",
        "        training_helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inputs,\n",
        "                                                            decoder_target_lengths,\n",
        "                                                            time_major=True)\n",
        "\n",
        "        # See https://github.com/tensorflow/tensorflow/issues/11904\n",
        "        if hparams.use_attention:\n",
        "            # Attention\n",
        "            # encoder_outputs is time major, so transopse it to batch major.\n",
        "            # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
        "            attention_encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2])\n",
        "\n",
        "            # Create an attention mechanism\n",
        "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "                hparams.num_units,\n",
        "                attention_encoder_outputs,\n",
        "                memory_sequence_length=encoder_inputs_lengths)\n",
        "\n",
        "            wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "                decoder_cell, attention_mechanism,\n",
        "                attention_layer_size=hparams.num_units)\n",
        "\n",
        "            initial_state = wrapped_decoder_cell.zero_state(hparams.batch_size,\n",
        "                                                            tf.float32).clone(\n",
        "                cell_state=encoder_state)\n",
        "        else:\n",
        "            wrapped_decoder_cell = decoder_cell\n",
        "            initial_state = encoder_state\n",
        "\n",
        "            # Decoder and decode\n",
        "        training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "            wrapped_decoder_cell, training_helper, initial_state,\n",
        "            output_layer=projection_layer)\n",
        "\n",
        "        # Dynamic decoding\n",
        "        #   final_outputs.rnn_output: [batch_size, decoder_length,\n",
        "        #                             vocab_size], list of RNN state.\n",
        "        #   final_outputs.sample_id: [batch_size, decoder_length],\n",
        "        #                            list of argmax of rnn_output.\n",
        "        #   final_state: [batch_size, num_units],\n",
        "        #                list of final state of RNN on decode process.\n",
        "        #   final_sequence_lengths: [batch_size], list of each decoded sequence. \n",
        "        final_outputs, _final_state, _final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n",
        "            training_decoder)\n",
        "\n",
        "        if hparams.debug_verbose:\n",
        "            print(\"rnn_output.shape=\", final_outputs.rnn_output.shape)\n",
        "            print(\"sample_id.shape=\", final_outputs.sample_id.shape)\n",
        "            print(\"final_state=\", _final_state)\n",
        "            print(\"final_sequence_lengths.shape=\",\n",
        "                  _final_sequence_lengths.shape)\n",
        "\n",
        "        logits = final_outputs.rnn_output\n",
        "        return logits, wrapped_decoder_cell, initial_state\n",
        "\n",
        "    def _build_decoder(self, hparams, encoder_inputs_lengths, embedding_encoder,\n",
        "                       encoder_state, encoder_outputs):\n",
        "        # Decoder input\n",
        "        #   decoder_inputs: [decoder_length, batch_size]\n",
        "        #   decoder_target_lengths: [batch_size]\n",
        "        #   This is grand truth target inputs for training.\n",
        "        decoder_inputs = tf.placeholder(tf.int32, shape=(\n",
        "            hparams.decoder_length, hparams.batch_size), name=\"decoder_inputs\")\n",
        "        decoder_target_lengths = tf.placeholder(tf.int32,\n",
        "                                                shape=hparams.batch_size,\n",
        "                                                name=\"decoder_target_lengths\")\n",
        "\n",
        "        # Look up embedding:\n",
        "        #   decoder_inputs: [decoder_length, batch_size]\n",
        "        #   decoder_emb_inp: [decoder_length, batch_size, embedding_size]\n",
        "        decoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder,\n",
        "                                                    decoder_inputs)\n",
        "\n",
        "        # https://stackoverflow.com/questions/39573188/output-projection-in-seq2seq-model-tensorflow\n",
        "        # Internally, a neural network operates on dense vectors of some size,\n",
        "        # often 256, 512 or 1024 floats (let's say 512 for here). \n",
        "        # But at the end it needs to predict a word\n",
        "        # from the vocabulary which is often much larger,\n",
        "        # e.g., 40000 words. Output projection is the final linear layer\n",
        "        # that converts (projects) from the internal representation\n",
        "        #  to the larger one.\n",
        "        # So, for example, it can consist of a 512 x 40000 parameter matrix\n",
        "        # and a 40000 parameter for the bias vector.\n",
        "        projection_layer = layers_core.Dense(hparams.vocab_size, use_bias=False)\n",
        "\n",
        "        # We share this between training and inference.\n",
        "        cell_list = []\n",
        "        for _ in range(hparams.num_layers):\n",
        "           cell_list.append(tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
        "        decoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
        "        \n",
        "#        decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
        "\n",
        "        # Training graph\n",
        "        logits, wrapped_decoder_cell, initial_state = self._build_training_decoder(\n",
        "            hparams, encoder_inputs_lengths, encoder_state, encoder_outputs,\n",
        "            decoder_cell, decoder_emb_inputs, decoder_target_lengths,\n",
        "            projection_layer)\n",
        "\n",
        "        return decoder_inputs, decoder_target_lengths, logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JzDknaQZV-iU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class ChatbotInferenceModel:\n",
        "    def __init__(self, sess, hparams, model_path, scope='ChatbotModel'):\n",
        "        self.sess = sess\n",
        "        # todo remove\n",
        "        self.hparams = hparams\n",
        "\n",
        "        # todo\n",
        "        self.model_path = model_path\n",
        "        self.name = scope\n",
        "\n",
        "        self.encoder_inputs, self.encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder = self._build_encoder(\n",
        "            hparams, scope)\n",
        "        self.decoder_inputs, self.decoder_target_lengths, self.replies, self.beam_replies, self.infer_logits = self._build_decoder(\n",
        "            hparams, self.encoder_inputs_lengths, embedding_encoder,\n",
        "            encoder_state, encoder_outputs)\n",
        "\n",
        "        self.reward = tf.placeholder(tf.float32, name=\"reward\")\n",
        "\n",
        "        # we can't use variable length here, \n",
        "        # because tiled_batch requires constant length.\n",
        "        self.batch_size = 1\n",
        "\n",
        "        # Initialize saver after model created\n",
        "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
        "\n",
        "    def restore(self):\n",
        "        ckpt = tf.train.get_checkpoint_state(self.model_path)\n",
        "        if ckpt:\n",
        "            last_model = ckpt.model_checkpoint_path\n",
        "            self.saver.restore(self.sess, last_model)\n",
        "            return True\n",
        "        else:\n",
        "            print(\"Created fresh model.\")\n",
        "            return False\n",
        "\n",
        "    def infer(self, encoder_inputs, encoder_inputs_lengths):\n",
        "        # Should not call this when beam search enabled.\n",
        "        assert(self.hparams.beam_width == 0)\n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "        }\n",
        "        replies = self.sess.run(self.replies, feed_dict=inference_feed_dict)\n",
        "        return replies\n",
        "\n",
        "    def infer_beam_search(self, encoder_inputs, encoder_inputs_lengths):\n",
        "        # Should not call this when beam search disabled.\n",
        "        assert(self.hparams.beam_width > 0)      \n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "        }\n",
        "        replies = self.sess.run(self.beam_replies,\n",
        "                                feed_dict=inference_feed_dict)\n",
        "        return replies\n",
        "     \n",
        "    # imakoko\n",
        "    def infer_mi(self, swapped_model, encoder_inputs, encoder_inputs_lengths):\n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths,\n",
        "        }\n",
        "        beam_replies = self.sess.run(self.beam_replies, feed_dict=inference_feed_dict)\n",
        "        # beam_replis [batch_size, length, , batch_width]\n",
        "        # for now we assume encoder_inputs is batch_size = 1\n",
        "        \n",
        "        swapped_encoder_inputs = beam_replies[0]\n",
        "        # beam_width = batch_size\n",
        "        swapped_batch_size = swapped_encoder_inputs.shape[1]\n",
        "        \n",
        "        # beam_replies can be shorten less than decoder_output_legth, so we pad them.\n",
        "        paddings = tf.constant([[0, self.hparams.encoder_length - swapped_encoder_inputs.shape[0],], [0, 0]])\n",
        "        swapped_encoder_inputs = swapped_model.sess.run(tf.pad(swapped_encoder_inputs, paddings, \"CONSTANT\", constant_values=self.hparams.pad_id))\n",
        "        swapped_encoder_inputs_lengths = np.empty(swapped_batch_size, dtype=np.int)\n",
        "        for i in range(swapped_batch_size):\n",
        "          swapped_encoder_inputs_lengths[i] = swapped_encoder_inputs.shape[0]\n",
        "        \n",
        "        return swapped_model.infer_beam_search(swapped_encoder_inputs, swapped_encoder_inputs_lengths)\n",
        "        # todo make correct length\n",
        "#        for repy in beam_replies:\n",
        "          # logits from swapped_model for this reply\n",
        "          # cals prob for in original encoder_input\n",
        "      \n",
        "\n",
        "    def log_prob(self, encoder_inputs, encoder_inputs_lengths, expected_output):\n",
        "        \"\"\"Return sum of log probability of given\n",
        "           one specific expected_output for encoder_inputs.\n",
        "    \n",
        "        Args:\n",
        "            encoder_inputs: [encoder_length, batch_size], eg) tweets\n",
        "            expected_output: [1, decoder_length or less than decoder_length],\n",
        "            eg) One reply.\n",
        "    \n",
        "        Returns:\n",
        "            Return log probablity of expected output for given encoder inputs.\n",
        "            eg) sum of log probability of reply \"Good\" when given [\"How are you?\",\n",
        "             \"What's up?\"]\n",
        "        \"\"\"\n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths\n",
        "        }\n",
        "\n",
        "        # Logits\n",
        "        #   logits_value: [batch_size, actual_decoder_length, vocab_size]\n",
        "        logits_batch_value = self.sess.run(self.infer_logits,\n",
        "                                           feed_dict=inference_feed_dict)\n",
        "\n",
        "        sum_p = []\n",
        "        # For each batch: [actual_decoder_length, vocab_size]\n",
        "        for logits in logits_batch_value:\n",
        "            p = 1\n",
        "            # Note that expected_output and logits don't always have\n",
        "            # same length, but zip takes care of the case.\n",
        "            for word_id, logit in zip(expected_output, logits):\n",
        "                # Apply softmax first, see definition of softmax.\n",
        "                norm = (self._softmax(logit))[word_id]\n",
        "                p *= norm\n",
        "            p = np.log(p)\n",
        "            sum_p.append(p)\n",
        "        ret = np.sum(sum_p) / len(sum_p)\n",
        "        return ret\n",
        "\n",
        "    def reward_ease_of_answering(self, encoder_inputs, encoder_inputs_lengths,\n",
        "                                 expected_outputs):\n",
        "        \"\"\" Return reward for ease of answering. \n",
        "            See Deep Reinforcement Learning for Dialogue Generation\n",
        "            for more details.\n",
        "    \n",
        "        Args:\n",
        "            encoder_inputs: [encoder_length, batch_size], eg) tweets\n",
        "            expected_outputs: [number of pre-defined dull responses,\n",
        "            decoder_length or less than decoder_length].\n",
        "            eg) [[\"I'm\", \"Good\"], [\"fine\"]]\n",
        "    \n",
        "        Returns:\n",
        "            Return reward for ease of answering.\n",
        "            Note that this can be calculated\n",
        "            by calling log_prob function for each dull response,\n",
        "            but this function is more efficient\n",
        "            because this calculated the reward at once.\n",
        "        \"\"\"\n",
        "        inference_feed_dict = {\n",
        "            self.encoder_inputs: encoder_inputs,\n",
        "            self.encoder_inputs_lengths: encoder_inputs_lengths\n",
        "        }\n",
        "\n",
        "        # Logits\n",
        "        #   logits_value: [batch_size, actual_decoder_length, vocab_size]\n",
        "        logits_batch_value = self.sess.run(self.infer_logits,\n",
        "                                           feed_dict=inference_feed_dict)\n",
        "\n",
        "        batch_sum_p = []\n",
        "        # For each batch: [actual_decoder_length, vocab_size]\n",
        "        for logits in logits_batch_value:\n",
        "            sum_p = []\n",
        "            for expected_output in expected_outputs:\n",
        "                p = 1\n",
        "                # Note that expected_output and logits don't\n",
        "                # always have same length, but zip takes care of the case.\n",
        "                for word_id, logit in zip(expected_output, logits):\n",
        "                    # Apply softmax first, see definition of softmax.\n",
        "                    norm = (self._softmax(logit))[word_id]\n",
        "                    p *= norm\n",
        "                p = np.log(p) / len(expected_output)\n",
        "                sum_p.append(p)\n",
        "            one_batch_p = np.sum(sum_p)\n",
        "            batch_sum_p.append(one_batch_p)\n",
        "        ret = np.sum(batch_sum_p) / len(batch_sum_p)\n",
        "        return -ret\n",
        "\n",
        "    @staticmethod\n",
        "    def _softmax(x):\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_encoder(hparams, scope):\n",
        "        # Encoder\n",
        "        #   encoder_inputs: [encoder_length, batch_size]\n",
        "        #   This is time major where encoder_length\n",
        "        #   comes first instead of batch_size.\n",
        "        #   encoder_inputs_lengths: [batch_size]\n",
        "        encoder_inputs = tf.placeholder(tf.int32,\n",
        "                                        shape=[hparams.encoder_length, None],\n",
        "                                        name=\"encoder_inputs\")\n",
        "        encoder_inputs_lengths = tf.placeholder(tf.int32, shape=[None],\n",
        "                                                name=\"encoder_inputs_lengths\")\n",
        "\n",
        "        # Embedding\n",
        "        #   We originally didn't share embedding between encoder and decoder.\n",
        "        #   But now we share it. It makes much easier to calculate rewards.\n",
        "        #   Matrix for embedding: [vocab_size, embedding_size]\n",
        "        #   Should be shared between training and inference.\n",
        "        with tf.variable_scope(scope):\n",
        "            embedding_encoder = tf.get_variable(\"embedding_encoder\",\n",
        "                                                [hparams.vocab_size,\n",
        "                                                 hparams.embedding_size])\n",
        "\n",
        "        # Look up embedding:\n",
        "        #   encoder_inputs: [encoder_length, batch_size]\n",
        "        #   encoder_emb_inputs: [encoder_length, batch_size, embedding_size]\n",
        "        encoder_emb_inputs = tf.nn.embedding_lookup(embedding_encoder,\n",
        "                                                    encoder_inputs)\n",
        "\n",
        "        # LSTM cell.\n",
        "        with tf.variable_scope(scope):\n",
        "            # Should be shared between training and inference.\n",
        "#            encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
        "            cell_list = []\n",
        "            for _ in range(hparams.num_layers):\n",
        "               cell_list.append(tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
        "            encoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
        "\n",
        "        # Run Dynamic RNN\n",
        "        #   encoder_outputs: [encoder_length, batch_size, num_units]\n",
        "        #   encoder_state: [batch_size, num_units],\n",
        "        #   this is final state of the cell for each batch.\n",
        "        with tf.variable_scope(scope):\n",
        "            encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell,\n",
        "                                                               encoder_emb_inputs,\n",
        "                                                               time_major=True,\n",
        "                                                               dtype=tf.float32,\n",
        "                                                               sequence_length=encoder_inputs_lengths)\n",
        "\n",
        "        return encoder_inputs, encoder_inputs_lengths, encoder_outputs, encoder_state, embedding_encoder\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_greedy_inference(hparams, embedding_encoder, encoder_state,\n",
        "                                encoder_inputs_lengths, encoder_outputs,\n",
        "                                decoder_cell, projection_layer):\n",
        "        if hparams.beam_width > 0:\n",
        "          return None, None\n",
        "        \n",
        "        # Greedy decoder\n",
        "        dynamic_batch_size = tf.shape(encoder_inputs_lengths)[0]\n",
        "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
        "            embedding_encoder,\n",
        "            tf.fill([dynamic_batch_size], hparams.sos_id), hparams.eos_id)\n",
        "\n",
        "        # See https://github.com/tensorflow/tensorflow/issues/11904\n",
        "        if hparams.use_attention:\n",
        "            # Attention\n",
        "            # encoder_outputs is time major, so transopse it to batch major.\n",
        "            # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
        "            attention_encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2])\n",
        "\n",
        "            # Create an attention mechanism\n",
        "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "                hparams.num_units,\n",
        "                attention_encoder_outputs,\n",
        "                memory_sequence_length=encoder_inputs_lengths)\n",
        "\n",
        "            wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "                decoder_cell, attention_mechanism,\n",
        "                attention_layer_size=hparams.num_units)\n",
        "\n",
        "            initial_state = wrapped_decoder_cell.zero_state(dynamic_batch_size,\n",
        "                                                            tf.float32).clone(\n",
        "                cell_state=encoder_state)\n",
        "        else:\n",
        "            wrapped_decoder_cell = decoder_cell\n",
        "            initial_state = encoder_state\n",
        "\n",
        "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "            wrapped_decoder_cell, inference_helper, initial_state,\n",
        "            output_layer=projection_layer)\n",
        "\n",
        "        # len(inferred_reply) is lte encoder_length,\n",
        "        # because we are targeting tweet (140 for each tweet)\n",
        "        # Also by doing this,\n",
        "        # we can pass the reply to other seq2seq w/o shorten it.\n",
        "        maximum_iterations = hparams.encoder_length\n",
        "\n",
        "        # Dynamic decoding\n",
        "        outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "            inference_decoder, maximum_iterations=maximum_iterations)\n",
        "        replies = outputs.sample_id\n",
        "\n",
        "        # We use infer_logits instead of logits when calculating log_prob,\n",
        "        # because infer_logits doesn't require decoder_target_lengths input.\n",
        "        infer_logits = outputs.rnn_output\n",
        "        return infer_logits, replies\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_beam_search_inference(hparams, encoder_inputs_lengths,\n",
        "                                     embedding_encoder, encoder_state,\n",
        "                                     encoder_outputs, decoder_cell,\n",
        "                                     projection_layer):\n",
        "      \n",
        "        if hparams.beam_width == 0:\n",
        "          return None\n",
        "      \n",
        "        dynamic_batch_size = tf.shape(encoder_inputs_lengths)[0]\n",
        "        # https://github.com/tensorflow/tensorflow/issues/11904\n",
        "        if hparams.use_attention:\n",
        "            # Attention\n",
        "            # encoder_outputs is time major, so transopse it to batch major.\n",
        "            # attention_encoder_outputs: [batch_size, encoder_length, num_units]\n",
        "            attention_encoder_outputs = tf.transpose(encoder_outputs, [1, 0, 2])\n",
        "\n",
        "            tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(\n",
        "                attention_encoder_outputs, multiplier=hparams.beam_width)\n",
        "            tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(\n",
        "                encoder_state, multiplier=hparams.beam_width)\n",
        "            tiled_encoder_inputs_lengths = tf.contrib.seq2seq.tile_batch(\n",
        "                encoder_inputs_lengths, multiplier=hparams.beam_width)\n",
        "\n",
        "            # Create an attention mechanism\n",
        "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "                hparams.num_units, tiled_encoder_outputs,\n",
        "                memory_sequence_length=tiled_encoder_inputs_lengths)\n",
        "\n",
        "            wrapped_decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "                decoder_cell, attention_mechanism,\n",
        "                attention_layer_size=hparams.num_units)\n",
        "\n",
        "            decoder_initial_state = wrapped_decoder_cell.zero_state(\n",
        "                dtype=tf.float32,\n",
        "                batch_size=dynamic_batch_size * hparams.beam_width)\n",
        "            decoder_initial_state = decoder_initial_state.clone(\n",
        "                cell_state=tiled_encoder_final_state)\n",
        "        else:\n",
        "            wrapped_decoder_cell = decoder_cell\n",
        "            decoder_initial_state = tf.contrib.seq2seq.tile_batch(encoder_state,\n",
        "                                                                  multiplier=hparams.beam_width)\n",
        "\n",
        "        # len(inferred_reply) is lte encoder_length,\n",
        "        # because we are targeting tweet (140 for each tweet)\n",
        "        # Also by doing this,\n",
        "        # we can pass the reply to other seq2seq w/o shorten it.\n",
        "        maximum_iterations = hparams.encoder_length\n",
        "\n",
        "        inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
        "            cell=wrapped_decoder_cell,\n",
        "            embedding=embedding_encoder,\n",
        "            start_tokens=tf.fill([dynamic_batch_size], hparams.sos_id),\n",
        "            end_token=hparams.eos_id,\n",
        "            initial_state=decoder_initial_state,\n",
        "            beam_width=hparams.beam_width,\n",
        "            output_layer=projection_layer,\n",
        "            length_penalty_weight=0.0)\n",
        "\n",
        "        # Dynamic decoding\n",
        "        beam_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "            inference_decoder, maximum_iterations=maximum_iterations)\n",
        "        beam_replies = beam_outputs.predicted_ids\n",
        "        return beam_replies\n",
        "\n",
        "    def _build_decoder(self, hparams, encoder_inputs_lengths, embedding_encoder,\n",
        "                       encoder_state, encoder_outputs):\n",
        "        # Decoder input\n",
        "        #   decoder_inputs: [decoder_length, batch_size]\n",
        "        #   decoder_target_lengths: [batch_size]\n",
        "        #   This is grand truth target inputs for training.\n",
        "        decoder_inputs = tf.placeholder(tf.int32,\n",
        "                                        shape=[hparams.decoder_length, None],\n",
        "                                        name=\"decoder_inputs\")\n",
        "        decoder_target_lengths = tf.placeholder(tf.int32, shape=[None],\n",
        "                                                name=\"decoder_target_lengths\")\n",
        "\n",
        "        # https://stackoverflow.com/questions/39573188/output-projection-in-seq2seq-model-tensorflow\n",
        "        # Internally, a neural network operates on dense vectors of some size,\n",
        "        # often 256, 512 or 1024 floats (let's say 512 for here). \n",
        "        # But at the end it needs to predict a word\n",
        "        # from the vocabulary which is often much larger,\n",
        "        # e.g., 40000 words. Output projection is the final linear\n",
        "        # layer that converts (projects) from the internal\n",
        "        # representation to the larger one.\n",
        "        # So, for example, it can consist of a 512 x 40000 parameter\n",
        "        # matrix and a 40000 parameter for the bias vector.\n",
        "        projection_layer = layers_core.Dense(hparams.vocab_size, use_bias=False)\n",
        "\n",
        "        # We share this between training and inference.\n",
        "#        decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units)\n",
        "        cell_list = []\n",
        "        for _ in range(hparams.num_layers):\n",
        "           cell_list.append(tf.nn.rnn_cell.BasicLSTMCell(hparams.num_units))\n",
        "        decoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
        "        \n",
        "\n",
        "        # Greedy Inference graph\n",
        "        infer_logits, replies = self._build_greedy_inference(hparams,\n",
        "                                                             embedding_encoder,\n",
        "                                                             encoder_state,\n",
        "                                                             encoder_inputs_lengths,\n",
        "                                                             encoder_outputs,\n",
        "                                                             decoder_cell,\n",
        "                                                             projection_layer)\n",
        "\n",
        "        # Beam Search Inference graph\n",
        "        beam_replies = self._build_beam_search_inference(hparams,\n",
        "                                                         encoder_inputs_lengths,\n",
        "                                                         embedding_encoder,\n",
        "                                                         encoder_state,\n",
        "                                                         encoder_outputs,\n",
        "                                                         decoder_cell,\n",
        "                                                         projection_layer)\n",
        "\n",
        "        return decoder_inputs, decoder_target_lengths, replies, beam_replies, infer_logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ul5WBjSF3vy9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class InferenceHelper:\n",
        "    def __init__(self, model, vocab, rev_vocab):\n",
        "        self.model = model\n",
        "        self.vocab = vocab\n",
        "        self.rev_vocab = rev_vocab\n",
        "\n",
        "    def inferences(self, tweet):\n",
        "        encoder_inputs, encoder_inputs_lengths = self.create_inference_input(\n",
        "            tweet)\n",
        "        if self.model.hparams.beam_width == 0:\n",
        "          replies = self.model.infer(encoder_inputs, encoder_inputs_lengths)\n",
        "          ids = replies[0].tolist()\n",
        "          return [self.ids_to_words(ids)]\n",
        "        else:\n",
        "          beam_replies = self.model.infer_beam_search(encoder_inputs,\n",
        "                                                      encoder_inputs_lengths)\n",
        "\n",
        "          return [self.ids_to_words(beam_replies[0][:, i]) for i in range(self.model.hparams.beam_width)]\n",
        "\n",
        "    def print_inferences(self, tweet):\n",
        "        print(tweet)\n",
        "        for i, reply in enumerate(self.inferences(tweet)):\n",
        "            print(\"    [{}]{}\".format(i, reply))\n",
        "\n",
        "    def words_to_ids(self, words):\n",
        "        ids = []\n",
        "        for word in words:\n",
        "            if word in self.vocab:\n",
        "                ids.append(self.vocab[word])\n",
        "            else:\n",
        "                ids.append(self.model.hparams.unk_id)\n",
        "        return ids\n",
        "\n",
        "    def ids_to_words(self, ids):\n",
        "        words = \"\"\n",
        "        for id in ids:\n",
        "            words += self.rev_vocab[id]\n",
        "        return words\n",
        "\n",
        "    def create_inference_input(self, text):\n",
        "        inference_encoder_inputs = np.empty((self.model.hparams.encoder_length, 1),\n",
        "                                            dtype=np.int)\n",
        "        inference_encoder_inputs_lengths = np.empty(1, dtype=np.int)\n",
        "        text = TrainDataGenerator.sanitize_line(text)\n",
        "        tagger = MeCab.Tagger(\"-Owakati\")\n",
        "        words = tagger.parse(text).split()\n",
        "        ids = self.words_to_ids(words)\n",
        "        ids = ids[:self.model.hparams.encoder_length]\n",
        "        len_ids = len(ids)\n",
        "        ids.extend([self.model.hparams.pad_id] * (self.model.hparams.encoder_length - len(ids)))\n",
        "        for i in range(1):\n",
        "            inference_encoder_inputs[:, i] = np.array(ids, dtype=np.int)\n",
        "            inference_encoder_inputs_lengths[i] = len_ids\n",
        "        return inference_encoder_inputs, inference_encoder_inputs_lengths\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DQg8kU-2Dr-q",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Helper functions to test\n",
        "def make_test_training_data(hparams):\n",
        "    train_encoder_inputs = np.empty(\n",
        "        (hparams.encoder_length, hparams.batch_size), dtype=np.int)\n",
        "    train_encoder_inputs_lengths = np.empty(hparams.batch_size, dtype=np.int)\n",
        "    training_target_labels = np.empty(\n",
        "        (hparams.batch_size, hparams.decoder_length), dtype=np.int)\n",
        "    training_decoder_inputs = np.empty(\n",
        "        (hparams.decoder_length, hparams.batch_size), dtype=np.int)\n",
        "\n",
        "    # We keep first tweet to validate inference.\n",
        "    first_tweet = None\n",
        "\n",
        "    for i in range(hparams.batch_size):\n",
        "        # Tweet\n",
        "        tweet = np.random.randint(low=0, high=hparams.vocab_size,\n",
        "                                  size=hparams.encoder_length)\n",
        "        train_encoder_inputs[:, i] = tweet\n",
        "        train_encoder_inputs_lengths[i] = len(tweet)\n",
        "        # Reply\n",
        "        #   Note that low = 2, as 0 and 1 are reserved.\n",
        "        reply = np.random.randint(low=2, high=hparams.vocab_size,\n",
        "                                  size=hparams.decoder_length - 1)\n",
        "\n",
        "        training_target_label = np.concatenate((reply, np.array([hparams.eos_id])))\n",
        "        training_target_labels[i] = training_target_label\n",
        "\n",
        "        training_decoder_input = np.concatenate(([hparams.sos_id], reply))\n",
        "        training_decoder_inputs[:, i] = training_decoder_input\n",
        "\n",
        "        if i == 0:\n",
        "            first_tweet = tweet\n",
        "            info(\"0th tweet={}\".format(tweet), hparams)\n",
        "            info(\"0th reply_with_eos_suffix={}\".format(training_target_label),\n",
        "                 hparams)\n",
        "            info(\"0th reply_with_sos_prefix={}\".format(training_decoder_input),\n",
        "                 hparams)\n",
        "\n",
        "        info(\"Tweets\", hparams)\n",
        "        info(train_encoder_inputs, hparams)\n",
        "        info(\"Replies\", hparams)\n",
        "        info(training_target_labels, hparams)\n",
        "        info(training_decoder_inputs, hparams)\n",
        "    return first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs\n",
        "\n",
        "\n",
        "def test_training(test_hparams, model, infer_model):\n",
        "    if test_hparams.use_attention:\n",
        "        print(\"==== training model[attention] ====\")\n",
        "    else:\n",
        "        print(\"==== training model ====\")\n",
        "    first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs = make_test_training_data(\n",
        "        test_hparams)\n",
        "    for i in range(test_hparams.num_train_steps):\n",
        "        _ = model.train(train_encoder_inputs,\n",
        "                        train_encoder_inputs_lengths,\n",
        "                        training_target_labels,\n",
        "                        training_decoder_inputs,\n",
        "                        np.ones(test_hparams.batch_size,\n",
        "                                dtype=int) * test_hparams.decoder_length)\n",
        "        if i % 5 == 0 and test_hparams.debug_verbose:\n",
        "            print('.', end='')\n",
        "\n",
        "        if i % 15 == 0:\n",
        "            model.save()\n",
        "\n",
        "    inference_encoder_inputs = np.empty((test_hparams.encoder_length, 1),\n",
        "                                        dtype=np.int)\n",
        "    inference_encoder_inputs_lengths = np.empty(1, dtype=np.int)\n",
        "    for i in range(1):\n",
        "        inference_encoder_inputs[:, i] = first_tweet\n",
        "        inference_encoder_inputs_lengths[i] = len(first_tweet)\n",
        "\n",
        "    # testing \n",
        "    log_prob54 = infer_model.log_prob(inference_encoder_inputs,\n",
        "                                      inference_encoder_inputs_lengths,\n",
        "                                      np.array([5, 4]))\n",
        "    log_prob65 = infer_model.log_prob(inference_encoder_inputs,\n",
        "                                      inference_encoder_inputs_lengths,\n",
        "                                      np.array([6, 5]))\n",
        "    print(\"log_prob for 54\", log_prob54)\n",
        "    print(\"log_prob for 65\", log_prob65)\n",
        "\n",
        "    reward = infer_model.reward_ease_of_answering(inference_encoder_inputs,\n",
        "                                                  inference_encoder_inputs_lengths,\n",
        "                                                  np.array([[5], [6]]))\n",
        "    print(\"reward=\", reward)\n",
        "\n",
        "    if test_hparams.debug_verbose:\n",
        "        print(inference_encoder_inputs)\n",
        "    replies = infer_model.infer(inference_encoder_inputs,\n",
        "                                inference_encoder_inputs_lengths)\n",
        "    print(\"Infered replies\", replies[0])\n",
        "    print(\"Expected replies\", training_target_labels[0])\n",
        "\n",
        "def create_train_infer_models(graph, sess, hparams, force_restore=False):\n",
        "    with graph.as_default():\n",
        "        with tf.variable_scope('root'):\n",
        "            model = ChatbotModel(sess, hparams, model_path=hparams.model_path)\n",
        "           \n",
        "        with tf.variable_scope('root', reuse=True):\n",
        "            infer_model = ChatbotInferenceModel(sess, hparams,\n",
        "                                                model_path=hparams.model_path)\n",
        "            restored = model.restore()\n",
        "            if not restored:\n",
        "                if force_restore:\n",
        "                    raise Exception(\"Oops, couldn't restore\")\n",
        "                else:\n",
        "                    sess.run(tf.global_variables_initializer())\n",
        "        return model, infer_model\n",
        "\n",
        "\n",
        "def create_train_infer_models_in_graphs(train_graph, train_sess, infer_graph,\n",
        "                                        infer_sess, hparams):\n",
        "    with train_graph.as_default():\n",
        "        with tf.variable_scope('root'):\n",
        "            model = ChatbotModel(train_sess, hparams, model_path=hparams.model_path)\n",
        "            if not model.restore():\n",
        "                train_sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # note that infer_model is not sharing variable with training model.\n",
        "    with infer_graph.as_default():\n",
        "        with tf.variable_scope('root'):\n",
        "            infer_model = ChatbotInferenceModel(infer_sess, hparams,\n",
        "                                                model_path=hparams.model_path)\n",
        "\n",
        "    return model, infer_model\n",
        "\n",
        "\n",
        "def test_multiple_models_training():\n",
        "    first_tweet, train_encoder_inputs, train_encoder_inputs_length, training_target_labels, training_decoder_inputs = make_test_training_data(\n",
        "        test_hparams)\n",
        "    \n",
        "    hparams1 = copy.deepcopy(test_hparams).override_from_dict({'model_path': p(ModelDirectory.test_multiple1.value)})\n",
        "    hparams2 = copy.deepcopy(test_hparams).override_from_dict({'model_path': p(ModelDirectory.test_multiple2.value)})\n",
        "    \n",
        "\n",
        "    graph1 = tf.Graph()\n",
        "    sess1 = tf.Session(graph=graph1)\n",
        "    model, infer_model = create_train_infer_models(graph1, sess1, hparams1)\n",
        "    test_training(test_hparams, model, infer_model)\n",
        "\n",
        "    graph2 = tf.Graph()\n",
        "    sess2 = tf.Session(graph=graph2)\n",
        "    model2, infer_model2 = create_train_infer_models(graph2, sess2, hparams2)\n",
        "\n",
        "    test_training(test_hparams, model2, infer_model2)\n",
        "    dull_responses = [[4, 6, 6], [5, 5]]\n",
        "    model2.train_with_reward(infer_model2, infer_model, train_encoder_inputs,\n",
        "                             train_encoder_inputs_length,\n",
        "                             training_target_labels, training_decoder_inputs,\n",
        "                             np.ones((test_hparams.batch_size),\n",
        "                                     dtype=int) * test_hparams.decoder_length,\n",
        "                             dull_responses)\n",
        "\n",
        "    # comment out until https://github.com/tensorflow/tensorflow/issues/10731 is fixed\n",
        "    graph3 = tf.Graph()\n",
        "    sess3 = tf.Session(graph=graph3)\n",
        "#    hparams3 = copy.deepcopy(test_hparams).override_from_dict({'model_path': p(ModelDirectory.test_multiple3.value), 'use_attention': True})\n",
        "#    model3, infer_model3 = create_train_infer_models(graph3, sess3, hparams3)    \n",
        "#    test_training(test_attention_hparams, model3, infer_model3)        \n",
        "\n",
        "\n",
        "def test_save_restore_multiple_models_training():\n",
        "  \n",
        "    for d in [ModelDirectory.test_multiple1, ModelDirectory.test_multiple2, ModelDirectory.test_multiple3]:\n",
        "      shutil.rmtree(p(d.value))\n",
        "      os.makedirs(p(d.value), exist_ok=True)\n",
        "\n",
        "    # Fresh model\n",
        "    test_multiple_models_training()\n",
        "\n",
        "    # Saved model\n",
        "    test_multiple_models_training()\n",
        "\n",
        "\n",
        "def test_distributed_pattern(hparams):\n",
        "  \n",
        "    for d in [hparams.model_path]:\n",
        "      shutil.rmtree(p(d), ignore_errors=True)\n",
        "      os.makedirs(p(d), exist_ok=True)\n",
        "\n",
        "    print('==== test_distributed_pattern[{} {}] ===='.format('attention' if hparams.use_attention else '', 'beam' if hparams.beam_width > 0 else ''))\n",
        "        \n",
        "    first_tweet, train_encoder_inputs, train_encoder_inputs_lengths, training_target_labels, training_decoder_inputs = make_test_training_data(\n",
        "        hparams)\n",
        "\n",
        "    train_graph = tf.Graph()\n",
        "    infer_graph = tf.Graph()\n",
        "    train_sess = tf.Session(graph=train_graph)\n",
        "    infer_sess = tf.Session(graph=infer_graph)\n",
        "    \n",
        "\n",
        "    model, infer_model = create_train_infer_models_in_graphs(train_graph,\n",
        "                                                             train_sess,\n",
        "                                                             infer_graph,\n",
        "                                                             infer_sess,\n",
        "                                                             hparams)\n",
        "\n",
        "    for i in range(hparams.num_train_steps):\n",
        "        _ = model.train(train_encoder_inputs,\n",
        "                        train_encoder_inputs_lengths,\n",
        "                        training_target_labels,\n",
        "                        training_decoder_inputs,\n",
        "                        np.ones(hparams.batch_size,\n",
        "                                dtype=int) * hparams.decoder_length)\n",
        "\n",
        "    model.save()\n",
        "\n",
        "    inference_encoder_inputs = np.empty((hparams.encoder_length, 1),\n",
        "                                        dtype=np.int)\n",
        "    inference_encoder_inputs_lengths = np.empty(1, dtype=np.int)\n",
        "\n",
        "    inference_encoder_inputs[:, 0] = first_tweet\n",
        "    inference_encoder_inputs_lengths[0] = len(first_tweet)\n",
        "\n",
        "    infer_model.restore()\n",
        "    if hparams.beam_width == 0:\n",
        "      replies = infer_model.infer(inference_encoder_inputs,\n",
        "                                  inference_encoder_inputs_lengths)\n",
        "      print(\"Inferred replies\", replies[0])\n",
        "\n",
        "    if hparams.beam_width > 0:\n",
        "      beam_replies = infer_model.infer_beam_search(inference_encoder_inputs,\n",
        "                                                   inference_encoder_inputs_lengths)\n",
        "      print(\"Inferred replies candidate0\", beam_replies[0][:, 0])\n",
        "      print(\"Inferred replies candidate1\", beam_replies[0][:, 1])\n",
        "    print(\"Expected replies\", training_target_labels[0])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W_ciBCflZq5o",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "test_save_restore_multiple_models_training()\n",
        "\n",
        "def test_distributed_one(enable_beam, enable_attention):\n",
        "  hparams = copy.deepcopy(test_hparams).override_from_dict({\n",
        "      'model_path': p(ModelDirectory.test_distributed.value),\n",
        "      'use_attention': enable_attention,\n",
        "      'beam_width': 2 if enable_beam else 0\n",
        "  })\n",
        "  test_distributed_pattern(hparams)\n",
        "  \n",
        "test_distributed_one(enable_beam=False, enable_attention=False)\n",
        "test_distributed_one(enable_beam=False, enable_attention=True)\n",
        "test_distributed_one(enable_beam=True, enable_attention=False)\n",
        "test_distributed_one(enable_beam=False, enable_attention=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kxeWpXO5FThm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def download_file_if_necessary(file_name):\n",
        "    if os.path.exists(file_name):\n",
        "        return\n",
        "    print(\"downloading {}...\".format(file_name))\n",
        "    content = read_file_from_drive(file_name)\n",
        "    f = open(file_name, 'w')\n",
        "    f.write(content)\n",
        "    f.close()\n",
        "    print(\"downloaded\")\n",
        "\n",
        "\n",
        "def read_file_from_drive(file_name):\n",
        "    seq2seq_data_dir_id = GoogleDriveFolder.root.value\n",
        "    file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(\n",
        "        seq2seq_data_dir_id)}).GetList()\n",
        "    found = [file for file in file_list if file['title'] == file_name]\n",
        "    if found:\n",
        "        downloaded = drive.CreateFile({'id': found[0]['id']})\n",
        "        return downloaded.GetContentString()\n",
        "    else:\n",
        "        raise ValueError(\"file {} not found.\".format(file_name))\n",
        "\n",
        "\n",
        "def read_file(file_name):\n",
        "    f = open(file_name)\n",
        "    data = f.read()\n",
        "    f.close()\n",
        "    return data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W3nUhj80H6BE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def download_model_data_if_necessary(drive, model_folder_in_drive, model_path):\n",
        "    if drive is None:\n",
        "        return\n",
        "    file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(\n",
        "        model_folder_in_drive)}).GetList()\n",
        "    if not os.path.exists(model_path):\n",
        "        os.makedirs(model_path)\n",
        "    for file in file_list:\n",
        "        print(\"Downloading \", file['title'], \"...\", end='')\n",
        "        target_file = \"{}/{}\".format(model_path, file['title'])\n",
        "        if not os.path.exists(target_file):\n",
        "            file.GetContentFile(\"{}/{}\".format(model_path, file['title']))\n",
        "        print(\"done\")\n",
        "\n",
        "def generic_train_loop(train_feed_data, val_feed_data, local_vocab, local_rev_vocab,\n",
        "                       hparams, generate_models_func,\n",
        "                       inference_hook_func, tweets, drive=None, short_loop=False):\n",
        "    def save_and_infer():\n",
        "        model.save()\n",
        "        inference_hook_func(infer_model)\n",
        "        helper = InferenceHelper(infer_model, local_vocab, local_rev_vocab)\n",
        "        print(\"==== {} ====\".format(global_step))\n",
        "        for tweet in tweets:\n",
        "          helper.print_inferences(tweet)\n",
        "        \n",
        "\n",
        "    print(\"generic train loop {}:{}\".format(drive, hparams.model_folder_in_drive))\n",
        "    download_model_data_if_necessary(drive, hparams.model_folder_in_drive, hparams.model_path)\n",
        "\n",
        "    graph, sess, model, infer_model = generate_models_func(hparams)\n",
        "    writer = tf.summary.FileWriter(hparams.model_path, graph)\n",
        "\n",
        "    with graph.as_default():\n",
        "        train_data_iterator = train_feed_data.make_one_shot_iterator()\n",
        "        val_data_iterator = val_feed_data.make_one_shot_iterator()\n",
        "\n",
        "        last_saved_time = datetime.datetime.now()\n",
        "        last_time = datetime.datetime.now()\n",
        "        x = []\n",
        "        y = []\n",
        "        for i in range(hparams.num_train_steps):\n",
        "            train_data = sess.run(train_data_iterator.get_next())\n",
        "            enc_input_index = 0\n",
        "            enc_input_length_index = 1\n",
        "            dec_input_index = 3\n",
        "            dec_input_length_index = 4\n",
        "            \n",
        "            global_step, learning_rate, summary = model.train(train_data[enc_input_index], train_data[enc_input_length_index],\n",
        "                                               train_data[2], train_data[dec_input_index],\n",
        "                                               train_data[dec_input_length_index])\n",
        "            writer.add_summary(summary, global_step)\n",
        "\n",
        "\n",
        "            if short_loop and i == 2:\n",
        "                save_and_infer()\n",
        "                break\n",
        "            elif i != 0 and i % 15 == 0:\n",
        "                save_and_infer()\n",
        "                val_data = sess.run(val_data_iterator.get_next())\n",
        "                val_loss, val_loss_log = model.batch_loss(val_data[enc_input_index],\n",
        "                                                          val_data[enc_input_length_index],\n",
        "                                                          val_data[2],\n",
        "                                                          val_data[dec_input_index],\n",
        "                                                          val_data[dec_input_length_index])\n",
        "                writer.add_summary(val_loss_log, global_step)\n",
        "                print(\"validation loss\", val_loss)\n",
        "                print(\"learning rate\", learning_rate)\n",
        "                delta = (\n",
        "                                datetime.datetime.now() - last_time).total_seconds() * 1000\n",
        "                print(\n",
        "                    \"{:.2f} msec/data\".format(delta / hparams.batch_size / 15))\n",
        "                last_time = datetime.datetime.now()\n",
        "                x.append(global_step)\n",
        "                y.append(val_loss)\n",
        "            else:\n",
        "                print('.', end='')\n",
        "            now = datetime.datetime.now()\n",
        "            if (now - last_saved_time).total_seconds() > 3600 and drive is not None:\n",
        "#            if (now - last_saved_time).total_seconds() > 120 and drive is not None:           \n",
        "                drive = make_drive()\n",
        "                last_saved_time = datetime.datetime.now()\n",
        "                save_model_in_drive(drive, hparams.model_folder_in_drive, hparams.model_path)\n",
        "\n",
        "            if i != 0 and i % 100 == 0:\n",
        "                plot_validation_loss(x, y)\n",
        "\n",
        "\n",
        "def plot_validation_loss(x, y):\n",
        "  if colab():\n",
        "    plt.plot(x, y, label=\"Validation Loss\")\n",
        "    plt.plot()\n",
        "    plt.ylabel(\"Validation Loss\")\n",
        "    plt.xlabel(\"steps\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def train_loop(train_feed_data, val_feed_data, vocab, rev_vocab,\n",
        "               hparams, tweets, drive=None, short_loop=False):\n",
        "    def inference_hook(_):\n",
        "        None\n",
        "\n",
        "    def generate_models(local_hparams):\n",
        "        graph = tf.Graph()\n",
        "        sess = tf.Session(graph=graph)\n",
        "        model, infer_model = create_train_infer_models(graph, sess,\n",
        "                                                       local_hparams)\n",
        "        return graph, sess, model, infer_model\n",
        "\n",
        "    generic_train_loop(train_feed_data, val_feed_data, vocab, rev_vocab,\n",
        "                       hparams, generate_models, inference_hook, tweets,\n",
        "                       drive, short_loop)\n",
        "\n",
        "\n",
        "def train_loop_distributed_pattern(train_feed_data, val_feed_data, vocab,\n",
        "                                   rev_vocab, hparams, tweets, drive=None,\n",
        "                                   short_loop=False):\n",
        "    def inference_hook(infer_model):\n",
        "        # always restore from file, because it's in different graph.\n",
        "        restored = infer_model.restore()\n",
        "        assert restored\n",
        "\n",
        "    def generate_models(local_hparams):\n",
        "        train_graph = tf.Graph()\n",
        "        infer_graph = tf.Graph()\n",
        "        # See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
        "        config = tf.ConfigProto(log_device_placement=True)\n",
        "        config.gpu_options.allow_growth = True\n",
        "        train_sess = tf.Session(graph=train_graph, config=config)\n",
        "        print(\"train_sess=\", train_sess)\n",
        "        infer_sess = tf.Session(graph=infer_graph, config=config)\n",
        "\n",
        "        device = '/cpu:0'\n",
        "        if has_gpu0():\n",
        "            device = '/gpu:0'\n",
        "            print(\"!!!GPU ENABLED !!!\")\n",
        "        with tf.device(device):\n",
        "            model, infer_model = create_train_infer_models_in_graphs(\n",
        "                train_graph,\n",
        "                train_sess,\n",
        "                infer_graph,\n",
        "                infer_sess,\n",
        "                local_hparams)\n",
        "        return train_graph, train_sess, model, infer_model\n",
        "\n",
        "    generic_train_loop(train_feed_data, val_feed_data, vocab, rev_vocab,\n",
        "                       hparams, generate_models, inference_hook,\n",
        "                       tweets, drive, short_loop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "81_wjAUyCVBp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class TrainDataGenerator:\n",
        "    def __init__(self, source_path, hparams):\n",
        "        self.source_path = source_path\n",
        "        self.hparams = hparams\n",
        "        basename, extension = os.path.splitext(self.source_path)\n",
        "        self.enc_path = \"{}_enc{}\".format(basename, extension)\n",
        "        self.dec_path = \"{}_dec{}\".format(basename, extension)\n",
        "        self.enc_idx_path = \"{}_enc_idx{}\".format(basename, extension)\n",
        "        self.dec_idx_path = \"{}_dec_idx{}\".format(basename, extension)\n",
        "        self.dec_idx_eos_path = \"{}_dec_idx_eos{}\".format(basename, extension)\n",
        "        self.dec_idx_sos_path = \"{}_dec_idx_sos{}\".format(basename, extension)\n",
        "        self.dec_idx_len_path = \"{}_dec_idx_len{}\".format(basename, extension)\n",
        "\n",
        "        self.enc_idx_padded_path = \"{}_enc_idx_padded{}\".format(basename,\n",
        "                                                                extension)\n",
        "        self.enc_idx_len_path = \"{}_enc_idx_len{}\".format(basename, extension)\n",
        "\n",
        "        self.vocab_path = \"{}_vocab{}\".format(basename, extension)\n",
        "        self.max_vocab_size = hparams.vocab_size\n",
        "        self.start_vocabs = [hparams.sos_token, hparams.eos_token, hparams.pad_token, hparams.unk_token]\n",
        "        self.tagger = MeCab.Tagger(\"-Owakati\")\n",
        "\n",
        "    def generate(self):\n",
        "        print(\"generating enc and dec files...\")\n",
        "        self._generate_enc_dec()\n",
        "        print(\"generating vocab file...\")\n",
        "        self._generate_vocab()\n",
        "        print(\"loading vocab...\")\n",
        "        vocab, _ = self._load_vocab()\n",
        "        print(\"generating id files...\")\n",
        "        self._generate_id_file(self.enc_path, self.enc_idx_path, vocab)\n",
        "        self._generate_id_file(self.dec_path, self.dec_idx_path, vocab)\n",
        "        print(\"generating padded input file...\")\n",
        "        self._generate_enc_idx_padded(self.enc_idx_path,\n",
        "                                      self.enc_idx_padded_path,\n",
        "                                      self.enc_idx_len_path,\n",
        "                                      self.hparams.encoder_length)\n",
        "        print(\"generating dec eos/sos files...\")\n",
        "        self._generate_dec_idx_eos(self.dec_idx_path, self.dec_idx_eos_path,\n",
        "                                   self.hparams.decoder_length)\n",
        "        self._generate_dec_idx_sos(self.dec_idx_path, self.dec_idx_sos_path,\n",
        "                                   self.dec_idx_len_path,\n",
        "                                   self.hparams.decoder_length)\n",
        "        print(\"done\")\n",
        "        return self._create_dataset()\n",
        "\n",
        "    def _generate_id_file(self, source_path, dest_path, vocab):\n",
        "        if gfile.Exists(dest_path):\n",
        "            return\n",
        "        with gfile.GFile(source_path, mode=\"rb\") as f, gfile.GFile(dest_path,\n",
        "                                                                   mode=\"wb\") as of:\n",
        "            for line in f:\n",
        "                line = line.decode('utf-8')\n",
        "                words = self.tagger.parse(line).split()\n",
        "                ids = [vocab.get(w, self.hparams.unk_id) for w in words]\n",
        "                of.write(\" \".join([str(id) for id in ids]) + \"\\n\")\n",
        "\n",
        "    def _load_vocab(self):\n",
        "        rev_vocab = []\n",
        "        with gfile.GFile(self.vocab_path, mode=\"r\") as f:\n",
        "            rev_vocab.extend(f.readlines())\n",
        "            rev_vocab = [line.strip() for line in rev_vocab]\n",
        "            # Dictionary of (word, idx)\n",
        "            vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
        "            return vocab, rev_vocab\n",
        "\n",
        "    def _generate_vocab(self):\n",
        "        if gfile.Exists(self.vocab_path):\n",
        "            return\n",
        "        vocab_dic = self._build_vocab_dic(self.enc_path)\n",
        "        vocab_dic = self._build_vocab_dic(self.dec_path, vocab_dic)\n",
        "        vocab_list = self.start_vocabs + sorted(vocab_dic, key=vocab_dic.get,\n",
        "                                                reverse=True)\n",
        "        if len(vocab_list) > self.max_vocab_size:\n",
        "            vocab_list = vocab_list[:self.max_vocab_size]\n",
        "        with gfile.GFile(self.vocab_path, mode=\"w\") as vocab_file:\n",
        "            for w in vocab_list:\n",
        "                vocab_file.write(w + \"\\n\")\n",
        "\n",
        "    def _generate_enc_dec(self):\n",
        "        if gfile.Exists(self.enc_path) and gfile.Exists(self.dec_path):\n",
        "            return\n",
        "        with gfile.GFile(self.source_path, mode=\"rb\") as f, gfile.GFile(\n",
        "                self.enc_path, mode=\"w+\") as ef, gfile.GFile(self.dec_path,\n",
        "                                                             mode=\"w+\") as df:\n",
        "            tweet = None\n",
        "            reply = None\n",
        "            for i, line in enumerate(f):\n",
        "                line = line.decode('utf-8')\n",
        "                line = self.sanitize_line(line)\n",
        "                if i % 2 == 0:\n",
        "                  tweet = line\n",
        "                else:\n",
        "                  reply = line\n",
        "                  if tweet and reply:\n",
        "                    ef.write(tweet)\n",
        "                    df.write(reply)\n",
        "                  tweet = None\n",
        "                  reply = None\n",
        "\n",
        "    def _generate_enc_idx_padded(self, source_path, dest_path, dest_len_path,\n",
        "                                 max_line_len):\n",
        "        if gfile.Exists(dest_path):\n",
        "            return\n",
        "        with open(source_path) as fin, open(dest_path,\n",
        "                                            \"w\") as fout, open(dest_len_path,\n",
        "                                                               \"w\") as flen:\n",
        "            line = fin.readline()\n",
        "            while line:\n",
        "                ids = [int(x) for x in line.split()]\n",
        "                if len(ids) > max_line_len:\n",
        "                    ids = ids[:max_line_len]\n",
        "                flen.write(str(len(ids)))\n",
        "                flen.write(\"\\n\")\n",
        "                if len(ids) < max_line_len:\n",
        "                    ids.extend([self.hparams.pad_id] * (max_line_len - len(ids)))\n",
        "                ids = [str(x) for x in ids]\n",
        "                fout.write(\" \".join(ids))\n",
        "                fout.write(\"\\n\")\n",
        "                line = fin.readline()\n",
        "\n",
        "    # read decoder_idx file and append eos at the end of idx list.\n",
        "    def _generate_dec_idx_eos(self, source_path, dest_path, max_line_len):\n",
        "        if gfile.Exists(dest_path):\n",
        "            return\n",
        "        with open(source_path) as fin, open(dest_path, \"w\") as fout:\n",
        "            line = fin.readline()\n",
        "            while line:\n",
        "                ids = [int(x) for x in line.split()]\n",
        "                if len(ids) > max_line_len - 1:\n",
        "                    ids = ids[:max_line_len - 1]\n",
        "                ids.append(self.hparams.eos_id)\n",
        "                if len(ids) < max_line_len:\n",
        "                    ids.extend([self.hparams.pad_id] * (max_line_len - len(ids)))\n",
        "                ids = [str(x) for x in ids]\n",
        "                fout.write(\" \".join(ids))\n",
        "                fout.write(\"\\n\")\n",
        "                line = fin.readline()\n",
        "\n",
        "    # read decoder_idx file and put sos at the beginning of the idx list.\n",
        "    # also write out length of index list.\n",
        "    def _generate_dec_idx_sos(self, source_path, dest_path, dest_len_path,\n",
        "                              max_line_len):\n",
        "        if gfile.Exists(dest_path):\n",
        "            return\n",
        "        with open(source_path) as fin, open(dest_path, \"w\") as fout, open(\n",
        "                dest_len_path, \"w\") as flen:\n",
        "            line = fin.readline()\n",
        "            while line:\n",
        "                ids = [self.hparams.sos_id]\n",
        "                ids.extend([int(x) for x in line.split()])\n",
        "                if len(ids) > max_line_len:\n",
        "                    ids = ids[:max_line_len]\n",
        "                flen.write(str(len(ids)))\n",
        "                flen.write(\"\\n\")\n",
        "                if len(ids) < max_line_len:\n",
        "                    ids.extend([self.hparams.pad_id] * (max_line_len - len(ids)))\n",
        "                ids = [str(x) for x in ids]\n",
        "                fout.write(\" \".join(ids))\n",
        "                fout.write(\"\\n\")\n",
        "                line = fin.readline()\n",
        "\n",
        "    @staticmethod\n",
        "    def sanitize_line(line):\n",
        "        # replace @username\n",
        "        # replacing @username had bad impace where USERNAME token shows up everywhere.\n",
        "#        line = re.sub(r\"@([A-Za-z0-9_]+)\", \"USERNAME\", line)\n",
        "        line = re.sub(r\"@([A-Za-z0-9_]+)\", \"\", line)\n",
        "        # Remove URL\n",
        "        line = re.sub(r'https?:\\/\\/.*', \"\", line)\n",
        "        line = line.lstrip()\n",
        "        return line\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_source_target_swapped(source_path):\n",
        "        basename, extension = os.path.splitext(source_path)\n",
        "        dest_path = \"{}_swapped{}\".format(basename, extension)\n",
        "        with gfile.GFile(source_path, mode=\"rb\") as fin, gfile.GFile(dest_path,\n",
        "                                                                     mode=\"w+\") as fout:\n",
        "            temp = None\n",
        "            for i, line in enumerate(fin):\n",
        "                if i % 2 == 0:\n",
        "                    temp = line\n",
        "                else:\n",
        "                    fout.write(line)\n",
        "                    fout.write(temp)\n",
        "                    temp = None\n",
        "        return dest_path\n",
        "\n",
        "    def _build_vocab_dic(self, source_path, vocab_dic={}):\n",
        "        with gfile.GFile(source_path, mode=\"r\") as f:\n",
        "            for line in f:\n",
        "                words = self.tagger.parse(line).split()\n",
        "                for word in words:\n",
        "                    if word in vocab_dic:\n",
        "                        vocab_dic[word] += 1\n",
        "                    else:\n",
        "                        vocab_dic[word] = 1\n",
        "            return vocab_dic\n",
        "\n",
        "    @staticmethod\n",
        "    def _read_file(source_path):\n",
        "        f = open(source_path)\n",
        "        data = f.read()\n",
        "        f.close()\n",
        "        return data\n",
        "\n",
        "    def _read_vocab(self, source_path):\n",
        "        rev_vocab = []\n",
        "        rev_vocab.extend(self._read_file(source_path).splitlines())\n",
        "        rev_vocab = [line.strip() for line in rev_vocab]\n",
        "        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
        "        return vocab, rev_vocab\n",
        "\n",
        "    def text_line_split_dataset(self, filename):\n",
        "        return tf.data.TextLineDataset(filename).map(self.split_to_int_values)\n",
        "\n",
        "    @staticmethod\n",
        "    def split_to_int_values(x):\n",
        "        return tf.string_to_number(tf.string_split([x]).values, tf.int32)\n",
        "\n",
        "    def _create_dataset(self):\n",
        "\n",
        "        tweets_dataset = self.text_line_split_dataset(self.enc_idx_padded_path)\n",
        "        tweets_lengths_dataset = tf.data.TextLineDataset(\n",
        "            self.enc_idx_len_path)\n",
        "\n",
        "        replies_sos_dataset = self.text_line_split_dataset(\n",
        "            self.dec_idx_sos_path)\n",
        "        replies_eos_dataset = self.text_line_split_dataset(\n",
        "            self.dec_idx_eos_path)\n",
        "        replies_sos_lengths_dataset = tf.data.TextLineDataset(\n",
        "            self.dec_idx_len_path)\n",
        "\n",
        "        tweets_transposed = tweets_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(\n",
        "                self.hparams.batch_size)).map(\n",
        "            lambda x: tf.transpose(x))\n",
        "        tweets_lengths = tweets_lengths_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(self.hparams.batch_size))\n",
        "\n",
        "        replies_with_eos_suffix = replies_eos_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(self.hparams.batch_size))\n",
        "        replies_with_sos_prefix = replies_sos_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(\n",
        "                self.hparams.batch_size)).map(\n",
        "            lambda x: tf.transpose(x))\n",
        "        replies_with_sos_suffix_lengths = replies_sos_lengths_dataset.apply(\n",
        "            tf.contrib.data.batch_and_drop_remainder(\n",
        "                self.hparams.batch_size))\n",
        "        vocab, rev_vocab = self._read_vocab(self.vocab_path)\n",
        "        return tf.data.Dataset.zip((tweets_transposed, tweets_lengths,\n",
        "                                    replies_with_eos_suffix,\n",
        "                                    replies_with_sos_prefix,\n",
        "                                    replies_with_sos_suffix_lengths)), vocab, rev_vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I0-o2wg0gzvA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def list_model_file(path):\n",
        "    f = open('{}/checkpoint'.format(path))\n",
        "    text = f.read()\n",
        "    f.close()\n",
        "    print(text)\n",
        "    m = re.match(r\".*ChatbotModel\\-(\\d+)\", text)\n",
        "    model_name = m.group(1)\n",
        "    all = [\"checkpoint\"]\n",
        "    all.extend([x for x in os.listdir(path) if re.search(model_name, x) or re.search('events.out', x)])\n",
        "    print(\"all=\", all)\n",
        "    return all\n",
        "\n",
        "\n",
        "def save_model_in_drive(drive, model_folder_in_drive, model_path):\n",
        "    file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(\n",
        "        model_folder_in_drive)}).GetList()\n",
        "    for model_file in list_model_file(model_path):\n",
        "        file = drive.CreateFile({'title': model_file, \"parents\": [\n",
        "            {\"kind\": \"drive#fileLink\", \"id\": model_folder_in_drive}]})\n",
        "        file.SetContentFile(\"{}/{}\".format(model_path, model_file))\n",
        "        print(\"Uploading \", model_file, \"...\", end=\"\")\n",
        "        file.Upload()\n",
        "        print(\"done\")\n",
        "    for file in file_list:\n",
        "        f = drive.CreateFile({'id': file['id']})\n",
        "        f.Delete()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g7qybKCEFgXE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "if colab():\n",
        "    !pip install pydrive\n",
        "    from pydrive.auth import GoogleAuth\n",
        "    from pydrive.drive import GoogleDrive\n",
        "    from google.colab import auth\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "\n",
        "    def make_drive():\n",
        "        # 1. Authenticate and create the PyDrive client.\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        drive = GoogleDrive(gauth)\n",
        "        return drive\n",
        "    drive = make_drive()\n",
        "else:        \n",
        "    drive = None\n",
        "    def make_drive():\n",
        "        return None\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PBo5SLAk3vza",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get -qq install -y mecab libmecab-dev mecab-ipadic mecab-ipadic-utf8\n",
        "!pip install mecab-python3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LPHzPckzWBQ8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls \n",
        "!wc -l tweets_small_vocab.txt\n",
        "!cat tweets_small_vocab.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JrvS_DURF5Pq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 2
            },
            {
              "item_id": 3
            },
            {
              "item_id": 11
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "outputId": "942c9e21-c80f-43c0-fd75-e19ca3dbe32e"
      },
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "\n",
        "\n",
        "def clean_model_path(model_path):\n",
        "    shutil.rmtree(model_path)\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "\n",
        "def print_header(text):\n",
        "    print(\"============== {} ==============\".format(text))\n",
        "\n",
        "\n",
        "def test_tweets_small(hparams):\n",
        "    print_header(\"train_loop_distributed_pattern\")\n",
        "    clean_model_path(hparams.model_path)\n",
        "\n",
        "    source_path = \"tweets_small.txt\"\n",
        "    download_file_if_necessary(source_path)\n",
        "    generator = TrainDataGenerator(source_path=source_path, hparams=hparams)\n",
        "    train_dataset, vocab, rev_vocab = generator.generate()\n",
        "\n",
        "    tweets = [\"おはようございます。寒いですね。\", \"さて帰ろう。明日は早い。\", \"今回もよろしくです。\"]\n",
        "\n",
        "    train_loop_distributed_pattern(train_dataset.repeat(100),\n",
        "                                   train_dataset.repeat(100),\n",
        "                                   vocab,\n",
        "                                   rev_vocab,\n",
        "                                   hparams, tweets, short_loop=False)\n",
        "\n",
        "\n",
        "def test_tweets_small_swapped(hparams):\n",
        "    print_header(\"train_loop_distributed_pattern swapped\")\n",
        "\n",
        "    clean_model_path(hparams.model_path)\n",
        "\n",
        "    source_path = \"tweets_small.txt\"\n",
        "    download_file_if_necessary(source_path)\n",
        "    TrainDataGenerator.generate_source_target_swapped(p(source_path))\n",
        "\n",
        "    swapped_path = \"tweets_small_swapped.txt\"\n",
        "\n",
        "    generator = TrainDataGenerator(source_path=swapped_path, hparams=hparams)\n",
        "    train_dataset, vocab, rev_vocab = generator.generate()\n",
        "\n",
        "    replies = [\"@higepon おはようございます！\", \"おつかれさまー。気をつけて。\", \"こちらこそよろしくお願いします。\"]\n",
        "\n",
        "    print_header(\"train_loop_distributed_pattern\")\n",
        "    train_loop_distributed_pattern(train_dataset.repeat(100),\n",
        "                                   train_dataset.repeat(100),\n",
        "                                   vocab,\n",
        "                                   rev_vocab,\n",
        "                                   hparams, replies, short_loop=False)\n",
        "\n",
        "# vocab size 変えたら動かなくなった\n",
        "tweet_small_hparams = copy.deepcopy(base_hparams).override_from_dict(\n",
        "    {\n",
        "        'batch_size': 6,  # of tweets should be dividable by batch_size\n",
        "        'encoder_length': 8,\n",
        "        'decoder_length': 8,\n",
        "        'num_units': 256,\n",
        "        'num_layers': 2,\n",
        "        'vocab_size': 34,\n",
        "        'embedding_size': 40,\n",
        "        'beam_width': 2,  # for faster iteration, this should be 10\n",
        "        'num_train_steps': 200,\n",
        "        'model_path': p(ModelDirectory.tweet_small.value),\n",
        "        'learning_rate': 0.05,\n",
        "        'use_attention': True,\n",
        "    })\n",
        "\n",
        "tweet_small_swapped_hparams = copy.deepcopy(\n",
        "    tweet_small_hparams).override_from_dict(\n",
        "    {'model_path': p(ModelDirectory.tweet_small_swapped.value)})\n",
        "\n",
        "!rm tweets_small*\n",
        "test_tweets_small(tweet_small_hparams)\n",
        "test_tweets_small_swapped(tweet_small_swapped_hparams)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".INFO:tensorflow:Restoring parameters from model/tweet_small/ChatbotModel-91\n",
            "==== 91 ====\n",
            "おはようございます。寒いですね。\n",
            "    [0]おつかれさまー[EOS][EOS]\n",
            "    [1]おつかれさまー。[EOS]\n",
            "さて帰ろう。明日は早い。\n",
            "    [0]おつかれさまー[EOS][EOS]\n",
            "    [1]おつかれさまー。[EOS]\n",
            "今回もよろしくです。\n",
            "    [0]こちらこそよろしくお願い。。[EOS]\n",
            "    [1]こちらこそよろしくお願いし。[EOS]\n",
            "validation loss 8.310265\n",
            "learning rate 0.05\n",
            "30.32 msec/data\n",
            ".........."
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFYCAYAAAB+s6Q9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f997b62de80>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "....INFO:tensorflow:Restoring parameters from model/tweet_small/ChatbotModel-106\n",
            "==== 106 ====\n",
            "おはようございます。寒いですね。\n",
            "    [0]おはようございます！ー。。。\n",
            "    [1]おはようございます！ー。。[EOS]\n",
            "さて帰ろう。明日は早い。\n",
            "    [0]おつかれさまーー気気\n",
            "    [1]おつかれさまーー。気\n",
            "今回もよろしくです。\n",
            "    [0]こちらこそよろしくお願いしし。。\n",
            "    [1]こちらこそよろしくお願いしします。\n",
            "validation loss 6.1730757\n",
            "learning rate 0.05\n",
            "32.03 msec/data\n",
            "......."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EnmDxS2l4dDY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#!cat tweets_small_enc.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tzh2rhEPguJ9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "run_large = False\n",
        "run_large_swapped = True\n",
        "\n",
        "\n",
        "def test_tweets_large(hparams):\n",
        "    print_header(\"train_loop_distributed_pattern\")\n",
        "\n",
        "    source_path = \"tweets_large.txt\"\n",
        "    download_file_if_necessary(source_path)\n",
        "    generator = TrainDataGenerator(source_path=source_path, hparams=hparams)\n",
        "    train_dataset, vocab, rev_vocab = generator.generate()\n",
        "\n",
        "    tweets = [\"今日のドラマ面白そう！\",\"お腹すいたー\", \"おやすみ～\", \"おはようございます。寒いですね。\", \"さて帰ろう。明日は早い。\", \"今回もよろしくです。\", \"ばいとおわ！\"]\n",
        "\n",
        "    train_loop_distributed_pattern(train_dataset.repeat().shuffle(1024),\n",
        "                                   train_dataset.repeat().shuffle(1024, seed=1234),\n",
        "                                   vocab,\n",
        "                                   rev_vocab,\n",
        "                                   hparams, tweets, short_loop=False,\n",
        "                                   drive=drive)\n",
        "\n",
        "\n",
        "def test_tweets_large_swapped(hparams):\n",
        "    print_header(\"train_loop_distributed_pattern\")\n",
        "\n",
        "    source_path = \"tweets_large.txt\"\n",
        "    download_file_if_necessary(source_path)\n",
        "    swapped_source_path = TrainDataGenerator.generate_source_target_swapped(\n",
        "        source_path)\n",
        "\n",
        "    generator = TrainDataGenerator(source_path=swapped_source_path,\n",
        "                                   hparams=hparams)\n",
        "    train_dataset, vocab, rev_vocab = generator.generate()\n",
        "\n",
        "    tweets = [\"今日のドラマ面白そう！\",\"お腹すいたー\", \"おやすみ～\", \"おはようございます。寒いですね。\", \"さて帰ろう。明日は早い。\", \"今回もよろしくです。\", \"ばいとおわ！\"]\n",
        "\n",
        "\n",
        "    train_loop_distributed_pattern(train_dataset.repeat().shuffle(1024),\n",
        "                                   train_dataset.repeat().shuffle(1024, seed=1234),\n",
        "                                   vocab,\n",
        "                                   rev_vocab,\n",
        "                                   hparams, tweets, short_loop=False,\n",
        "                                   drive=drive)\n",
        "\n",
        "\n",
        "tweet_large_hparams = copy.deepcopy(base_hparams).override_from_dict(\n",
        "    {\n",
        "        # In typical seq2seq chatbot\n",
        "        # num_layers=3, learning_rate=0.5, batch_size=64, vocab=20000-100000, learning_rate decay is 0.99, which is taken care as default parameter in AdamOptimizer.\n",
        "        'batch_size': 64,  # of tweets should be dividable by batch_size\n",
        "        'encoder_length': 30,\n",
        "        'decoder_length': 30,\n",
        "        'num_units': 1024,\n",
        "        'num_layers': 3,\n",
        "        'vocab_size': 50000,\n",
        "        'embedding_size': 1024,\n",
        "        'beam_width': 2,  # for faster iteration, this should be 10\n",
        "        'num_train_steps': 1000000,\n",
        "        'model_folder_in_drive': GoogleDriveFolder.seq2seq.value,\n",
        "        'model_path': p(ModelDirectory.tweet_large.value),\n",
        "        'learning_rate': 0.5, # For vocab_size 50000, num_layers 3, num_units 1024, tweet_large, starting learning_rate 0.05 works well, change it t0 0.01 at perplexity 800, changed it to 0.005 at 200.\n",
        "        'learning_rate_decay': 0.99,\n",
        "        'use_attention': True,\n",
        "        # testing new restore learning rate and no USERNAME TOKEN\n",
        "    })\n",
        "\n",
        "tweet_large_swapped_hparams = copy.deepcopy(\n",
        "    tweet_large_hparams).override_from_dict(\n",
        "    {\n",
        "        'model_folder_in_drive': GoogleDriveFolder.seq2seq_swapped.value,\n",
        "        'model_path': p(ModelDirectory.tweet_large_swapped.value)\n",
        "    })\n",
        "\n",
        "if run_large:\n",
        "    test_tweets_large(tweet_large_hparams)\n",
        "if run_large_swapped:\n",
        "    test_tweets_large_swapped(tweet_large_swapped_hparams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dv4xb9avyi88",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "##!rm tweets_large* \n",
        "#!rm -rf model/tweet_large\n",
        "!ls\n",
        "!grep  -in '^$' tweets_large_dec.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wQcU64vOFTEp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3jxF0tWHoBL-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "|def test_mutual_information(hparams, swapped_hparams):\n",
        "    graph = tf.Graph()\n",
        "    swapped_graph = tf.Graph()\n",
        "    config = tf.ConfigProto(log_device_placement=True)\n",
        "    config.gpu_options.allow_growth = True\n",
        "\n",
        "    drive = make_drive()\n",
        "    download_model_data_if_necessary(drive, hparams.model_folder_in_drive,\n",
        "                                     p(ModelDirectory.tweet.value))\n",
        "    download_model_data_if_necessary(drive,\n",
        "                                     swapped_hparams.model_folder_in_drive,\n",
        "                                     p(ModelDirectory.tweet_swapped.value))\n",
        "\n",
        "    with graph.as_default():\n",
        "        infer_sess = tf.Session(graph=graph, config=config)\n",
        "        with tf.variable_scope('root'):\n",
        "            model = ChatbotInferenceModel(infer_sess, hparams,\n",
        "                                          model_path=p(ModelDirectory.tweet.value))\n",
        "            model.restore()\n",
        "\n",
        "    with swapped_graph.as_default():\n",
        "        swap_sess = tf.Session(graph=swapped_graph, config=config)\n",
        "        with tf.variable_scope('root'):\n",
        "            smodel = ChatbotInferenceModel(swap_sess, swapped_hparams,\n",
        "                                           model_path=p(\n",
        "                                               ModelDirectory.tweet_swapped.value))\n",
        "            smodel.restore()\n",
        "            helper = InferenceHelper(model, vocab, rev_vocab)\n",
        "            helper.print_inferences(\"疲れた\")\n",
        "\n",
        "            shelper = InferenceHelper(smodel, vocab, rev_vocab)\n",
        "            shelper.print_inferences(\"お疲れ様\")\n",
        "\n",
        "\n",
        "#large_beam_hparams = copy.deepcopy(large_hparams)\n",
        "#large_beam_hparams.beam_width = 20\n",
        "#test_mutual_information(large_beam_hparams, large_swapped_hparams)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FGFWGrQv_kZ-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "ｑ"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
